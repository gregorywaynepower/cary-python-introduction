[
  {
    "objectID": "SECURITY.html",
    "href": "SECURITY.html",
    "title": "Security Policy",
    "section": "",
    "text": "File the vulnerability as an issue."
  },
  {
    "objectID": "SECURITY.html#reporting-a-vulnerability",
    "href": "SECURITY.html#reporting-a-vulnerability",
    "title": "Security Policy",
    "section": "",
    "text": "File the vulnerability as an issue."
  },
  {
    "objectID": "notebooks/reading_local_files.html",
    "href": "notebooks/reading_local_files.html",
    "title": "An Introduction to Jupyter Notebooks",
    "section": "",
    "text": "Jupyter Notebooks are a file format (*.ipynb) that you can execute and explain your code in a step-wise format. &gt; Jupyter Notebooks supports not only code execution in Python, but over 40 languages including R, Lua, Rust, and Julia with numerous kernels.\nWe can write in Markdown to write text with some level of control over your formatting. - Here’s a Link to Basic Markdown - Here’s a link to Markdown’s Extended Syntax\nTopics We Will Cover - Importing different files and filetypes with pandas - Basic Statistical Analysis of tabular data with pandas and numpy - Creating Charts with python packages from the Matplotlib, Plotly, or HoloViz Ecosystem - Evaluate the potential usecases for each visualization package\n  This is you, enjoying the learning process.\nStep 1: Import pandas into your python program.\n\nimport pandas as pd\n\n# This will import the pandas and numpy packages into your Python program.\n\ndf_json = pd.read_json('../data/food-waste-pilot/food-waste-pilot.json')\ndf_csv = pd.read_csv('../data/food-waste-pilot/food-waste-pilot.csv')\ndf_xlsx = pd.read_excel('../data/food-waste-pilot/food-waste-pilot.xlsx')\n\n\ndf_csv.shape\n\n(152, 3)\n\n\n\ndf_csv.head() # Grabs the top 5 items in your Dataframe by default.\n\n\n\n\n\n\n\n\nCollection Date\nFood Waste Collected\nEstimated Earned Compost Created\n\n\n\n\n0\n2022-02-25\n250.8\n25\n\n\n1\n2022-03-02\n298.8\n30\n\n\n2\n2022-03-21\n601.2\n60\n\n\n3\n2022-03-28\n857.2\n86\n\n\n4\n2022-03-30\n610.8\n61\n\n\n\n\n\n\n\n\ndf_csv.tail() # Grabs the bottom 5 items in your Dataframe by default.\n\n\n\n\n\n\n\n\nCollection Date\nFood Waste Collected\nEstimated Earned Compost Created\n\n\n\n\n147\n2022-10-12\n385.8\n39\n\n\n148\n2022-10-28\n713.6\n71\n\n\n149\n2022-10-31\n953.4\n95\n\n\n150\n2022-12-14\n694.4\n69\n\n\n151\n2023-01-06\n968.6\n97\n\n\n\n\n\n\n\n\ndf_csv.columns\n\nIndex(['Collection Date', 'Food Waste Collected',\n       'Estimated Earned Compost Created'],\n      dtype='object')\n\n\n\ndf_csv.dtypes # Returns the data types of your columns.\n\nCollection Date                      object\nFood Waste Collected                float64\nEstimated Earned Compost Created      int64\ndtype: object\n\n\n\ndf_csv.describe()\n\n\n\n\n\n\n\n\nFood Waste Collected\nEstimated Earned Compost Created\n\n\n\n\ncount\n152.000000\n152.000000\n\n\nmean\n526.873684\n52.611842\n\n\nstd\n197.838075\n19.787631\n\n\nmin\n0.000000\n0.000000\n\n\n25%\n398.050000\n39.750000\n\n\n50%\n531.500000\n53.000000\n\n\n75%\n658.900000\n66.000000\n\n\nmax\n1065.800000\n107.000000\n\n\n\n\n\n\n\n\ndf_csv.info() # Returns index, column names, a count of Non-Null values, and data types.\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 152 entries, 0 to 151\nData columns (total 3 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   Collection Date                   152 non-null    object \n 1   Food Waste Collected              152 non-null    float64\n 2   Estimated Earned Compost Created  152 non-null    int64  \ndtypes: float64(1), int64(1), object(1)\nmemory usage: 3.7+ KB\n\n\nThere are multiple methods to do type conversion using pandas as well.\n\n# Oh no, we can see that our Collection Date is not the data type that we want, we need to convert it to a date value.\n\ndf_csv['Collection Date'] = pd.to_datetime(df_csv['Collection Date'])\n\n\ndf_csv.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 152 entries, 0 to 151\nData columns (total 3 columns):\n #   Column                            Non-Null Count  Dtype         \n---  ------                            --------------  -----         \n 0   Collection Date                   152 non-null    datetime64[ns]\n 1   Food Waste Collected              152 non-null    float64       \n 2   Estimated Earned Compost Created  152 non-null    int64         \ndtypes: datetime64[ns](1), float64(1), int64(1)\nmemory usage: 3.7 KB\n\n\n\n# An alternative way to do this date conversion:\n\ndf_csv['Collection Date'] = df_csv['Collection Date'].apply(pd.to_datetime)\n\n\n# astype() is more generic method to convert data types\n\ndf_csv['Collection Date'] = df_csv['Collection Date'].astype('datetime64[ns]')\n\n\ndf_csv.dtypes\n\nCollection Date                     datetime64[ns]\nFood Waste Collected                       float64\nEstimated Earned Compost Created             int64\ndtype: object\n\n\n\n# Now that we have converted our Collection Date column to a datetime data type, we can use the dt.day_name() method to create a new column that contains the day of the week.\n\ndf_csv['Day of Week'] = df_csv['Collection Date'].dt.day_name()\n\n\n# What if we want to know the date that we collected the most food waste?\n\ndf_csv.loc[\n    df_csv['Food Waste Collected'].idxmax(),\n    ['Collection Date']\n]\n\nCollection Date    2022-08-10 00:00:00\nName: 20, dtype: object\n\n\n\n# If you wanted to see our top 10 collection dates, you could do this:\n\ndf_csv.nlargest(10,'Food Waste Collected')\n\n\n\n\n\n\n\n\nCollection Date\nFood Waste Collected\nEstimated Earned Compost Created\nDay of Week\n\n\n\n\n20\n2022-08-10\n1065.8\n107\nWednesday\n\n\n124\n2022-12-27\n987.4\n99\nTuesday\n\n\n48\n2022-09-12\n977.8\n98\nMonday\n\n\n151\n2023-01-06\n968.6\n97\nFriday\n\n\n149\n2022-10-31\n953.4\n95\nMonday\n\n\n3\n2022-03-28\n857.2\n86\nMonday\n\n\n123\n2022-11-28\n844.4\n84\nMonday\n\n\n57\n2022-12-05\n834.4\n83\nMonday\n\n\n91\n2022-12-30\n815.4\n82\nFriday\n\n\n137\n2022-07-18\n807.8\n81\nMonday\n\n\n\n\n\n\n\n\ndf_csv.nsmallest(10,'Food Waste Collected')\n\n\n\n\n\n\n\n\nCollection Date\nFood Waste Collected\nEstimated Earned Compost Created\nDay of Week\n\n\n\n\n53\n2022-11-11\n0.0\n0\nFriday\n\n\n95\n2023-01-16\n0.0\n0\nMonday\n\n\n114\n2022-09-05\n0.0\n0\nMonday\n\n\n97\n2022-02-09\n59.0\n6\nWednesday\n\n\n36\n2022-02-16\n102.8\n10\nWednesday\n\n\n63\n2022-02-21\n183.8\n18\nMonday\n\n\n61\n2022-02-11\n197.0\n20\nFriday\n\n\n39\n2022-04-15\n200.8\n20\nFriday\n\n\n62\n2022-02-18\n202.8\n20\nFriday\n\n\n29\n2022-12-10\n205.8\n21\nSaturday\n\n\n\n\n\n\n\n\ndf_csv.plot()\n\n\n\n\n\n\n\n\n\n\n\ndf_csv_parsed_dates = pd.read_csv('../data/food-waste-pilot/food-waste-pilot.csv', parse_dates=True, index_col=\"Collection Date\")\n\n\ndf_csv_parsed_dates.plot()",
    "crumbs": [
      "Home",
      "An Introduction to Jupyter Notebooks"
    ]
  },
  {
    "objectID": "notebooks/reading_local_files.html#you-have-to-make-sure-that-pandas-parses-your-dates",
    "href": "notebooks/reading_local_files.html#you-have-to-make-sure-that-pandas-parses-your-dates",
    "title": "An Introduction to Jupyter Notebooks",
    "section": "",
    "text": "df_csv_parsed_dates = pd.read_csv('../data/food-waste-pilot/food-waste-pilot.csv', parse_dates=True, index_col=\"Collection Date\")\n\n\ndf_csv_parsed_dates.plot()",
    "crumbs": [
      "Home",
      "An Introduction to Jupyter Notebooks"
    ]
  },
  {
    "objectID": "notebooks/python_101.html",
    "href": "notebooks/python_101.html",
    "title": "Python 101",
    "section": "",
    "text": "This is an optional notebook to get you up to speed with Python in case you are new to Python or need a refresher. The material here is a crash course in Python; I highly recommend the official Python tutorial for a deeper dive. Consider reading this page in the Python docs for background on Python and bookmarking the glossary.\n\n\n\n\nNumbers in Python can be represented as integers (e.g. 5) or floats (e.g. 5.0). We can perform operations on them:\n\n5 + 6\n\n11\n\n\n\n2.5 / 3\n\n0.8333333333333334\n\n\n\n\n\nWe can check for equality giving us a Boolean:\n\n5 == 6\n\nFalse\n\n\n\n5 &lt; 6\n\nTrue\n\n\nThese statements can be combined with logical operators: not, and, or\n\n(5 &lt; 6) and not (5 == 6)\n\nTrue\n\n\n\nFalse or True\n\nTrue\n\n\n\nTrue or False\n\nTrue\n\n\n\n\n\nUsing strings, we can handle text in Python. These values must be surrounded in quotes — single ('...') is the standard, but double (\"...\") works as well:\n\n'hello'\n\n'hello'\n\n\nWe can also perform operations on strings. For example, we can see how long it is with len():\n\nlen('hello')\n\n5\n\n\nWe can select parts of the string by specifying the index. Note that in Python the 1st character is at index 0:\n\n'hello'[0]\n\n'h'\n\n\nWe can concatentate strings with +:\n\n'hello' + ' ' + 'world'\n\n'hello world'\n\n\nWe can check if characters are in the string with the in operator:\n\n'h' in 'hello'\n\nTrue\n\n\n\n\n\n\nNotice that just typing text causes an error. Errors in Python attempt to clue us in to what went wrong with our code. In this case, we have a NameError exception which tells us that 'hello' is not defined. This means that the Python interpreter looked for a variable named hello, but it didn’t find one.\n\nhello\n\nNameError: name 'hello' is not defined\n\n\nVariables give us a way to store data types. We define a variable using the variable_name = value syntax:\n\nx = 5\ny = 7\nx + y\n\n12\n\n\nThe variable name cannot contain spaces; we usually use _ instead. The best variable names are descriptive ones:\n\nbook_title = 'Hands-On Data Analysis with Pandas'\n\nVariables can be any data type. We can check which one it is with type(), which is a function (more on that later):\n\ntype(x)\n\nint\n\n\n\ntype(book_title)\n\nstr\n\n\nIf we need to see the value of a variable, we can print it using the print() function:\n\nprint(book_title)\n\nHands-On Data Analysis with Pandas\n\n\n\n\n\n\n\nWe can store a collection of items in a list:\n\n['hello', ' ', 'world']\n\n['hello', ' ', 'world']\n\n\nThe list can be stored in a variable. Note that the items in the list can be of different types:\n\nmy_list = ['hello', 3.8, True, 'Python']\ntype(my_list)\n\nlist\n\n\nWe can see how many elements are in the list with len():\n\nlen(my_list)\n\n4\n\n\nWe can also use the in operator to check if a value is in the list:\n\n'world' in my_list\n\nFalse\n\n\nWe can select items in the list just as we did with strings, by providing the index to select:\n\nmy_list[1]\n\n3.8\n\n\nPython also allows us to use negative values, so we can easily select the last one:\n\nmy_list[-1]\n\n'Python'\n\n\nAnother powerful feature of lists (and strings) is slicing. We can grab the middle 2 elements in the list:\n\nmy_list[1:3]\n\n[3.8, True]\n\n\n… or every other one:\n\nmy_list[::2]\n\n['hello', True]\n\n\nWe can even select the list in reverse:\n\nmy_list[::-1]\n\n['Python', True, 3.8, 'hello']\n\n\nNote: This syntax is [start:stop:step] where the selection is inclusive of the start index, but exclusive of the stop index. If start isn’t provided, 0 is used. If stop isn’t provided, the number of elements is used (4, in our case); this works because the stop is exclusive. If step isn’t provided, it is 1.\nWe can use the join() method on a string object to concatenate all the items of a list into single string. The string we call the join() method on will be used as the separator, here we separate with a pipe (|):\n\n'|'.join(['x', 'y', 'z'])\n\n'x|y|z'\n\n\n\n\n\nTuples are similar to lists; however, they can’t be modified after creation i.e. they are immutable. Instead of square brackets, we use parenthesis to create tuples:\n\nmy_tuple = ('a', 5)\ntype(my_tuple)\n\ntuple\n\n\n\nmy_tuple[0]\n\n'a'\n\n\nImmutable objects can’t be modified:\n\nmy_tuple[0] = 'b'\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\nWe can store mappings of key-value pairs using dictionaries:\n\nshopping_list = {\n    'veggies': ['spinach', 'kale', 'beets'],\n    'fruits': 'bananas',\n    'meat': 0    \n}\ntype(shopping_list)\n\ndict\n\n\nTo access the values associated with a specific key, we use the square bracket notation again:\n\nshopping_list['veggies']\n\n['spinach', 'kale', 'beets']\n\n\nWe can extract all of the keys with keys():\n\nshopping_list.keys()\n\ndict_keys(['veggies', 'fruits', 'meat'])\n\n\nWe can extract all of the values with values():\n\nshopping_list.values()\n\ndict_values([['spinach', 'kale', 'beets'], 'bananas', 0])\n\n\nFinally, we can call items() to get back pairs of (key, value) pairs:\n\nshopping_list.items()\n\ndict_items([('veggies', ['spinach', 'kale', 'beets']), ('fruits', 'bananas'), ('meat', 0)])\n\n\n\n\n\nA set is a collection of unique items; a common use is to remove duplicates from a list. These are written with curly braces also, but notice there is no key-value mapping:\n\nmy_set = {1, 1, 2, 'a'}\ntype(my_set)\n\nset\n\n\nHow many items are in this set?\n\nlen(my_set)\n\n3\n\n\nWe put in 4 items but the set only has 3 because duplicates are removed:\n\nmy_set\n\n{1, 2, 'a'}\n\n\nWe can check if a value is in the set:\n\n2 in my_set\n\nTrue\n\n\n\n\n\n\nWe can define functions to package up our code for reuse. We have already seen some functions: len(), type(), and print(). They are all functions that take arguments. Note that functions don’t need to accept arguments, in which case they are called without passing in anything (e.g. print() versus print(my_string)).\nAside: we can also create lists, sets, dictionaries, and tuples with functions: list(), set(), dict(), and tuple()\n\n\nWe use the def keyword to define functions. Let’s create a function called add() with 2 parameters, x and y, which will be the names the code in the function will use to refer to the arguments we pass in when calling it:\n\ndef add(x, y):\n    \"\"\"This is a docstring. It is used to explain how the code works and is optional (but encouraged).\"\"\"\n    # this is a comment; it allows us to annotate the code\n    print('Performing addition')\n    return x + y\n\nOnce we run the code above, our function is ready to use:\n\ntype(add)\n\nfunction\n\n\nLet’s add some numbers:\n\nadd(1, 2)\n\nPerforming addition\n\n\n3\n\n\n\n\n\nWe can store the result in a variable for later:\n\nresult = add(1, 2)\n\nPerforming addition\n\n\nNotice the print statement wasn’t captured in result. This variable will only have what the function returns. This is what the return line in the function definition did:\n\nresult\n\n3\n\n\nNote that functions don’t have to return anything. Consider print():\n\nprint_result = print('hello world')\n\nhello world\n\n\nIf we take a look at what we got back, we see it is a NoneType object:\n\ntype(print_result)\n\nNoneType\n\n\nIn Python, the value None represents null values. We can check if our variable is None:\n\nprint_result is None\n\nTrue\n\n\nWarning: make sure to use comparison operators (e.g. &gt;, &gt;=, &lt;, &lt;=, ==, !=) to compare to values other than None.\n\n\n\nNote that function arguments can be anything, even other functions. We will see several examples of this in the text.\nThe function we defined requires arguments. If we don’t provide them all, it will cause an error:\n\nadd(1)\n\nTypeError: add() missing 1 required positional argument: 'y'\n\n\nWe can use help() to check what arguments the function needs (notice the docstring ends up here):\n\nhelp(add)\n\nHelp on function add in module __main__:\n\nadd(x, y)\n    This is a docstring. It is used to explain how the code works and is optional (but encouraged).\n\n\n\nWe will also get errors if we pass in data types that add() can’t work with:\n\nadd(set(), set())\n\nPerforming addition\n\n\nTypeError: unsupported operand type(s) for +: 'set' and 'set'\n\n\nWe will discuss error handling in the text.\n\n\n\n\nSometimes we want to vary the path the code takes based on some criteria. For this we have if, elif, and else. We can use if on its own:\n\ndef make_positive(x):\n    \"\"\"Returns a positive x\"\"\"\n    if x &lt; 0:\n        x *= -1\n    return x\n\nCalling this function with negative input causes the code under the if statement to run:\n\nmake_positive(-1)\n\n1\n\n\nCalling this function with positive input skips the code under the if statement, keeping the number positive:\n\nmake_positive(2)\n\n2\n\n\nSometimes we need an else statement as well:\n\ndef add_or_subtract(operation, x, y):\n    if operation == 'add':\n        return x + y\n    else:\n        return x - y\n\nThis triggers the code under the if statement:\n\nadd_or_subtract('add', 1, 2)\n\n3\n\n\nSince the Boolean check in the if statement was False, this triggers the code under the else statement:\n\nadd_or_subtract('subtract', 1, 2)\n\n-1\n\n\nFor more complicated logic, we can also use elif. We can have any number of elif statements. Optionally, we can include else.\n\ndef calculate(operation, x, y):\n    if operation == 'add':\n        return x + y\n    elif operation == 'subtract':\n        return x - y\n    elif operation == 'multiply':\n        return x * y\n    elif operation == 'division':\n        return x / y\n    else:\n        print(\"This case hasn't been handled\")\n\nThe code keeps checking the conditions in the if statements from top to bottom until it finds multiply:\n\ncalculate('multiply', 3, 4)\n\n12\n\n\nThe code keeps checking the conditions in the if statements from top to bottom until it hits the else statement:\n\ncalculate('power', 3, 4)\n\nThis case hasn't been handled\n\n\n\n\n\n\n\nWith while loops, we can keep running code until some stopping condition is met:\n\ndone = False\nvalue = 2\nwhile not done:\n    print('Still going...', value)\n    value *= 2\n    if value &gt; 10:\n        done = True\n\nStill going... 2\nStill going... 4\nStill going... 8\n\n\nNote this can also be written as, by moving the condition to the while statement:\n\nvalue = 2\nwhile value &lt; 10:\n    print('Still going...', value)\n    value *= 2\n\nStill going... 2\nStill going... 4\nStill going... 8\n\n\n\n\n\nWith for loops, we can run our code for each element in a collection:\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\nWe can use for loops with lists, tuples, sets, and dictionaries as well:\n\nfor element in my_list:\n    print(element)\n\nhello\n3.8\nTrue\nPython\n\n\n\nfor key, value in shopping_list.items():\n    print('For', key, 'we need to buy', value)\n\nFor veggies we need to buy ['spinach', 'kale', 'beets']\nFor fruits we need to buy bananas\nFor meat we need to buy 0\n\n\nWith for loops, we don’t have to worry about checking if we have reached the stopping condition. Conversely, while loops can cause infinite loops if we don’t remember to update variables.\n\n\n\n\nWe have been working with the portion of Python that is available without importing additional functionality. The Python standard library that comes with the install of Python is broken up into several modules, but we often only need a few. We can import whatever we need: a module in the standard library, a 3rd-party library, or code that we wrote. This is done with an import statement:\n\nimport math\n\nprint(math.pi)\n\n3.141592653589793\n\n\nIf we only need a small piece from that module, we can do the following instead:\n\nfrom math import pi\n\nprint(pi)\n\n3.141592653589793\n\n\nWarning: anything you import is added to the namespace, so if you create a new variable/function/etc. with the same name it will overwrite the previous value. For this reason, we have to be careful with variable names e.g. if you name something sum, you won’t be able to add using the sum() built-in function anymore. Using notebooks or an IDE will help you avoid these issues with syntax highlighting.\n\n\n\nWe can use pip or conda to install packages, depending on how we created our virtual environment. We will walk through the commands to create virtual environments with conda. The environment MUST be activated before installing the packages for this text; otherwise, it’s possible they interfere with other projects on your machine or vice versa.\nTo install a package, we can use conda install &lt;package_name&gt; to download a package from the default conda channel. Optionally, we can provide a specific version to install conda install pandas==0.23.4. Even further, can define which channel that we install a package from for example we can install a package from the conda-forge channel by with conda install -c conda-forge pandas=0.23.4. Without that specification, we will get the most stable version. When we have many packages to install we will typically use a environment.yml or requirements.txt file: conda env update -f environment.yml from within your active environment or conda env update -n ENVNAME -f environment.yml if you are updating an update you are not actively in.\nNote: running conda env export ENVNAME &gt; environment.yml will send the list of platform-specific packages installed in the activate environment and their respective versions to the environment.yml file.\n\n\n\nSo far we have used Python as a functional programming language, but we also have the option to use it for object-oriented programming. You can think of a class as a way to group similar functionality together. Let’s create a calculator class which can handle mathematical operations for us. For this, we use the class keyword and define methods for taking actions on the calculator. These methods are functions that take self as the first argument. When calling them, we don’t pass in anything for that argument (example after this):\n\nclass Calculator:\n    \"\"\"This is the class docstring.\"\"\"\n    \n    def __init__(self):\n        \"\"\"This is a method and it is called when we create an object of type `Calculator`.\"\"\"\n        self.on = False\n        \n    def turn_on(self):\n        \"\"\"This method turns on the calculator.\"\"\"\n        self.on = True\n    \n    def add(self, x, y):\n        \"\"\"Perform addition if calculator is on\"\"\"\n        if self.on:\n            return x + y\n        else:\n            print('the calculator is not on')\n\nIn order to use the calculator, we need to instantiate an instance or object of type Calculator. Since the __init__() method has no parameters other than self, we don’t need to provide anything:\n\nmy_calculator = Calculator()\n\nLet’s try to add some numbers:\n\nmy_calculator.add(1, 2)\n\nthe calculator is not on\n\n\nOops!! The calculator is not on. Let’s turn it on:\n\nmy_calculator.turn_on()\n\nLet’s try again:\n\nmy_calculator.add(1, 2)\n\n3\n\n\nWe can access attributes on object with dot notation. In this example, the only attribute is on, and it is set in the __init__() method:\n\nmy_calculator.on\n\nTrue\n\n\nNote that we can also update attributes:\n\nmy_calculator.on = False\nmy_calculator.add(1, 2)\n\nthe calculator is not on\n\n\nFinally, we can use help() to get more information on the object:\n\nhelp(my_calculator)\n\nHelp on Calculator in module __main__ object:\n\nclass Calculator(builtins.object)\n |  This is the class docstring.\n |  \n |  Methods defined here:\n |  \n |  __init__(self)\n |      This is a method and it is called when we create an object of type `Calculator`.\n |  \n |  add(self, x, y)\n |      Perform addition if calculator is on\n |  \n |  turn_on(self)\n |      This method turns on the calculator.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n\n\n… and also for a method:\n\nhelp(my_calculator.add)\n\nHelp on method add in module __main__:\n\nadd(x, y) method of __main__.Calculator instance\n    Perform addition if calculator is on\n\n\n\n\n\n\nThis was a crash course in Python. This isn’t an exhaustive list of all of the features available to you.",
    "crumbs": [
      "Home",
      "Python 101"
    ]
  },
  {
    "objectID": "notebooks/python_101.html#basic-data-types",
    "href": "notebooks/python_101.html#basic-data-types",
    "title": "Python 101",
    "section": "",
    "text": "Numbers in Python can be represented as integers (e.g. 5) or floats (e.g. 5.0). We can perform operations on them:\n\n5 + 6\n\n11\n\n\n\n2.5 / 3\n\n0.8333333333333334\n\n\n\n\n\nWe can check for equality giving us a Boolean:\n\n5 == 6\n\nFalse\n\n\n\n5 &lt; 6\n\nTrue\n\n\nThese statements can be combined with logical operators: not, and, or\n\n(5 &lt; 6) and not (5 == 6)\n\nTrue\n\n\n\nFalse or True\n\nTrue\n\n\n\nTrue or False\n\nTrue\n\n\n\n\n\nUsing strings, we can handle text in Python. These values must be surrounded in quotes — single ('...') is the standard, but double (\"...\") works as well:\n\n'hello'\n\n'hello'\n\n\nWe can also perform operations on strings. For example, we can see how long it is with len():\n\nlen('hello')\n\n5\n\n\nWe can select parts of the string by specifying the index. Note that in Python the 1st character is at index 0:\n\n'hello'[0]\n\n'h'\n\n\nWe can concatentate strings with +:\n\n'hello' + ' ' + 'world'\n\n'hello world'\n\n\nWe can check if characters are in the string with the in operator:\n\n'h' in 'hello'\n\nTrue",
    "crumbs": [
      "Home",
      "Python 101"
    ]
  },
  {
    "objectID": "notebooks/python_101.html#variables",
    "href": "notebooks/python_101.html#variables",
    "title": "Python 101",
    "section": "",
    "text": "Notice that just typing text causes an error. Errors in Python attempt to clue us in to what went wrong with our code. In this case, we have a NameError exception which tells us that 'hello' is not defined. This means that the Python interpreter looked for a variable named hello, but it didn’t find one.\n\nhello\n\nNameError: name 'hello' is not defined\n\n\nVariables give us a way to store data types. We define a variable using the variable_name = value syntax:\n\nx = 5\ny = 7\nx + y\n\n12\n\n\nThe variable name cannot contain spaces; we usually use _ instead. The best variable names are descriptive ones:\n\nbook_title = 'Hands-On Data Analysis with Pandas'\n\nVariables can be any data type. We can check which one it is with type(), which is a function (more on that later):\n\ntype(x)\n\nint\n\n\n\ntype(book_title)\n\nstr\n\n\nIf we need to see the value of a variable, we can print it using the print() function:\n\nprint(book_title)\n\nHands-On Data Analysis with Pandas",
    "crumbs": [
      "Home",
      "Python 101"
    ]
  },
  {
    "objectID": "notebooks/python_101.html#collections-of-items",
    "href": "notebooks/python_101.html#collections-of-items",
    "title": "Python 101",
    "section": "",
    "text": "We can store a collection of items in a list:\n\n['hello', ' ', 'world']\n\n['hello', ' ', 'world']\n\n\nThe list can be stored in a variable. Note that the items in the list can be of different types:\n\nmy_list = ['hello', 3.8, True, 'Python']\ntype(my_list)\n\nlist\n\n\nWe can see how many elements are in the list with len():\n\nlen(my_list)\n\n4\n\n\nWe can also use the in operator to check if a value is in the list:\n\n'world' in my_list\n\nFalse\n\n\nWe can select items in the list just as we did with strings, by providing the index to select:\n\nmy_list[1]\n\n3.8\n\n\nPython also allows us to use negative values, so we can easily select the last one:\n\nmy_list[-1]\n\n'Python'\n\n\nAnother powerful feature of lists (and strings) is slicing. We can grab the middle 2 elements in the list:\n\nmy_list[1:3]\n\n[3.8, True]\n\n\n… or every other one:\n\nmy_list[::2]\n\n['hello', True]\n\n\nWe can even select the list in reverse:\n\nmy_list[::-1]\n\n['Python', True, 3.8, 'hello']\n\n\nNote: This syntax is [start:stop:step] where the selection is inclusive of the start index, but exclusive of the stop index. If start isn’t provided, 0 is used. If stop isn’t provided, the number of elements is used (4, in our case); this works because the stop is exclusive. If step isn’t provided, it is 1.\nWe can use the join() method on a string object to concatenate all the items of a list into single string. The string we call the join() method on will be used as the separator, here we separate with a pipe (|):\n\n'|'.join(['x', 'y', 'z'])\n\n'x|y|z'\n\n\n\n\n\nTuples are similar to lists; however, they can’t be modified after creation i.e. they are immutable. Instead of square brackets, we use parenthesis to create tuples:\n\nmy_tuple = ('a', 5)\ntype(my_tuple)\n\ntuple\n\n\n\nmy_tuple[0]\n\n'a'\n\n\nImmutable objects can’t be modified:\n\nmy_tuple[0] = 'b'\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\nWe can store mappings of key-value pairs using dictionaries:\n\nshopping_list = {\n    'veggies': ['spinach', 'kale', 'beets'],\n    'fruits': 'bananas',\n    'meat': 0    \n}\ntype(shopping_list)\n\ndict\n\n\nTo access the values associated with a specific key, we use the square bracket notation again:\n\nshopping_list['veggies']\n\n['spinach', 'kale', 'beets']\n\n\nWe can extract all of the keys with keys():\n\nshopping_list.keys()\n\ndict_keys(['veggies', 'fruits', 'meat'])\n\n\nWe can extract all of the values with values():\n\nshopping_list.values()\n\ndict_values([['spinach', 'kale', 'beets'], 'bananas', 0])\n\n\nFinally, we can call items() to get back pairs of (key, value) pairs:\n\nshopping_list.items()\n\ndict_items([('veggies', ['spinach', 'kale', 'beets']), ('fruits', 'bananas'), ('meat', 0)])\n\n\n\n\n\nA set is a collection of unique items; a common use is to remove duplicates from a list. These are written with curly braces also, but notice there is no key-value mapping:\n\nmy_set = {1, 1, 2, 'a'}\ntype(my_set)\n\nset\n\n\nHow many items are in this set?\n\nlen(my_set)\n\n3\n\n\nWe put in 4 items but the set only has 3 because duplicates are removed:\n\nmy_set\n\n{1, 2, 'a'}\n\n\nWe can check if a value is in the set:\n\n2 in my_set\n\nTrue",
    "crumbs": [
      "Home",
      "Python 101"
    ]
  },
  {
    "objectID": "notebooks/python_101.html#functions",
    "href": "notebooks/python_101.html#functions",
    "title": "Python 101",
    "section": "",
    "text": "We can define functions to package up our code for reuse. We have already seen some functions: len(), type(), and print(). They are all functions that take arguments. Note that functions don’t need to accept arguments, in which case they are called without passing in anything (e.g. print() versus print(my_string)).\nAside: we can also create lists, sets, dictionaries, and tuples with functions: list(), set(), dict(), and tuple()\n\n\nWe use the def keyword to define functions. Let’s create a function called add() with 2 parameters, x and y, which will be the names the code in the function will use to refer to the arguments we pass in when calling it:\n\ndef add(x, y):\n    \"\"\"This is a docstring. It is used to explain how the code works and is optional (but encouraged).\"\"\"\n    # this is a comment; it allows us to annotate the code\n    print('Performing addition')\n    return x + y\n\nOnce we run the code above, our function is ready to use:\n\ntype(add)\n\nfunction\n\n\nLet’s add some numbers:\n\nadd(1, 2)\n\nPerforming addition\n\n\n3\n\n\n\n\n\nWe can store the result in a variable for later:\n\nresult = add(1, 2)\n\nPerforming addition\n\n\nNotice the print statement wasn’t captured in result. This variable will only have what the function returns. This is what the return line in the function definition did:\n\nresult\n\n3\n\n\nNote that functions don’t have to return anything. Consider print():\n\nprint_result = print('hello world')\n\nhello world\n\n\nIf we take a look at what we got back, we see it is a NoneType object:\n\ntype(print_result)\n\nNoneType\n\n\nIn Python, the value None represents null values. We can check if our variable is None:\n\nprint_result is None\n\nTrue\n\n\nWarning: make sure to use comparison operators (e.g. &gt;, &gt;=, &lt;, &lt;=, ==, !=) to compare to values other than None.\n\n\n\nNote that function arguments can be anything, even other functions. We will see several examples of this in the text.\nThe function we defined requires arguments. If we don’t provide them all, it will cause an error:\n\nadd(1)\n\nTypeError: add() missing 1 required positional argument: 'y'\n\n\nWe can use help() to check what arguments the function needs (notice the docstring ends up here):\n\nhelp(add)\n\nHelp on function add in module __main__:\n\nadd(x, y)\n    This is a docstring. It is used to explain how the code works and is optional (but encouraged).\n\n\n\nWe will also get errors if we pass in data types that add() can’t work with:\n\nadd(set(), set())\n\nPerforming addition\n\n\nTypeError: unsupported operand type(s) for +: 'set' and 'set'\n\n\nWe will discuss error handling in the text.",
    "crumbs": [
      "Home",
      "Python 101"
    ]
  },
  {
    "objectID": "notebooks/python_101.html#control-flow-statements",
    "href": "notebooks/python_101.html#control-flow-statements",
    "title": "Python 101",
    "section": "",
    "text": "Sometimes we want to vary the path the code takes based on some criteria. For this we have if, elif, and else. We can use if on its own:\n\ndef make_positive(x):\n    \"\"\"Returns a positive x\"\"\"\n    if x &lt; 0:\n        x *= -1\n    return x\n\nCalling this function with negative input causes the code under the if statement to run:\n\nmake_positive(-1)\n\n1\n\n\nCalling this function with positive input skips the code under the if statement, keeping the number positive:\n\nmake_positive(2)\n\n2\n\n\nSometimes we need an else statement as well:\n\ndef add_or_subtract(operation, x, y):\n    if operation == 'add':\n        return x + y\n    else:\n        return x - y\n\nThis triggers the code under the if statement:\n\nadd_or_subtract('add', 1, 2)\n\n3\n\n\nSince the Boolean check in the if statement was False, this triggers the code under the else statement:\n\nadd_or_subtract('subtract', 1, 2)\n\n-1\n\n\nFor more complicated logic, we can also use elif. We can have any number of elif statements. Optionally, we can include else.\n\ndef calculate(operation, x, y):\n    if operation == 'add':\n        return x + y\n    elif operation == 'subtract':\n        return x - y\n    elif operation == 'multiply':\n        return x * y\n    elif operation == 'division':\n        return x / y\n    else:\n        print(\"This case hasn't been handled\")\n\nThe code keeps checking the conditions in the if statements from top to bottom until it finds multiply:\n\ncalculate('multiply', 3, 4)\n\n12\n\n\nThe code keeps checking the conditions in the if statements from top to bottom until it hits the else statement:\n\ncalculate('power', 3, 4)\n\nThis case hasn't been handled",
    "crumbs": [
      "Home",
      "Python 101"
    ]
  },
  {
    "objectID": "notebooks/python_101.html#loops",
    "href": "notebooks/python_101.html#loops",
    "title": "Python 101",
    "section": "",
    "text": "With while loops, we can keep running code until some stopping condition is met:\n\ndone = False\nvalue = 2\nwhile not done:\n    print('Still going...', value)\n    value *= 2\n    if value &gt; 10:\n        done = True\n\nStill going... 2\nStill going... 4\nStill going... 8\n\n\nNote this can also be written as, by moving the condition to the while statement:\n\nvalue = 2\nwhile value &lt; 10:\n    print('Still going...', value)\n    value *= 2\n\nStill going... 2\nStill going... 4\nStill going... 8\n\n\n\n\n\nWith for loops, we can run our code for each element in a collection:\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\nWe can use for loops with lists, tuples, sets, and dictionaries as well:\n\nfor element in my_list:\n    print(element)\n\nhello\n3.8\nTrue\nPython\n\n\n\nfor key, value in shopping_list.items():\n    print('For', key, 'we need to buy', value)\n\nFor veggies we need to buy ['spinach', 'kale', 'beets']\nFor fruits we need to buy bananas\nFor meat we need to buy 0\n\n\nWith for loops, we don’t have to worry about checking if we have reached the stopping condition. Conversely, while loops can cause infinite loops if we don’t remember to update variables.",
    "crumbs": [
      "Home",
      "Python 101"
    ]
  },
  {
    "objectID": "notebooks/python_101.html#imports",
    "href": "notebooks/python_101.html#imports",
    "title": "Python 101",
    "section": "",
    "text": "We have been working with the portion of Python that is available without importing additional functionality. The Python standard library that comes with the install of Python is broken up into several modules, but we often only need a few. We can import whatever we need: a module in the standard library, a 3rd-party library, or code that we wrote. This is done with an import statement:\n\nimport math\n\nprint(math.pi)\n\n3.141592653589793\n\n\nIf we only need a small piece from that module, we can do the following instead:\n\nfrom math import pi\n\nprint(pi)\n\n3.141592653589793\n\n\nWarning: anything you import is added to the namespace, so if you create a new variable/function/etc. with the same name it will overwrite the previous value. For this reason, we have to be careful with variable names e.g. if you name something sum, you won’t be able to add using the sum() built-in function anymore. Using notebooks or an IDE will help you avoid these issues with syntax highlighting.",
    "crumbs": [
      "Home",
      "Python 101"
    ]
  },
  {
    "objectID": "notebooks/python_101.html#installing-3rd-party-packages",
    "href": "notebooks/python_101.html#installing-3rd-party-packages",
    "title": "Python 101",
    "section": "",
    "text": "We can use pip or conda to install packages, depending on how we created our virtual environment. We will walk through the commands to create virtual environments with conda. The environment MUST be activated before installing the packages for this text; otherwise, it’s possible they interfere with other projects on your machine or vice versa.\nTo install a package, we can use conda install &lt;package_name&gt; to download a package from the default conda channel. Optionally, we can provide a specific version to install conda install pandas==0.23.4. Even further, can define which channel that we install a package from for example we can install a package from the conda-forge channel by with conda install -c conda-forge pandas=0.23.4. Without that specification, we will get the most stable version. When we have many packages to install we will typically use a environment.yml or requirements.txt file: conda env update -f environment.yml from within your active environment or conda env update -n ENVNAME -f environment.yml if you are updating an update you are not actively in.\nNote: running conda env export ENVNAME &gt; environment.yml will send the list of platform-specific packages installed in the activate environment and their respective versions to the environment.yml file.",
    "crumbs": [
      "Home",
      "Python 101"
    ]
  },
  {
    "objectID": "notebooks/python_101.html#classes",
    "href": "notebooks/python_101.html#classes",
    "title": "Python 101",
    "section": "",
    "text": "So far we have used Python as a functional programming language, but we also have the option to use it for object-oriented programming. You can think of a class as a way to group similar functionality together. Let’s create a calculator class which can handle mathematical operations for us. For this, we use the class keyword and define methods for taking actions on the calculator. These methods are functions that take self as the first argument. When calling them, we don’t pass in anything for that argument (example after this):\n\nclass Calculator:\n    \"\"\"This is the class docstring.\"\"\"\n    \n    def __init__(self):\n        \"\"\"This is a method and it is called when we create an object of type `Calculator`.\"\"\"\n        self.on = False\n        \n    def turn_on(self):\n        \"\"\"This method turns on the calculator.\"\"\"\n        self.on = True\n    \n    def add(self, x, y):\n        \"\"\"Perform addition if calculator is on\"\"\"\n        if self.on:\n            return x + y\n        else:\n            print('the calculator is not on')\n\nIn order to use the calculator, we need to instantiate an instance or object of type Calculator. Since the __init__() method has no parameters other than self, we don’t need to provide anything:\n\nmy_calculator = Calculator()\n\nLet’s try to add some numbers:\n\nmy_calculator.add(1, 2)\n\nthe calculator is not on\n\n\nOops!! The calculator is not on. Let’s turn it on:\n\nmy_calculator.turn_on()\n\nLet’s try again:\n\nmy_calculator.add(1, 2)\n\n3\n\n\nWe can access attributes on object with dot notation. In this example, the only attribute is on, and it is set in the __init__() method:\n\nmy_calculator.on\n\nTrue\n\n\nNote that we can also update attributes:\n\nmy_calculator.on = False\nmy_calculator.add(1, 2)\n\nthe calculator is not on\n\n\nFinally, we can use help() to get more information on the object:\n\nhelp(my_calculator)\n\nHelp on Calculator in module __main__ object:\n\nclass Calculator(builtins.object)\n |  This is the class docstring.\n |  \n |  Methods defined here:\n |  \n |  __init__(self)\n |      This is a method and it is called when we create an object of type `Calculator`.\n |  \n |  add(self, x, y)\n |      Perform addition if calculator is on\n |  \n |  turn_on(self)\n |      This method turns on the calculator.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n\n\n… and also for a method:\n\nhelp(my_calculator.add)\n\nHelp on method add in module __main__:\n\nadd(x, y) method of __main__.Calculator instance\n    Perform addition if calculator is on",
    "crumbs": [
      "Home",
      "Python 101"
    ]
  },
  {
    "objectID": "notebooks/python_101.html#next-steps",
    "href": "notebooks/python_101.html#next-steps",
    "title": "Python 101",
    "section": "",
    "text": "This was a crash course in Python. This isn’t an exhaustive list of all of the features available to you.",
    "crumbs": [
      "Home",
      "Python 101"
    ]
  },
  {
    "objectID": "notebooks/pandas_plotting_module.html",
    "href": "notebooks/pandas_plotting_module.html",
    "title": "The pandas.plotting module",
    "section": "",
    "text": "Pandas provides some extra plotting functions for some new plot types.\n\n\nIn this notebook, we will be working with Facebook’s stock price throughout 2018 (obtained using the stock_analysis package).\n\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfb = pd.read_csv(\n    '../data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True\n)\n\n\n\n\nEasily create scatter plots between all columns in the dataset:\n\nfrom pandas.plotting import scatter_matrix\nscatter_matrix(fb, figsize=(10, 10))\n\narray([[&lt;Axes: xlabel='open', ylabel='open'&gt;,\n        &lt;Axes: xlabel='high', ylabel='open'&gt;,\n        &lt;Axes: xlabel='low', ylabel='open'&gt;,\n        &lt;Axes: xlabel='close', ylabel='open'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='open'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='high'&gt;,\n        &lt;Axes: xlabel='high', ylabel='high'&gt;,\n        &lt;Axes: xlabel='low', ylabel='high'&gt;,\n        &lt;Axes: xlabel='close', ylabel='high'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='high'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='low'&gt;,\n        &lt;Axes: xlabel='high', ylabel='low'&gt;,\n        &lt;Axes: xlabel='low', ylabel='low'&gt;,\n        &lt;Axes: xlabel='close', ylabel='low'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='low'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='close'&gt;,\n        &lt;Axes: xlabel='high', ylabel='close'&gt;,\n        &lt;Axes: xlabel='low', ylabel='close'&gt;,\n        &lt;Axes: xlabel='close', ylabel='close'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='close'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='high', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='low', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='close', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='volume'&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nChanging the diagonal from histograms to KDE:\n\nscatter_matrix(fb, figsize=(10, 10), diagonal='kde')\n\narray([[&lt;Axes: xlabel='open', ylabel='open'&gt;,\n        &lt;Axes: xlabel='high', ylabel='open'&gt;,\n        &lt;Axes: xlabel='low', ylabel='open'&gt;,\n        &lt;Axes: xlabel='close', ylabel='open'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='open'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='high'&gt;,\n        &lt;Axes: xlabel='high', ylabel='high'&gt;,\n        &lt;Axes: xlabel='low', ylabel='high'&gt;,\n        &lt;Axes: xlabel='close', ylabel='high'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='high'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='low'&gt;,\n        &lt;Axes: xlabel='high', ylabel='low'&gt;,\n        &lt;Axes: xlabel='low', ylabel='low'&gt;,\n        &lt;Axes: xlabel='close', ylabel='low'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='low'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='close'&gt;,\n        &lt;Axes: xlabel='high', ylabel='close'&gt;,\n        &lt;Axes: xlabel='low', ylabel='close'&gt;,\n        &lt;Axes: xlabel='close', ylabel='close'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='close'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='high', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='low', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='close', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='volume'&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n\n\n\nLag plots let us see how the variable correlates with past observations of itself. Random data has no pattern:\n\nfrom pandas.plotting import lag_plot\nnp.random.seed(0) # make this repeatable\nlag_plot(pd.Series(np.random.random(size=200)))\n\n\n\n\n\n\n\n\nData with some level of correlation to itself (autocorrelation) may have patterns. Stock prices are highly autocorrelated:\n\nlag_plot(fb.close)\n\n\n\n\n\n\n\n\nThe default lag is 1, but we can alter this with the lag parameter. Let’s look at a 5 day lag (a week of trading activity):\n\nlag_plot(fb.close, lag=5)\n\n\n\n\n\n\n\n\n\n\n\nWe can use the autocorrelation plot to see if this relationship may be meaningful or is just noise. Random data will not have any significant autocorrelation (it stays within the bounds below):\n\nfrom pandas.plotting import autocorrelation_plot\nnp.random.seed(0) # make this repeatable\nautocorrelation_plot(pd.Series(np.random.random(size=200)))\n\n\n\n\n\n\n\n\nStock data, on the other hand, does have significant autocorrelation:\n\nautocorrelation_plot(fb.close)\n\n\n\n\n\n\n\n\n\n\n\nThis plot helps us understand the uncertainty in our summary statistics:\n\nfrom pandas.plotting import bootstrap_plot\nfig = bootstrap_plot(fb.volume, fig=plt.figure(figsize=(10, 6)))",
    "crumbs": [
      "Home",
      "The `pandas.plotting` module"
    ]
  },
  {
    "objectID": "notebooks/pandas_plotting_module.html#about-the-data",
    "href": "notebooks/pandas_plotting_module.html#about-the-data",
    "title": "The pandas.plotting module",
    "section": "",
    "text": "In this notebook, we will be working with Facebook’s stock price throughout 2018 (obtained using the stock_analysis package).",
    "crumbs": [
      "Home",
      "The `pandas.plotting` module"
    ]
  },
  {
    "objectID": "notebooks/pandas_plotting_module.html#setup",
    "href": "notebooks/pandas_plotting_module.html#setup",
    "title": "The pandas.plotting module",
    "section": "",
    "text": "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfb = pd.read_csv(\n    '../data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True\n)",
    "crumbs": [
      "Home",
      "The `pandas.plotting` module"
    ]
  },
  {
    "objectID": "notebooks/pandas_plotting_module.html#scatter-matrix",
    "href": "notebooks/pandas_plotting_module.html#scatter-matrix",
    "title": "The pandas.plotting module",
    "section": "",
    "text": "Easily create scatter plots between all columns in the dataset:\n\nfrom pandas.plotting import scatter_matrix\nscatter_matrix(fb, figsize=(10, 10))\n\narray([[&lt;Axes: xlabel='open', ylabel='open'&gt;,\n        &lt;Axes: xlabel='high', ylabel='open'&gt;,\n        &lt;Axes: xlabel='low', ylabel='open'&gt;,\n        &lt;Axes: xlabel='close', ylabel='open'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='open'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='high'&gt;,\n        &lt;Axes: xlabel='high', ylabel='high'&gt;,\n        &lt;Axes: xlabel='low', ylabel='high'&gt;,\n        &lt;Axes: xlabel='close', ylabel='high'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='high'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='low'&gt;,\n        &lt;Axes: xlabel='high', ylabel='low'&gt;,\n        &lt;Axes: xlabel='low', ylabel='low'&gt;,\n        &lt;Axes: xlabel='close', ylabel='low'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='low'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='close'&gt;,\n        &lt;Axes: xlabel='high', ylabel='close'&gt;,\n        &lt;Axes: xlabel='low', ylabel='close'&gt;,\n        &lt;Axes: xlabel='close', ylabel='close'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='close'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='high', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='low', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='close', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='volume'&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nChanging the diagonal from histograms to KDE:\n\nscatter_matrix(fb, figsize=(10, 10), diagonal='kde')\n\narray([[&lt;Axes: xlabel='open', ylabel='open'&gt;,\n        &lt;Axes: xlabel='high', ylabel='open'&gt;,\n        &lt;Axes: xlabel='low', ylabel='open'&gt;,\n        &lt;Axes: xlabel='close', ylabel='open'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='open'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='high'&gt;,\n        &lt;Axes: xlabel='high', ylabel='high'&gt;,\n        &lt;Axes: xlabel='low', ylabel='high'&gt;,\n        &lt;Axes: xlabel='close', ylabel='high'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='high'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='low'&gt;,\n        &lt;Axes: xlabel='high', ylabel='low'&gt;,\n        &lt;Axes: xlabel='low', ylabel='low'&gt;,\n        &lt;Axes: xlabel='close', ylabel='low'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='low'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='close'&gt;,\n        &lt;Axes: xlabel='high', ylabel='close'&gt;,\n        &lt;Axes: xlabel='low', ylabel='close'&gt;,\n        &lt;Axes: xlabel='close', ylabel='close'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='close'&gt;],\n       [&lt;Axes: xlabel='open', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='high', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='low', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='close', ylabel='volume'&gt;,\n        &lt;Axes: xlabel='volume', ylabel='volume'&gt;]], dtype=object)",
    "crumbs": [
      "Home",
      "The `pandas.plotting` module"
    ]
  },
  {
    "objectID": "notebooks/pandas_plotting_module.html#lag-plot",
    "href": "notebooks/pandas_plotting_module.html#lag-plot",
    "title": "The pandas.plotting module",
    "section": "",
    "text": "Lag plots let us see how the variable correlates with past observations of itself. Random data has no pattern:\n\nfrom pandas.plotting import lag_plot\nnp.random.seed(0) # make this repeatable\nlag_plot(pd.Series(np.random.random(size=200)))\n\n\n\n\n\n\n\n\nData with some level of correlation to itself (autocorrelation) may have patterns. Stock prices are highly autocorrelated:\n\nlag_plot(fb.close)\n\n\n\n\n\n\n\n\nThe default lag is 1, but we can alter this with the lag parameter. Let’s look at a 5 day lag (a week of trading activity):\n\nlag_plot(fb.close, lag=5)",
    "crumbs": [
      "Home",
      "The `pandas.plotting` module"
    ]
  },
  {
    "objectID": "notebooks/pandas_plotting_module.html#autocorrelation-plots",
    "href": "notebooks/pandas_plotting_module.html#autocorrelation-plots",
    "title": "The pandas.plotting module",
    "section": "",
    "text": "We can use the autocorrelation plot to see if this relationship may be meaningful or is just noise. Random data will not have any significant autocorrelation (it stays within the bounds below):\n\nfrom pandas.plotting import autocorrelation_plot\nnp.random.seed(0) # make this repeatable\nautocorrelation_plot(pd.Series(np.random.random(size=200)))\n\n\n\n\n\n\n\n\nStock data, on the other hand, does have significant autocorrelation:\n\nautocorrelation_plot(fb.close)",
    "crumbs": [
      "Home",
      "The `pandas.plotting` module"
    ]
  },
  {
    "objectID": "notebooks/pandas_plotting_module.html#bootstrap-plot",
    "href": "notebooks/pandas_plotting_module.html#bootstrap-plot",
    "title": "The pandas.plotting module",
    "section": "",
    "text": "This plot helps us understand the uncertainty in our summary statistics:\n\nfrom pandas.plotting import bootstrap_plot\nfig = bootstrap_plot(fb.volume, fig=plt.figure(figsize=(10, 6)))",
    "crumbs": [
      "Home",
      "The `pandas.plotting` module"
    ]
  },
  {
    "objectID": "notebooks/making_dataframes_from_api_requests.html",
    "href": "notebooks/making_dataframes_from_api_requests.html",
    "title": "Making Pandas DataFrames from API Requests",
    "section": "",
    "text": "Making Pandas DataFrames from API Requests\nIn this example, we will use the U.S. Geological Survey’s API to grab a JSON object of earthquake data and convert it to a pandas.DataFrame.\nUSGS API: https://earthquake.usgs.gov/fdsnws/event/1/\n\nGet Data from API\n\nimport datetime as dt\nimport pandas as pd\nimport requests\n\nyesterday = dt.date.today() - dt.timedelta(days=1)\napi = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\npayload = {\n    'format': 'geojson',\n    'starttime': yesterday - dt.timedelta(days=30),\n    'endtime': yesterday\n}\nresponse = requests.get(api, params=payload)\n\n# let's make sure the request was OK\nresponse.status_code\n\n200\n\n\nResponse of 200 means OK, so we can pull the data out of the result. Since we asked the API for a JSON payload, we can extract it from the response with the json() method.\n\n\nIsolate the Data from the JSON Response\nWe need to check the structures of the response data to know where our data is.\n\nearthquake_json = response.json()\nearthquake_json.keys()\n\ndict_keys(['type', 'metadata', 'features', 'bbox'])\n\n\nThe USGS API provides information about our request in the metadata key. Note that your result will be different, regardless of the date range you chose, because the API includes a timestamp for when the data was pulled:\n\nearthquake_json['metadata']\n\n{'generated': 1686247399000,\n 'url': 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2023-05-08&endtime=2023-06-07',\n 'title': 'USGS Earthquakes',\n 'status': 200,\n 'api': '1.14.0',\n 'count': 11706}\n\n\nEach element in the JSON array features is a row of data for our dataframe.\n\ntype(earthquake_json['features'])\n\nlist\n\n\nYour data will be different depending on the date you run this.\n\nearthquake_json['features'][0]\n\n{'type': 'Feature',\n 'properties': {'mag': 0.5,\n  'place': '17km SE of Anza, CA',\n  'time': 1686095356120,\n  'updated': 1686178472172,\n  'tz': None,\n  'url': 'https://earthquake.usgs.gov/earthquakes/eventpage/ci40479680',\n  'detail': 'https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=ci40479680&format=geojson',\n  'felt': None,\n  'cdi': None,\n  'mmi': None,\n  'alert': None,\n  'status': 'reviewed',\n  'tsunami': 0,\n  'sig': 4,\n  'net': 'ci',\n  'code': '40479680',\n  'ids': ',ci40479680,',\n  'sources': ',ci,',\n  'types': ',nearby-cities,origin,phase-data,scitech-link,',\n  'nst': 21,\n  'dmin': 0.07016,\n  'rms': 0.26,\n  'gap': 86,\n  'magType': 'ml',\n  'type': 'earthquake',\n  'title': 'M 0.5 - 17km SE of Anza, CA'},\n 'geometry': {'type': 'Point', 'coordinates': [-116.5405, 33.447, 11.37]},\n 'id': 'ci40479680'}\n\n\n\n\nConvert to DataFrame\nWe need to grab the properties section out of every entry in the features JSON array to create our dataframe.\n\nearthquake_properties_data = [\n    quake['properties'] for quake in earthquake_json['features']\n]\ndf = pd.DataFrame(earthquake_properties_data)\ndf.head()\n\n\n\n\n\n\n\n\nmag\nplace\ntime\nupdated\ntz\nurl\ndetail\nfelt\ncdi\nmmi\n...\nids\nsources\ntypes\nnst\ndmin\nrms\ngap\nmagType\ntype\ntitle\n\n\n\n\n0\n0.50\n17km SE of Anza, CA\n1686095356120\n1686178472172\nNone\nhttps://earthquake.usgs.gov/earthquakes/eventp...\nhttps://earthquake.usgs.gov/fdsnws/event/1/que...\nNaN\nNaN\nNaN\n...\n,ci40479680,\n,ci,\n,nearby-cities,origin,phase-data,scitech-link,\n21.0\n0.070160\n0.26\n86.0\nml\nearthquake\nM 0.5 - 17km SE of Anza, CA\n\n\n1\n1.24\n6 km W of Blanchard, Oklahoma\n1686095288900\n1686140204481\nNone\nhttps://earthquake.usgs.gov/earthquakes/eventp...\nhttps://earthquake.usgs.gov/fdsnws/event/1/que...\n0.0\n1.0\nNaN\n...\n,ok2023lavp,\n,ok,\n,dyfi,origin,phase-data,\n64.0\n0.089083\n0.44\n42.0\nml\nearthquake\nM 1.2 - 6 km W of Blanchard, Oklahoma\n\n\n2\n5.20\nsoutheast of the Loyalty Islands\n1686094693572\n1686095577040\nNone\nhttps://earthquake.usgs.gov/earthquakes/eventp...\nhttps://earthquake.usgs.gov/fdsnws/event/1/que...\nNaN\nNaN\nNaN\n...\n,us7000k6rg,\n,us,\n,origin,phase-data,\n32.0\n2.755000\n0.82\n85.0\nmb\nearthquake\nM 5.2 - southeast of the Loyalty Islands\n\n\n3\n0.24\n10 km NNE of Government Camp, Oregon\n1686094611590\n1686118209350\nNone\nhttps://earthquake.usgs.gov/earthquakes/eventp...\nhttps://earthquake.usgs.gov/fdsnws/event/1/que...\nNaN\nNaN\nNaN\n...\n,uw61931036,\n,uw,\n,origin,phase-data,\n3.0\n0.032620\n0.04\n239.0\nml\nearthquake\nM 0.2 - 10 km NNE of Government Camp, Oregon\n\n\n4\n-0.01\n8 km NNE of Government Camp, Oregon\n1686094117310\n1686117953180\nNone\nhttps://earthquake.usgs.gov/earthquakes/eventp...\nhttps://earthquake.usgs.gov/fdsnws/event/1/que...\nNaN\nNaN\nNaN\n...\n,uw61931031,\n,uw,\n,origin,phase-data,\n4.0\n0.011510\n0.04\n173.0\nml\nearthquake\nM 0.0 - 8 km NNE of Government Camp, Oregon\n\n\n\n\n5 rows × 26 columns\n\n\n\n\n\n(Optional) Write Data to CSV\n\ndf.to_csv('earthquakes.csv', index=False)",
    "crumbs": [
      "Home",
      "Making Pandas DataFrames from API Requests"
    ]
  },
  {
    "objectID": "notebooks/introduction_to_data_analysis.html",
    "href": "notebooks/introduction_to_data_analysis.html",
    "title": "Introduction to Data Analysis",
    "section": "",
    "text": "This notebook serves as a summary of the fundamentals. For a Python crash-course/refresher, work through the python_101.ipynb notebook.\n\n\n\nimport sys\nsys.path.append('../src/')\nsys.dont_write_bytecode = True\n\nimport stats_viz\n\n\n\n\nWhen conducting a data analysis, we will move back and forth between four main processes:\n\nData Collection: Every analysis starts with collecting data. We can collect data from a variety of sources, including databases, APIs, flat files, and the Internet.\nData Wrangling: After we have our data, we need to prepare it for our analysis. This may involve reshaping it, changing data types, handling missing values, and/or aggregating it.\nExploratory Data Analysis (EDA): We can use visualizations to explore our data and summarize it. During this time, we will also begin exploring the data by looking at its structure, format, and summary statistics.\nDrawing Conclusions: After we have thoroughly explored our data, we can try to draw conclusions or model it.\n\n\n\n\nAs this is an overview of statistics, we will discuss some concepts. By no means is this exhaustive.\n\n\nSome resampling (sampling from the sample) techniques you will see: - simple random sampling: pick with a random number generator - stratified random sampling: randomly pick preserving the proportion of groups in the data - bootstrapping: sampling with replacement (more info: YouTube video and Wikipedia article)\n\n\n\nWe use descriptive statistics to describe the data. The data we work with is usually a sample taken from the population. The statistics we will discuss here are referred to as sample statistics because they are calculated on the sample and can be used as estimators for the population parameters.\n\n\nThree common ways to describe the central tendency of a distribution are mean, median, and mode. ##### Mean The sample mean is an estimator for the population mean (\\(\\mu\\)) and is defined as:\n\\[\\bar{x} = \\frac{\\sum_{1}^{n} x_i}{n}\\] ##### Median The median represents the 50th percentile of our data; this means that 50% of the values are greater than the median and 50% are less than the median. It is calculated by taking the middle value from an ordered list of values.\n\n\nThe mode is the most common value in the data. We can use it to describe categorical data or, for continuous data, the shape of the distribution:\n\nax = stats_viz.different_modal_plots()\n\n\n\n\n\n\n\n\n\n\n\n\nMeasures of spread tell us how the data is dispersed; this will indicate how thin (low dispersion) or wide (very spread out) our distribution is.\n\n\nThe range is the distance between the smallest value (minimum) and the largest value (maximum):\n\\[range = max(X) - min(X)\\]\n\n\n\nThe variance describes how far apart observations are spread out from their average value (the mean). When calculating the sample variance, we divide by n - 1 instead of n to account for using the sample mean (\\(\\bar{x}\\)):\n\\[s^2 = \\frac{\\sum_{1}^{n} (x_i - \\bar{x})^2}{n - 1}\\]\nThis is referred to as Bessel’s correction and is applied to get an unbiased estimator of the population variance.\nNote that this will be in units-squared of whatever was being measured.\n\n\n\nThe standard deviation is the square root of the variance, giving us a measure in the same units as our data. The sample standard deviation is calculated as follows:\n\\[s = \\sqrt{\\frac{\\sum_{1}^{n} (x_i - \\bar{x})^2}{n - 1}} = \\sqrt{s^2}\\]\n\nax = stats_viz.effect_of_std_dev()\n\n\n\n\n\n\n\n\nNote that \\(\\sigma^2\\) is the population variance and \\(\\sigma\\) is the population standard deviation.\n\n\n\nThe coefficient of variation (CV) gives us a unitless ratio of the standard deviation to the mean. Since, it has no units we can compare dispersion across datasets:\n\\[CV = \\frac{s}{\\bar{x}}\\]\n\n\n\nThe interquartile range (IQR) gives us the spread of data around the median and quantifies how much dispersion we have in the middle 50% of our distribution:\n\\[IQR = Q_3 - Q_1\\]\n\n\n\nThe quartile coefficient of dispersion also is a unitless statistic for comparing datasets. However, it uses the median as the measure of center. It is calculated by dividing the semi-quartile range (half the IQR) by the midhinge (midpoint between the first and third quartiles):\n\\[QCD = \\frac{\\frac{Q_3 - Q_1}{2}}{\\frac{Q_1 + Q_3}{2}} = \\frac{Q_3 - Q_1}{Q_3 + Q_1}\\]\n\n\n\n\nThe 5-number summary provides 5 descriptive statistics that summarize our data:\n\n\n\n\nQuartile\nStatistic\nPercentile\n\n\n\n\n1.\n\\(Q_0\\)\nminimum\n\\(0^{th}\\)\n\n\n2.\n\\(Q_1\\)\nN/A\n\\(25^{th}\\)\n\n\n3.\n\\(Q_2\\)\nmedian\n\\(50^{th}\\)\n\n\n4.\n\\(Q_3\\)\nN/A\n\\(75^{th}\\)\n\n\n5.\n\\(Q_4\\)\nmaximum\n\\(100^{th}\\)\n\n\n\nThis summary can be visualized using a box plot (also called box-and-whisker plot). The box has an upper bound of \\(Q_3\\) and a lower bound of \\(Q_1\\). The median will be a line somewhere in this box. The whiskers extend from the box towards the minimum/maximum. For our purposes, they will extend to \\(Q_3 + 1.5 \\times IQR\\) and \\(Q_1 - 1.5 \\times IQR\\) and anything beyond will be represented as individual points for outliers:\n\nax = stats_viz.example_boxplot()\n\n\n\n\n\n\n\n\nThe box plot doesn’t show us how the data is distributed within the quartiles. To get a better sense of the distribution, we can use a histogram, which will show us the amount of observations that fall into equal-width bins. We can vary the number of bins to use, but be aware that this can change our impression of what the distribution appears to be:\n\nax = stats_viz.example_histogram()\n\n\n\n\n\n\n\n\nWe can also visualize the distribution using a kernel density estimate (KDE). This will estimate the probability density function (PDF). This function shows how probability is distributed over the values. Higher values of the PDF mean higher likelihoods:\n\nax = stats_viz.example_kde()\n\n\n\n\n\n\n\n\nNote that both the KDE and histogram estimate the distribution:\n\nax = stats_viz.hist_and_kde()\n\n\n\n\n\n\n\n\nSkewed distributions have more observations on one side. The mean will be less than the median with negative skew, while the opposite is true of positive skew:\n\nax = stats_viz.skew_examples()\n\n\n\n\n\n\n\n\nWe can use the cumulative distribution function (CDF) to find probabilities of getting values within a certain range. The CDF is the integral of the PDF:\n\\[CDF = F(x) = \\int_{-\\infty}^{x} f(t) dt\\]\nNote that \\(f(t)\\) is the PDF and \\(\\int_{-\\infty}^{\\infty} f(t) dt = 1\\).\nThe probability of the random variable \\(X\\) being less than or equal to the specific value of \\(x\\) is denoted as \\(P(X ≤ x)\\). Note that for a continuous random variable the probability of it being exactly \\(x\\) is zero.\nLet’s look at the estimate of the CDF from the sample data we used for the box plot, called the empirical cumulative distribution function (ECDF):\n\nax = stats_viz.cdf_example()\n\n\n\n\n\n\n\n\nWe can find any range we want if we use some algebra as in the rightmost subplot above.\n\n\n\n\nGaussian (normal) distribution: looks like a bell curve and is parameterized by its mean (μ) and standard deviation (σ). Many things in nature happen to follow the normal distribution, like heights. Note that testing if a distribution is normal is not trivial. Written as \\(N(\\mu, \\sigma)\\).\nPoisson distribution: discrete distribution that is often used to model arrivals. Parameterized by its mean, lambda (λ). Written as \\(Pois(\\lambda)\\).\nExponential distribution: can be used to model the time between arrivals. Parameterized by its mean, lambda (λ). Written as \\(Exp(\\lambda)\\).\nUniform distribution: places equal likelihood on each value within its bounds (a and b). We often use this for random number generation. Written as \\(U(a, b)\\).\nBernoulli distribution: When we pick a random number to simulate a single success/failure outcome, it is called a Bernoulli trial. This is parameterized by the probability of success (p). Written as \\(Bernoulli(p)\\).\nBinomial distribution: When we run the same experiment n times, the total number of successes is then a binomial random variable. Written as \\(B(n, p)\\).\n\nWe can visualize both discrete and continuous distributions; however, discrete distributions give us a probability mass function (PMF) instead of a PDF:\n\nax = stats_viz.common_dists()\n\n\n\n\n\n\n\n\n\n\n\nIn order to compare variables from different distributions, we would have to scale the data, which we could do with the range by using min-max scaling:\n\\[x_{scaled}=\\frac{x - min(X)}{range(X)}\\]\nAnother way is to use a Z-score to standardize the data:\n\\[z_i = \\frac{x_i - \\bar{x}}{s}\\]\n\n\n\nThe covariance is a statistic for quantifying the relationship between variables by showing how one variable changes with respect to another (also referred to as their joint variance):\n\\[cov(X, Y) = E[(X-E[X])(Y-E[Y])]\\]\nE[X] is the expectation of the random variable X (its long-run average).\nThe sign of the covariance gives us the direction of the relationship, but we need the magnitude as well. For that, we calculate the Pearson correlation coefficient (\\(\\rho\\)):\n\\[\\rho_{X, Y} = \\frac{cov(X, Y)}{s_X s_Y}\\]\nExamples:\n\nax = stats_viz.correlation_coefficient_examples()\n\n\n\n\n\n\n\n\nFrom left to right: no correlation, weak negative correlation, strong positive correlation, and nearly perfect negative correlation.\nOften, it is more informative to use scatter plots to check for relationships between variables. This is because the correlation may be strong, but the relationship may not be linear:\n\nax = stats_viz.non_linear_relationships()\n\n\n\n\n\n\n\n\nRemember, correlation does not imply causation. While we may find a correlation between X and Y, it does not mean that X causes Y or Y causes X. It is possible there is some Z that causes both or that X causes some intermediary event that causes Y — it could even be a coincidence. Be sure to check out Tyler Vigen’s Spurious Correlations blog for some interesting correlations.\n\n\n\nNot only can our correlation coefficients be misleading, but so can summary statistics. Anscombe’s quartet is a collection of four different datasets that have identical summary statistics and correlation coefficients, however, when plotted, it is obvious they are not similar:\n\nax = stats_viz.anscombes_quartet()\n\n\n\n\n\n\n\n\nAnother example of this is the Datasaurus Dozen:\n\nax = stats_viz.datasaurus_dozen()\n\n\n\n\n\n\n\n\n\n\n\n\nSay our favorite ice cream shop has asked us to help predict how many ice creams they can expect to sell on a given day. They are convinced that the temperature outside has strong influence on their sales, so they collected data on the number of ice creams sold at a given temperature. We agree to help them, and the first thing we do is make a scatter plot of the data they gave us:\n\nax = stats_viz.example_scatter_plot()\n\n\n\n\n\n\n\n\nWe can observe an upward trend in the scatter plot: more ice creams are sold at higher temperatures. In order to help out the ice cream shop, though, we need to find a way to make predictions from this data. We can use a technique called regression to model the relationship between temperature and ice cream sales with an equation:\n\nax = stats_viz.example_regression()\n\n\n\n\n\n\n\n\nWe can use the resulting equation to make predictions for the number of ice creams sold at various temperatures. However, we must keep in mind if we are interpolating or extrapolating. If the temperature value we are using for prediction is within the range of the original data we used to build our regression model, then we are interpolating (solid portion of the red line). On the other hand, if the temperature is beyond the values in the original data, we are extrapolating, which is very dangerous, since we can’t assume the pattern continues indefinitely in each direction (dotted portion of the line). Extremely hot temperatures may cause people to stay inside, meaning no ice creams will be sold, while the equation indicates record-high sales.\nForecasting is a type of prediction for time series. In a process called time series decomposition, time series is decomposed into a trend component, a seasonality component, and a cyclical component. These components can be combined in an additive or multiplicative fashion:\n\nax = stats_viz.time_series_decomposition_example()\n\n\n\n\n\n\n\n\nThe trend component describes the behavior of the time series in the long-term without accounting for the seasonal or cyclical effects. Using the trend, we can make broad statements about the time series in the long-run, such as: the population of Earth is increasing or the value of a stock is stagnating. Seasonality of a time series explains the systematic and calendar-related movements of a time series. For example, the number of ice cream trucks on the streets of New York City is high in the summer and drops to nothing in the winter; this pattern repeats every year regardless of whether the actual amount each summer is the same. Lastly, the cyclical component accounts for anything else unexplained or irregular with the time series; this could be something like a hurricane driving the number of ice cream trucks down in the short-term because it isn’t safe to be outside. This component is difficult to anticipate with a forecast due to its unexpected nature.\nWhen making models to forecast time series, some common methods include ARIMA-family methods and exponential smoothing. ARIMA stands for autoregressive (AR), integrated (I), moving average (MA). Autoregressive models take advantage of the fact that an observation at time \\(t\\) is correlated to a previous observation, for example at time \\(t - 1\\). Note that not all time series are autoregressive. The integrated component concerns the differenced data, or the change in the data from one time to another. Lastly, the moving average component uses a sliding window to average the last \\(x\\) observations where \\(x\\) is the length of the sliding window.\nThe moving average puts equal weight on each time period in the past involved in the calculation. In practice, this isn’t always a realistic expectation of our data. Sometimes all past values are important, but they vary in their influence on future data points. For these cases, we can use exponential smoothing, which allows us to put more weight on more recent values and less weight on values further away from what we are predicting.\n\n\n\nInferential statistics deals with inferring or deducing things from the sample data we have in order to make statements about the population as a whole. Before doing so, we need to know whether we conducted an observational study or an experiment. An observational study can’t be used to determine causation because we can’t control for everything. An experiment on the other hand is controlled.\nRemember that the sample statistics we discussed earlier are estimators for the population parameters. Our estimators need confidence intervals, which provide a point estimate and a margin of error around it. This is the range that the true population parameter will be in at a certain confidence level. At the 95% confidence level, 95% of the confidence intervals calculated from random samples of the population contain the true population parameter.\nWe also have the option of using hypothesis testing. First, we define a null hypothesis (say the true population mean is 0), then we determine a significance level (1 - confidence level), which is the probability of rejecting the null hypothesis when it is true. Our result is statistically significant if the value for the null hypothesis is outside the confidence interval. More info.",
    "crumbs": [
      "Home",
      "Introduction to Data Analysis"
    ]
  },
  {
    "objectID": "notebooks/introduction_to_data_analysis.html#setup",
    "href": "notebooks/introduction_to_data_analysis.html#setup",
    "title": "Introduction to Data Analysis",
    "section": "",
    "text": "import sys\nsys.path.append('../src/')\nsys.dont_write_bytecode = True\n\nimport stats_viz",
    "crumbs": [
      "Home",
      "Introduction to Data Analysis"
    ]
  },
  {
    "objectID": "notebooks/introduction_to_data_analysis.html#fundamentals-of-data-analysis",
    "href": "notebooks/introduction_to_data_analysis.html#fundamentals-of-data-analysis",
    "title": "Introduction to Data Analysis",
    "section": "",
    "text": "When conducting a data analysis, we will move back and forth between four main processes:\n\nData Collection: Every analysis starts with collecting data. We can collect data from a variety of sources, including databases, APIs, flat files, and the Internet.\nData Wrangling: After we have our data, we need to prepare it for our analysis. This may involve reshaping it, changing data types, handling missing values, and/or aggregating it.\nExploratory Data Analysis (EDA): We can use visualizations to explore our data and summarize it. During this time, we will also begin exploring the data by looking at its structure, format, and summary statistics.\nDrawing Conclusions: After we have thoroughly explored our data, we can try to draw conclusions or model it.",
    "crumbs": [
      "Home",
      "Introduction to Data Analysis"
    ]
  },
  {
    "objectID": "notebooks/introduction_to_data_analysis.html#statistical-foundations",
    "href": "notebooks/introduction_to_data_analysis.html#statistical-foundations",
    "title": "Introduction to Data Analysis",
    "section": "",
    "text": "As this is an overview of statistics, we will discuss some concepts. By no means is this exhaustive.\n\n\nSome resampling (sampling from the sample) techniques you will see: - simple random sampling: pick with a random number generator - stratified random sampling: randomly pick preserving the proportion of groups in the data - bootstrapping: sampling with replacement (more info: YouTube video and Wikipedia article)\n\n\n\nWe use descriptive statistics to describe the data. The data we work with is usually a sample taken from the population. The statistics we will discuss here are referred to as sample statistics because they are calculated on the sample and can be used as estimators for the population parameters.\n\n\nThree common ways to describe the central tendency of a distribution are mean, median, and mode. ##### Mean The sample mean is an estimator for the population mean (\\(\\mu\\)) and is defined as:\n\\[\\bar{x} = \\frac{\\sum_{1}^{n} x_i}{n}\\] ##### Median The median represents the 50th percentile of our data; this means that 50% of the values are greater than the median and 50% are less than the median. It is calculated by taking the middle value from an ordered list of values.\n\n\nThe mode is the most common value in the data. We can use it to describe categorical data or, for continuous data, the shape of the distribution:\n\nax = stats_viz.different_modal_plots()\n\n\n\n\n\n\n\n\n\n\n\n\nMeasures of spread tell us how the data is dispersed; this will indicate how thin (low dispersion) or wide (very spread out) our distribution is.\n\n\nThe range is the distance between the smallest value (minimum) and the largest value (maximum):\n\\[range = max(X) - min(X)\\]\n\n\n\nThe variance describes how far apart observations are spread out from their average value (the mean). When calculating the sample variance, we divide by n - 1 instead of n to account for using the sample mean (\\(\\bar{x}\\)):\n\\[s^2 = \\frac{\\sum_{1}^{n} (x_i - \\bar{x})^2}{n - 1}\\]\nThis is referred to as Bessel’s correction and is applied to get an unbiased estimator of the population variance.\nNote that this will be in units-squared of whatever was being measured.\n\n\n\nThe standard deviation is the square root of the variance, giving us a measure in the same units as our data. The sample standard deviation is calculated as follows:\n\\[s = \\sqrt{\\frac{\\sum_{1}^{n} (x_i - \\bar{x})^2}{n - 1}} = \\sqrt{s^2}\\]\n\nax = stats_viz.effect_of_std_dev()\n\n\n\n\n\n\n\n\nNote that \\(\\sigma^2\\) is the population variance and \\(\\sigma\\) is the population standard deviation.\n\n\n\nThe coefficient of variation (CV) gives us a unitless ratio of the standard deviation to the mean. Since, it has no units we can compare dispersion across datasets:\n\\[CV = \\frac{s}{\\bar{x}}\\]\n\n\n\nThe interquartile range (IQR) gives us the spread of data around the median and quantifies how much dispersion we have in the middle 50% of our distribution:\n\\[IQR = Q_3 - Q_1\\]\n\n\n\nThe quartile coefficient of dispersion also is a unitless statistic for comparing datasets. However, it uses the median as the measure of center. It is calculated by dividing the semi-quartile range (half the IQR) by the midhinge (midpoint between the first and third quartiles):\n\\[QCD = \\frac{\\frac{Q_3 - Q_1}{2}}{\\frac{Q_1 + Q_3}{2}} = \\frac{Q_3 - Q_1}{Q_3 + Q_1}\\]\n\n\n\n\nThe 5-number summary provides 5 descriptive statistics that summarize our data:\n\n\n\n\nQuartile\nStatistic\nPercentile\n\n\n\n\n1.\n\\(Q_0\\)\nminimum\n\\(0^{th}\\)\n\n\n2.\n\\(Q_1\\)\nN/A\n\\(25^{th}\\)\n\n\n3.\n\\(Q_2\\)\nmedian\n\\(50^{th}\\)\n\n\n4.\n\\(Q_3\\)\nN/A\n\\(75^{th}\\)\n\n\n5.\n\\(Q_4\\)\nmaximum\n\\(100^{th}\\)\n\n\n\nThis summary can be visualized using a box plot (also called box-and-whisker plot). The box has an upper bound of \\(Q_3\\) and a lower bound of \\(Q_1\\). The median will be a line somewhere in this box. The whiskers extend from the box towards the minimum/maximum. For our purposes, they will extend to \\(Q_3 + 1.5 \\times IQR\\) and \\(Q_1 - 1.5 \\times IQR\\) and anything beyond will be represented as individual points for outliers:\n\nax = stats_viz.example_boxplot()\n\n\n\n\n\n\n\n\nThe box plot doesn’t show us how the data is distributed within the quartiles. To get a better sense of the distribution, we can use a histogram, which will show us the amount of observations that fall into equal-width bins. We can vary the number of bins to use, but be aware that this can change our impression of what the distribution appears to be:\n\nax = stats_viz.example_histogram()\n\n\n\n\n\n\n\n\nWe can also visualize the distribution using a kernel density estimate (KDE). This will estimate the probability density function (PDF). This function shows how probability is distributed over the values. Higher values of the PDF mean higher likelihoods:\n\nax = stats_viz.example_kde()\n\n\n\n\n\n\n\n\nNote that both the KDE and histogram estimate the distribution:\n\nax = stats_viz.hist_and_kde()\n\n\n\n\n\n\n\n\nSkewed distributions have more observations on one side. The mean will be less than the median with negative skew, while the opposite is true of positive skew:\n\nax = stats_viz.skew_examples()\n\n\n\n\n\n\n\n\nWe can use the cumulative distribution function (CDF) to find probabilities of getting values within a certain range. The CDF is the integral of the PDF:\n\\[CDF = F(x) = \\int_{-\\infty}^{x} f(t) dt\\]\nNote that \\(f(t)\\) is the PDF and \\(\\int_{-\\infty}^{\\infty} f(t) dt = 1\\).\nThe probability of the random variable \\(X\\) being less than or equal to the specific value of \\(x\\) is denoted as \\(P(X ≤ x)\\). Note that for a continuous random variable the probability of it being exactly \\(x\\) is zero.\nLet’s look at the estimate of the CDF from the sample data we used for the box plot, called the empirical cumulative distribution function (ECDF):\n\nax = stats_viz.cdf_example()\n\n\n\n\n\n\n\n\nWe can find any range we want if we use some algebra as in the rightmost subplot above.\n\n\n\n\nGaussian (normal) distribution: looks like a bell curve and is parameterized by its mean (μ) and standard deviation (σ). Many things in nature happen to follow the normal distribution, like heights. Note that testing if a distribution is normal is not trivial. Written as \\(N(\\mu, \\sigma)\\).\nPoisson distribution: discrete distribution that is often used to model arrivals. Parameterized by its mean, lambda (λ). Written as \\(Pois(\\lambda)\\).\nExponential distribution: can be used to model the time between arrivals. Parameterized by its mean, lambda (λ). Written as \\(Exp(\\lambda)\\).\nUniform distribution: places equal likelihood on each value within its bounds (a and b). We often use this for random number generation. Written as \\(U(a, b)\\).\nBernoulli distribution: When we pick a random number to simulate a single success/failure outcome, it is called a Bernoulli trial. This is parameterized by the probability of success (p). Written as \\(Bernoulli(p)\\).\nBinomial distribution: When we run the same experiment n times, the total number of successes is then a binomial random variable. Written as \\(B(n, p)\\).\n\nWe can visualize both discrete and continuous distributions; however, discrete distributions give us a probability mass function (PMF) instead of a PDF:\n\nax = stats_viz.common_dists()\n\n\n\n\n\n\n\n\n\n\n\nIn order to compare variables from different distributions, we would have to scale the data, which we could do with the range by using min-max scaling:\n\\[x_{scaled}=\\frac{x - min(X)}{range(X)}\\]\nAnother way is to use a Z-score to standardize the data:\n\\[z_i = \\frac{x_i - \\bar{x}}{s}\\]\n\n\n\nThe covariance is a statistic for quantifying the relationship between variables by showing how one variable changes with respect to another (also referred to as their joint variance):\n\\[cov(X, Y) = E[(X-E[X])(Y-E[Y])]\\]\nE[X] is the expectation of the random variable X (its long-run average).\nThe sign of the covariance gives us the direction of the relationship, but we need the magnitude as well. For that, we calculate the Pearson correlation coefficient (\\(\\rho\\)):\n\\[\\rho_{X, Y} = \\frac{cov(X, Y)}{s_X s_Y}\\]\nExamples:\n\nax = stats_viz.correlation_coefficient_examples()\n\n\n\n\n\n\n\n\nFrom left to right: no correlation, weak negative correlation, strong positive correlation, and nearly perfect negative correlation.\nOften, it is more informative to use scatter plots to check for relationships between variables. This is because the correlation may be strong, but the relationship may not be linear:\n\nax = stats_viz.non_linear_relationships()\n\n\n\n\n\n\n\n\nRemember, correlation does not imply causation. While we may find a correlation between X and Y, it does not mean that X causes Y or Y causes X. It is possible there is some Z that causes both or that X causes some intermediary event that causes Y — it could even be a coincidence. Be sure to check out Tyler Vigen’s Spurious Correlations blog for some interesting correlations.\n\n\n\nNot only can our correlation coefficients be misleading, but so can summary statistics. Anscombe’s quartet is a collection of four different datasets that have identical summary statistics and correlation coefficients, however, when plotted, it is obvious they are not similar:\n\nax = stats_viz.anscombes_quartet()\n\n\n\n\n\n\n\n\nAnother example of this is the Datasaurus Dozen:\n\nax = stats_viz.datasaurus_dozen()\n\n\n\n\n\n\n\n\n\n\n\n\nSay our favorite ice cream shop has asked us to help predict how many ice creams they can expect to sell on a given day. They are convinced that the temperature outside has strong influence on their sales, so they collected data on the number of ice creams sold at a given temperature. We agree to help them, and the first thing we do is make a scatter plot of the data they gave us:\n\nax = stats_viz.example_scatter_plot()\n\n\n\n\n\n\n\n\nWe can observe an upward trend in the scatter plot: more ice creams are sold at higher temperatures. In order to help out the ice cream shop, though, we need to find a way to make predictions from this data. We can use a technique called regression to model the relationship between temperature and ice cream sales with an equation:\n\nax = stats_viz.example_regression()\n\n\n\n\n\n\n\n\nWe can use the resulting equation to make predictions for the number of ice creams sold at various temperatures. However, we must keep in mind if we are interpolating or extrapolating. If the temperature value we are using for prediction is within the range of the original data we used to build our regression model, then we are interpolating (solid portion of the red line). On the other hand, if the temperature is beyond the values in the original data, we are extrapolating, which is very dangerous, since we can’t assume the pattern continues indefinitely in each direction (dotted portion of the line). Extremely hot temperatures may cause people to stay inside, meaning no ice creams will be sold, while the equation indicates record-high sales.\nForecasting is a type of prediction for time series. In a process called time series decomposition, time series is decomposed into a trend component, a seasonality component, and a cyclical component. These components can be combined in an additive or multiplicative fashion:\n\nax = stats_viz.time_series_decomposition_example()\n\n\n\n\n\n\n\n\nThe trend component describes the behavior of the time series in the long-term without accounting for the seasonal or cyclical effects. Using the trend, we can make broad statements about the time series in the long-run, such as: the population of Earth is increasing or the value of a stock is stagnating. Seasonality of a time series explains the systematic and calendar-related movements of a time series. For example, the number of ice cream trucks on the streets of New York City is high in the summer and drops to nothing in the winter; this pattern repeats every year regardless of whether the actual amount each summer is the same. Lastly, the cyclical component accounts for anything else unexplained or irregular with the time series; this could be something like a hurricane driving the number of ice cream trucks down in the short-term because it isn’t safe to be outside. This component is difficult to anticipate with a forecast due to its unexpected nature.\nWhen making models to forecast time series, some common methods include ARIMA-family methods and exponential smoothing. ARIMA stands for autoregressive (AR), integrated (I), moving average (MA). Autoregressive models take advantage of the fact that an observation at time \\(t\\) is correlated to a previous observation, for example at time \\(t - 1\\). Note that not all time series are autoregressive. The integrated component concerns the differenced data, or the change in the data from one time to another. Lastly, the moving average component uses a sliding window to average the last \\(x\\) observations where \\(x\\) is the length of the sliding window.\nThe moving average puts equal weight on each time period in the past involved in the calculation. In practice, this isn’t always a realistic expectation of our data. Sometimes all past values are important, but they vary in their influence on future data points. For these cases, we can use exponential smoothing, which allows us to put more weight on more recent values and less weight on values further away from what we are predicting.\n\n\n\nInferential statistics deals with inferring or deducing things from the sample data we have in order to make statements about the population as a whole. Before doing so, we need to know whether we conducted an observational study or an experiment. An observational study can’t be used to determine causation because we can’t control for everything. An experiment on the other hand is controlled.\nRemember that the sample statistics we discussed earlier are estimators for the population parameters. Our estimators need confidence intervals, which provide a point estimate and a margin of error around it. This is the range that the true population parameter will be in at a certain confidence level. At the 95% confidence level, 95% of the confidence intervals calculated from random samples of the population contain the true population parameter.\nWe also have the option of using hypothesis testing. First, we define a null hypothesis (say the true population mean is 0), then we determine a significance level (1 - confidence level), which is the probability of rejecting the null hypothesis when it is true. Our result is statistically significant if the value for the null hypothesis is outside the confidence interval. More info.",
    "crumbs": [
      "Home",
      "Introduction to Data Analysis"
    ]
  },
  {
    "objectID": "notebooks/hypothesis_testing.html",
    "href": "notebooks/hypothesis_testing.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Hypothesis Testing\nA hypothesis is a claim or statement about a parameter1. There are two types of statistical hypotheses: - Null Hypothesis - Alternative Hypothesis\nA null hypothesis is a statement that claims that the parameter is equal to some claimed value. - H0 is the symbol used to denote this. It can be pronounced as “H null”, “H zero” or “H nought” - It always contains one of these operators: \\(\\ge\\), \\(\\le\\), =. - This value is the one to always assume is true.\nAn alternative hypothesis is a statement that claims that the parameter is a different value than the null. - HA or H1 is the symbol used to denote this. It’s always called “alternative hypothesis.” - It always contains one of these operators: \\(\\gt\\), \\(\\lt\\), \\(\\neq\\).\n\nSteps to Solving a Hypothesis Test Problem\n\nWrite and label everything.\nWrite hypotheses:\n\nH0: (operator with equal sign)\nHA: (operator without equal sign)\n\nDraw graph (bell-curved)\n\nThe graph will either be right, left or two tailed.\n\nCarry out the necessary calculations to arrive to a solution.\n\nThis can involve solving a t-statisic or z-test.\n\nWrite a sentence summarizing the findings.\n\nUsually follows this format: “There is/is not sufficient evidence to support/reject the claim that…”\n\n\n\n\nRejection Explained\nEvery hypothesis test is rejected or failed to reject. This is because we either have enough data to be able to say the hypothesis is correct, or we don’t have enough data to prove otherwise. To determine this, we compare the significance level to the p-value .\nThe significance level is denoted by \\(\\alpha\\) which measures how strong the evidence must be in order to determine the evidence to be statistically significant.\nP-value is defined by Investopedia as “a statistical measurement used to validate a hypothesis against observed data.” We’re not going to go in-depth here regarding how the p-value is calculated, but just enough to scratch the surface. This value describes the likelihood of the data occurring randomly. P-values range from 0 to 1 and a smaller p-value denotes a smaller probability that the results occurred randomly.\n\nIf p-value \\(\\leq\\) \\(\\alpha\\), then reject H0.\nIf p-value \\(\\gt\\) \\(\\alpha\\), then fail to reject H0.\n\n\n\nDetermining the Tail of the Curve\nThe trick to remembering where the tail of the curve is by looking at the alternative hypothesis. - If the sign in HA is - \\(\\neq\\): two-tailed - \\(\\lt\\): left-taied - \\(\\gt\\): right-tailed\n\n\nErrors\nSometimes, error occurs with hypothesis testing and there are two types of it: - Type I error - This is known as the false-positive. - It occurs when the null hypothesis is rejected, but it is true. - Type II error - This is known as the false-negative. - It occurs when the null hypothesis is not rejected, but it is false.\nThis table below from Scribbr can be used to determine error type, if any.\n\n1A parameter is a measure done on an entire population of data.",
    "crumbs": [
      "Home",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/cleaning_data.html",
    "href": "notebooks/cleaning_data.html",
    "title": "Cleaning Data",
    "section": "",
    "text": "In this notebook, we will using daily temperature data from the National Centers for Environmental Information (NCEI) API. We will use the Global Historical Climatology Network - Daily (GHCND) dataset; see the documentation here.\nThis data was collected from the LaGuardia Airport station in New York City for October 2018. It contains: - the daily minimum temperature (TMIN) - the daily maximum temperature (TMAX) - the daily average temperature (TAVG)\nNote: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for “NCEI weather API” to find the updated one.\nIn addition, we will be using S&P 500 stock market data (obtained using the stock_analysis package and data for bitcoin for 2017 through 2018. For the first edition, the bitcoin data was collected from CoinMarketCap using the stock_analysis package; however, changes in the website led to the necessity of changing the data source to Yahoo! Finance. The bitcoin data that was collected before the CoinMarketCap website change should be equivalent to the historical data that can be viewed on this page.\n\n\n\nWe need to import pandas and read in the temperature data to get started:\n\nimport pandas as pd\n\ndf = pd.read_csv('../data/nyc_temperatures.csv')\ndf.head()\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nattributes\nvalue\n\n\n\n\n0\n2018-10-01T00:00:00\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n\n\n1\n2018-10-01T00:00:00\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n\n\n2\n2018-10-01T00:00:00\nTMIN\nGHCND:USW00014732\n,,W,2400\n18.3\n\n\n3\n2018-10-02T00:00:00\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n\n\n4\n2018-10-02T00:00:00\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n\n\n\n\n\n\n\n\n\n\nWe start out with the following columns:\n\ndf.columns\n\nIndex(['date', 'datatype', 'station', 'attributes', 'value'], dtype='object')\n\n\nWe want to rename the value column to indicate it contains the temperature in Celsius and the attributes column to say flags since each value in the comma-delimited string is a different flag about the data collection. For this task, we use the rename() method and pass in a dictionary mapping the column names to their new names. We pass inplace=True to change our original dataframe instead of getting a new one back:\n\ndf.rename(\n    columns={\n        'value': 'temp_C',\n        'attributes': 'flags'\n    }, inplace=True\n)\n\nThose columns have been successfully renamed:\n\ndf.columns\n\nIndex(['date', 'datatype', 'station', 'flags', 'temp_C'], dtype='object')\n\n\nWe can also perform string operations on the column names with rename():\n\ndf.rename(str.upper, axis='columns').columns\n\nIndex(['DATE', 'DATATYPE', 'STATION', 'FLAGS', 'TEMP_C'], dtype='object')\n\n\n\n\n\nThe date column is not currently being stored as a datetime:\n\ndf.dtypes\n\ndate         object\ndatatype     object\nstation      object\nflags        object\ntemp_C      float64\ndtype: object\n\n\nLet’s perform the conversion with pd.to_datetime():\n\ndf.loc[:,'date'] = pd.to_datetime(df.date)\ndf.dtypes\n\ndate         object\ndatatype     object\nstation      object\nflags        object\ntemp_C      float64\ndtype: object\n\n\nNow we get useful information when we use describe() on this column:\n\ndf.date.describe()\n\ncount                      93\nunique                     31\ntop       2018-10-01 00:00:00\nfreq                        3\nName: date, dtype: object\n\n\nWe can use tz_localize() on a DatetimeIndex object to convert to a desired timezone:\n\npd.date_range(start='2018-10-25', periods=2, freq='D').tz_localize('EST')\n\nDatetimeIndex(['2018-10-25 00:00:00-05:00', '2018-10-26 00:00:00-05:00'], dtype='datetime64[ns, EST]', freq=None)\n\n\nThis also works with Series/DataFrame objects that have an index of type DatetimeIndex. Let’s read in the CSV again for this example and set the date column to be the index and stored as a datetime:\n\neastern = pd.read_csv(\n    '../data/nyc_temperatures.csv', index_col='date', parse_dates=True\n).tz_localize('EST')\neastern.head()\n\n\n\n\n\n\n\n\ndatatype\nstation\nattributes\nvalue\n\n\ndate\n\n\n\n\n\n\n\n\n2018-10-01 00:00:00-05:00\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n\n\n2018-10-01 00:00:00-05:00\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n\n\n2018-10-01 00:00:00-05:00\nTMIN\nGHCND:USW00014732\n,,W,2400\n18.3\n\n\n2018-10-02 00:00:00-05:00\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n\n\n2018-10-02 00:00:00-05:00\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n\n\n\n\n\n\n\nWe can use tz_convert() to convert to another timezone from there. If we convert the Eastern datetimes to UTC, they will now be at 5 AM, since pandas will use the offsets to convert:\n\neastern.tz_convert('UTC').head()\n\n\n\n\n\n\n\n\ndatatype\nstation\nattributes\nvalue\n\n\ndate\n\n\n\n\n\n\n\n\n2018-10-01 05:00:00+00:00\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n\n\n2018-10-01 05:00:00+00:00\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n\n\n2018-10-01 05:00:00+00:00\nTMIN\nGHCND:USW00014732\n,,W,2400\n18.3\n\n\n2018-10-02 05:00:00+00:00\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n\n\n2018-10-02 05:00:00+00:00\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n\n\n\n\n\n\n\nWe can change the period of the index as well. We could change the period to be monthly to make it easier to aggregate later.\nThe reason we have to add the parameter within tz_localize() to None for this, is because we’ll get a warning from pandas that our output class PeriodArray doesn’t have time zone information and we’ll lose it.\n\neastern.tz_localize(None).to_period('M').index\n\nPeriodIndex(['2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10'],\n            dtype='period[M]', name='date')\n\n\nWe now get a PeriodIndex object, which we can change back into a DatetimeIndex object with to_timestamp():\n\neastern.tz_localize(None).to_period('M').to_timestamp().index\n\nDatetimeIndex(['2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01'],\n              dtype='datetime64[ns]', name='date', freq=None)\n\n\nWe can use the assign() method for working with multiple columns at once (or creating new ones). Since our date column has already been converted, we need to read in the data again:\n\ndf = pd.read_csv('../data/nyc_temperatures.csv').rename(\n    columns={\n        'value': 'temp_C',\n        'attributes': 'flags'\n    }\n)\n\nnew_df = df.assign(\n    date=pd.to_datetime(df.date),\n    temp_F=(df.temp_C * 9/5) + 32\n)\nnew_df.dtypes\n\ndate        datetime64[ns]\ndatatype            object\nstation             object\nflags               object\ntemp_C             float64\ntemp_F             float64\ndtype: object\n\n\nThe date column now has datetimes and the temp_F column was added:\n\nnew_df.head()\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_F\n\n\n\n\n0\n2018-10-01\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n70.16\n\n\n1\n2018-10-01\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n78.08\n\n\n2\n2018-10-01\nTMIN\nGHCND:USW00014732\n,,W,2400\n18.3\n64.94\n\n\n3\n2018-10-02\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n72.86\n\n\n4\n2018-10-02\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n78.98\n\n\n\n\n\n\n\nWe can also use astype() to perform conversions. Let’s create columns of the integer portion of the temperatures in Celsius and Fahrenheit. We will use lambda functions (first introduced in Chapter 2, Working with Pandas DataFrames), so that we can use the values being created in the temp_F column to calculate the temp_F_whole column. It is very common (and useful) to use lambda functions with assign():\n\ndf = df.assign(\n    date=lambda x: pd.to_datetime(x.date),\n    temp_C_whole=lambda x: x.temp_C.astype('int'),\n    temp_F=lambda x: (x.temp_C * 9/5) + 32,\n    temp_F_whole=lambda x: x.temp_F.astype('int')\n)\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n0\n2018-10-01\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n21\n70.16\n70\n\n\n1\n2018-10-01\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n2\n2018-10-01\nTMIN\nGHCND:USW00014732\n,,W,2400\n18.3\n18\n64.94\n64\n\n\n3\n2018-10-02\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n22\n72.86\n72\n\n\n4\n2018-10-02\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n\n\n\n\n\nCreating categories:\n\ndf_with_categories = df.assign(\n    station=df.station.astype('category'),\n    datatype=df.datatype.astype('category')\n)\ndf_with_categories.dtypes\n\ndate            datetime64[ns]\ndatatype              category\nstation               category\nflags                   object\ntemp_C                 float64\ntemp_C_whole             int32\ntemp_F                 float64\ntemp_F_whole             int32\ndtype: object\n\n\n\ndf_with_categories.describe(include='category')\n\n\n\n\n\n\n\n\ndatatype\nstation\n\n\n\n\ncount\n93\n93\n\n\nunique\n3\n1\n\n\ntop\nTAVG\nGHCND:USW00014732\n\n\nfreq\n31\n93\n\n\n\n\n\n\n\nOur categories have no order, but this is something that pandas supports:\n\npd.Categorical(\n    ['med', 'med', 'low', 'high'], \n    categories=['low', 'med', 'high'],\n    ordered=True\n)\n\n['med', 'med', 'low', 'high']\nCategories (3, object): ['low' &lt; 'med' &lt; 'high']\n\n\n\n\n\nSay we want to find the days that reached the hottest temperatures in the weather data; we can sort our values by the temp_C column with the largest on top to find this:\n\ndf[df.datatype == 'TMAX'].sort_values(by='temp_C', ascending=False).head(10)\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n19\n2018-10-07\nTMAX\nGHCND:USW00014732\n,,W,2400\n27.8\n27\n82.04\n82\n\n\n28\n2018-10-10\nTMAX\nGHCND:USW00014732\n,,W,2400\n27.8\n27\n82.04\n82\n\n\n31\n2018-10-11\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.7\n26\n80.06\n80\n\n\n10\n2018-10-04\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n4\n2018-10-02\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n1\n2018-10-01\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n25\n2018-10-09\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n7\n2018-10-03\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.0\n25\n77.00\n77\n\n\n13\n2018-10-05\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.8\n22\n73.04\n73\n\n\n22\n2018-10-08\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.8\n22\n73.04\n73\n\n\n\n\n\n\n\nHowever, this isn’t perfect because we have some ties, and they aren’t sorted consistently. In the first tie between the 7th and the 10th, the earlier date comes first, but the opposite is true with the tie between the 4th and the 2nd. We can use other columns to break ties and specify how to sort each with ascending. Let’s break ties with the date column and show earlier dates before later ones:\n\ndf[df.datatype == 'TMAX'].sort_values(by=['temp_C', 'date'], ascending=[False, True]).head(10)\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n19\n2018-10-07\nTMAX\nGHCND:USW00014732\n,,W,2400\n27.8\n27\n82.04\n82\n\n\n28\n2018-10-10\nTMAX\nGHCND:USW00014732\n,,W,2400\n27.8\n27\n82.04\n82\n\n\n31\n2018-10-11\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.7\n26\n80.06\n80\n\n\n4\n2018-10-02\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n10\n2018-10-04\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n1\n2018-10-01\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n25\n2018-10-09\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n7\n2018-10-03\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.0\n25\n77.00\n77\n\n\n13\n2018-10-05\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.8\n22\n73.04\n73\n\n\n22\n2018-10-08\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.8\n22\n73.04\n73\n\n\n\n\n\n\n\nNotice that the index was jumbled in the past 2 results. Here, our index only stores the row number in the original data, but we may not need to keep track of that information. In this case, we can pass in ignore_index=True to get a new index after sorting:\n\ndf[df.datatype == 'TMAX'].sort_values(by=['temp_C', 'date'], ascending=[False, True], ignore_index=True).head(10)\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n0\n2018-10-07\nTMAX\nGHCND:USW00014732\n,,W,2400\n27.8\n27\n82.04\n82\n\n\n1\n2018-10-10\nTMAX\nGHCND:USW00014732\n,,W,2400\n27.8\n27\n82.04\n82\n\n\n2\n2018-10-11\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.7\n26\n80.06\n80\n\n\n3\n2018-10-02\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n4\n2018-10-04\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n5\n2018-10-01\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n6\n2018-10-09\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n7\n2018-10-03\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.0\n25\n77.00\n77\n\n\n8\n2018-10-05\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.8\n22\n73.04\n73\n\n\n9\n2018-10-08\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.8\n22\n73.04\n73\n\n\n\n\n\n\n\nWhen just looking for the n-largest values, rather than wanting to sort all the data, we can use nlargest():\n\ndf[df.datatype == 'TAVG'].nlargest(n=10, columns='temp_C')\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n27\n2018-10-10\nTAVG\nGHCND:USW00014732\nH,,S,\n23.8\n23\n74.84\n74\n\n\n30\n2018-10-11\nTAVG\nGHCND:USW00014732\nH,,S,\n23.4\n23\n74.12\n74\n\n\n18\n2018-10-07\nTAVG\nGHCND:USW00014732\nH,,S,\n22.8\n22\n73.04\n73\n\n\n3\n2018-10-02\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n22\n72.86\n72\n\n\n6\n2018-10-03\nTAVG\nGHCND:USW00014732\nH,,S,\n21.8\n21\n71.24\n71\n\n\n24\n2018-10-09\nTAVG\nGHCND:USW00014732\nH,,S,\n21.8\n21\n71.24\n71\n\n\n9\n2018-10-04\nTAVG\nGHCND:USW00014732\nH,,S,\n21.3\n21\n70.34\n70\n\n\n0\n2018-10-01\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n21\n70.16\n70\n\n\n21\n2018-10-08\nTAVG\nGHCND:USW00014732\nH,,S,\n20.9\n20\n69.62\n69\n\n\n12\n2018-10-05\nTAVG\nGHCND:USW00014732\nH,,S,\n20.3\n20\n68.54\n68\n\n\n\n\n\n\n\nWe use nsmallest() for the n-smallest values.\n\ndf.nsmallest(n=5, columns=['temp_C', 'date'])\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n65\n2018-10-22\nTMIN\nGHCND:USW00014732\n,,W,2400\n5.6\n5\n42.08\n42\n\n\n77\n2018-10-26\nTMIN\nGHCND:USW00014732\n,,W,2400\n5.6\n5\n42.08\n42\n\n\n62\n2018-10-21\nTMIN\nGHCND:USW00014732\n,,W,2400\n6.1\n6\n42.98\n42\n\n\n74\n2018-10-25\nTMIN\nGHCND:USW00014732\n,,W,2400\n6.1\n6\n42.98\n42\n\n\n53\n2018-10-18\nTMIN\nGHCND:USW00014732\n,,W,2400\n6.7\n6\n44.06\n44\n\n\n\n\n\n\n\nThe sample() method will give us rows (or columns with axis=1) at random. We can provide a seed (random_state) to make this reproducible. The index after we do this is jumbled:\n\ndf.sample(5, random_state=0).index\n\nIndex([2, 30, 55, 16, 13], dtype='int64')\n\n\nWe can use sort_index() to order it again:\n\ndf.sample(5, random_state=0).sort_index().index\n\nIndex([2, 13, 16, 30, 55], dtype='int64')\n\n\nThe sort_index() method can also sort columns alphabetically:\n\ndf.sort_index(axis=1).head()\n\n\n\n\n\n\n\n\ndatatype\ndate\nflags\nstation\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n0\nTAVG\n2018-10-01\nH,,S,\nGHCND:USW00014732\n21.2\n21\n70.16\n70\n\n\n1\nTMAX\n2018-10-01\n,,W,2400\nGHCND:USW00014732\n25.6\n25\n78.08\n78\n\n\n2\nTMIN\n2018-10-01\n,,W,2400\nGHCND:USW00014732\n18.3\n18\n64.94\n64\n\n\n3\nTAVG\n2018-10-02\nH,,S,\nGHCND:USW00014732\n22.7\n22\n72.86\n72\n\n\n4\nTMAX\n2018-10-02\n,,W,2400\nGHCND:USW00014732\n26.1\n26\n78.98\n78\n\n\n\n\n\n\n\nThis can make selection with loc easier for many columns:\n\ndf.sort_index(axis=1).head().loc[:,'temp_C':'temp_F_whole']\n\n\n\n\n\n\n\n\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n0\n21.2\n21\n70.16\n70\n\n\n1\n25.6\n25\n78.08\n78\n\n\n2\n18.3\n18\n64.94\n64\n\n\n3\n22.7\n22\n72.86\n72\n\n\n4\n26.1\n26\n78.98\n78\n\n\n\n\n\n\n\nWe must sort the index to compare two dataframes. If the index is different, but the data is the same, they will be marked not-equal:\n\ndf.equals(df.sort_values(by='temp_C'))\n\nFalse\n\n\nSorting the index solves this issue:\n\ndf.equals(df.sort_values(by='temp_C').sort_index())\n\nTrue\n\n\nLet’s set the date column as our index:\n\ndf.set_index('date', inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n2018-10-01\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n21\n70.16\n70\n\n\n2018-10-01\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n2018-10-01\nTMIN\nGHCND:USW00014732\n,,W,2400\n18.3\n18\n64.94\n64\n\n\n2018-10-02\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n22\n72.86\n72\n\n\n2018-10-02\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n\n\n\n\n\nNow that we have an index of type DatetimeIndex, we can do datetime slicing and indexing. As long as we provide a date format that pandas understands, we can grab the data. To select all of 2018, we simply use df.loc['2018'], for the fourth quarter of 2018 we can use df.loc['2018-Q4'], grabbing October is as simple as using df.loc['2018-10']; these can also be combined to build ranges. Let’s grab October 11, 2018 through October 12, 2018 (inclusive of both endpoints)—note that using loc[] is optional for ranges:\n\ndf['2018-10-11':'2018-10-12']\n\n\n\n\n\n\n\n\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n2018-10-11\nTAVG\nGHCND:USW00014732\nH,,S,\n23.4\n23\n74.12\n74\n\n\n2018-10-11\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.7\n26\n80.06\n80\n\n\n2018-10-11\nTMIN\nGHCND:USW00014732\n,,W,2400\n21.7\n21\n71.06\n71\n\n\n2018-10-12\nTAVG\nGHCND:USW00014732\nH,,S,\n18.3\n18\n64.94\n64\n\n\n2018-10-12\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.2\n22\n71.96\n71\n\n\n2018-10-12\nTMIN\nGHCND:USW00014732\n,,W,2400\n12.2\n12\n53.96\n53\n\n\n\n\n\n\n\nWe can also use reset_index() to get a fresh index and move our current index into a column for safe keeping. This is especially useful if we had data, such as the date, in the index that we don’t want to lose:\n\ndf['2018-10-11':'2018-10-12'].reset_index()\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n0\n2018-10-11\nTAVG\nGHCND:USW00014732\nH,,S,\n23.4\n23\n74.12\n74\n\n\n1\n2018-10-11\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.7\n26\n80.06\n80\n\n\n2\n2018-10-11\nTMIN\nGHCND:USW00014732\n,,W,2400\n21.7\n21\n71.06\n71\n\n\n3\n2018-10-12\nTAVG\nGHCND:USW00014732\nH,,S,\n18.3\n18\n64.94\n64\n\n\n4\n2018-10-12\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.2\n22\n71.96\n71\n\n\n5\n2018-10-12\nTMIN\nGHCND:USW00014732\n,,W,2400\n12.2\n12\n53.96\n53\n\n\n\n\n\n\n\nReindexing allows us to conform our axis to contain a given set of labels. Let’s turn to the S&P 500 stock data in the sp500.csv file to see an example of this. Notice we only have data for trading days (weekdays, excluding holidays):\n\nsp = pd.read_csv(\n    '../data/sp500.csv', index_col='date', parse_dates=True\n).drop(columns=['adj_close'])\n\nsp.head(10).assign(\n    day_of_week=lambda x: x.index.day_name()\n)\n\n\n\n\n\n\n\n\nhigh\nlow\nopen\nclose\nvolume\nday_of_week\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2017-01-03\n2263.879883\n2245.129883\n2251.570068\n2257.830078\n3770530000\nTuesday\n\n\n2017-01-04\n2272.820068\n2261.600098\n2261.600098\n2270.750000\n3764890000\nWednesday\n\n\n2017-01-05\n2271.500000\n2260.449951\n2268.179932\n2269.000000\n3761820000\nThursday\n\n\n2017-01-06\n2282.100098\n2264.060059\n2271.139893\n2276.979980\n3339890000\nFriday\n\n\n2017-01-09\n2275.489990\n2268.899902\n2273.590088\n2268.899902\n3217610000\nMonday\n\n\n2017-01-10\n2279.270020\n2265.270020\n2269.719971\n2268.899902\n3638790000\nTuesday\n\n\n2017-01-11\n2275.320068\n2260.830078\n2268.600098\n2275.320068\n3620410000\nWednesday\n\n\n2017-01-12\n2271.780029\n2254.250000\n2271.139893\n2270.439941\n3462130000\nThursday\n\n\n2017-01-13\n2278.679932\n2271.510010\n2272.739990\n2274.639893\n3081270000\nFriday\n\n\n2017-01-17\n2272.080078\n2262.810059\n2269.139893\n2267.889893\n3584990000\nTuesday\n\n\n\n\n\n\n\nIf we want to look at the value of a portfolio (group of assets) that trade on different days, we need to handle the mismatch in the index. Bitcoin, for example, trades daily. If we sum up all the data we have for each day (aggregations will be covered in chapter 4, so don’t fixate on this part), we get the following:\n\nbitcoin = pd.read_csv(\n    '../data/bitcoin.csv', index_col='date', parse_dates=True\n).drop(columns=['market_cap'])\n\n# every day's closing price = S&P 500 close + Bitcoin close (same for other metrics)\nportfolio = pd.concat([sp, bitcoin], sort=False).groupby(level='date').sum()\n\nportfolio.head(10).assign(\n    day_of_week=lambda x: x.index.day_name()\n)\n\n\n\n\n\n\n\n\nhigh\nlow\nopen\nclose\nvolume\nday_of_week\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2017-01-01\n1003.080000\n958.700000\n963.660000\n998.330000\n147775008\nSunday\n\n\n2017-01-02\n1031.390000\n996.700000\n998.620000\n1021.750000\n222184992\nMonday\n\n\n2017-01-03\n3307.959883\n3266.729883\n3273.170068\n3301.670078\n3955698000\nTuesday\n\n\n2017-01-04\n3432.240068\n3306.000098\n3306.000098\n3425.480000\n4109835984\nWednesday\n\n\n2017-01-05\n3462.600000\n3170.869951\n3424.909932\n3282.380000\n4272019008\nThursday\n\n\n2017-01-06\n3328.910098\n3148.000059\n3285.379893\n3179.179980\n3691766000\nFriday\n\n\n2017-01-07\n908.590000\n823.560000\n903.490000\n908.590000\n279550016\nSaturday\n\n\n2017-01-08\n942.720000\n887.250000\n908.170000\n911.200000\n158715008\nSunday\n\n\n2017-01-09\n3189.179990\n3148.709902\n3186.830088\n3171.729902\n3359486992\nMonday\n\n\n2017-01-10\n3194.140020\n3166.330020\n3172.159971\n3176.579902\n3754598000\nTuesday\n\n\n\n\n\n\n\nIt may not be immediately obvious what is wrong with the previous data, but with a visualization we can easily see the cyclical pattern of drops on the days the stock market is closed. (Don’t worry about the plotting code too much, we will cover it in depth in chapters 5 and 6).\nWe will need to import matplotlib now:\n\nimport matplotlib.pyplot as plt # we use this module for plotting\nfrom matplotlib.ticker import StrMethodFormatter # for formatting the axis\n\nNow we can see why we need to reindex:\n\n# plot the closing price from Q4 2017 through Q2 2018\nax = portfolio['2017-Q4':'2018-Q2'].plot(\n    y='close', figsize=(15, 5), legend=False,\n    title='Bitcoin + S&P 500 value without accounting for different indices'\n)\n\n# formatting\nax.set_ylabel('price')\nax.yaxis.set_major_formatter(StrMethodFormatter('${x:,.0f}'))\nfor spine in ['top', 'right']:\n    ax.spines[spine].set_visible(False)\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\nWe need to align the index of the S&P 500 to match bitcoin in order to fix this. We will use the reindex() method, but by default we get NaN for the values that we don’t have data for:\n\nsp.reindex(bitcoin.index).head(10).assign(\n    day_of_week=lambda x: x.index.day_name()\n)\n\n\n\n\n\n\n\n\nhigh\nlow\nopen\nclose\nvolume\nday_of_week\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2017-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nSunday\n\n\n2017-01-02\nNaN\nNaN\nNaN\nNaN\nNaN\nMonday\n\n\n2017-01-03\n2263.879883\n2245.129883\n2251.570068\n2257.830078\n3.770530e+09\nTuesday\n\n\n2017-01-04\n2272.820068\n2261.600098\n2261.600098\n2270.750000\n3.764890e+09\nWednesday\n\n\n2017-01-05\n2271.500000\n2260.449951\n2268.179932\n2269.000000\n3.761820e+09\nThursday\n\n\n2017-01-06\n2282.100098\n2264.060059\n2271.139893\n2276.979980\n3.339890e+09\nFriday\n\n\n2017-01-07\nNaN\nNaN\nNaN\nNaN\nNaN\nSaturday\n\n\n2017-01-08\nNaN\nNaN\nNaN\nNaN\nNaN\nSunday\n\n\n2017-01-09\n2275.489990\n2268.899902\n2273.590088\n2268.899902\n3.217610e+09\nMonday\n\n\n2017-01-10\n2279.270020\n2265.270020\n2269.719971\n2268.899902\n3.638790e+09\nTuesday\n\n\n\n\n\n\n\nSo now we have rows for every day of the year, but all the weekends and holidays have NaN values. To address this, we can specify how to handle missing values with the method argument. In this case, we want to forward-fill, which will put the weekend and holiday values as the value they had for the Friday (or end of trading week) before:\n\nsp.reindex(bitcoin.index, method='ffill').head(10)\\\n    .assign(day_of_week=lambda x: x.index.day_name())\n\n\n\n\n\n\n\n\nhigh\nlow\nopen\nclose\nvolume\nday_of_week\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2017-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nSunday\n\n\n2017-01-02\nNaN\nNaN\nNaN\nNaN\nNaN\nMonday\n\n\n2017-01-03\n2263.879883\n2245.129883\n2251.570068\n2257.830078\n3.770530e+09\nTuesday\n\n\n2017-01-04\n2272.820068\n2261.600098\n2261.600098\n2270.750000\n3.764890e+09\nWednesday\n\n\n2017-01-05\n2271.500000\n2260.449951\n2268.179932\n2269.000000\n3.761820e+09\nThursday\n\n\n2017-01-06\n2282.100098\n2264.060059\n2271.139893\n2276.979980\n3.339890e+09\nFriday\n\n\n2017-01-07\n2282.100098\n2264.060059\n2271.139893\n2276.979980\n3.339890e+09\nSaturday\n\n\n2017-01-08\n2282.100098\n2264.060059\n2271.139893\n2276.979980\n3.339890e+09\nSunday\n\n\n2017-01-09\n2275.489990\n2268.899902\n2273.590088\n2268.899902\n3.217610e+09\nMonday\n\n\n2017-01-10\n2279.270020\n2265.270020\n2269.719971\n2268.899902\n3.638790e+09\nTuesday\n\n\n\n\n\n\n\nTo isolate the changes happening with the forward-filling, we can use the compare() method. It shows us the values that differ across identically-labeled dataframes (same names and same columns). Here, we can see that only weekends and holidays (Monday, January 16, 2017 was MLK day) have values forward-filled. Notice that consecutive days have the same values.\n\nsp.reindex(bitcoin.index)\\\n    .compare(sp.reindex(bitcoin.index, method='ffill'))\\\n    .head(10).assign(day_of_week=lambda x: x.index.day_name())\n\n\n\n\n\n\n\n\nhigh\nlow\nopen\nclose\nvolume\nday_of_week\n\n\n\nself\nother\nself\nother\nself\nother\nself\nother\nself\nother\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-01-07\nNaN\n2282.100098\nNaN\n2264.060059\nNaN\n2271.139893\nNaN\n2276.979980\nNaN\n3.339890e+09\nSaturday\n\n\n2017-01-08\nNaN\n2282.100098\nNaN\n2264.060059\nNaN\n2271.139893\nNaN\n2276.979980\nNaN\n3.339890e+09\nSunday\n\n\n2017-01-14\nNaN\n2278.679932\nNaN\n2271.510010\nNaN\n2272.739990\nNaN\n2274.639893\nNaN\n3.081270e+09\nSaturday\n\n\n2017-01-15\nNaN\n2278.679932\nNaN\n2271.510010\nNaN\n2272.739990\nNaN\n2274.639893\nNaN\n3.081270e+09\nSunday\n\n\n2017-01-16\nNaN\n2278.679932\nNaN\n2271.510010\nNaN\n2272.739990\nNaN\n2274.639893\nNaN\n3.081270e+09\nMonday\n\n\n2017-01-21\nNaN\n2276.959961\nNaN\n2265.010010\nNaN\n2269.959961\nNaN\n2271.310059\nNaN\n3.524970e+09\nSaturday\n\n\n2017-01-22\nNaN\n2276.959961\nNaN\n2265.010010\nNaN\n2269.959961\nNaN\n2271.310059\nNaN\n3.524970e+09\nSunday\n\n\n2017-01-28\nNaN\n2299.020020\nNaN\n2291.620117\nNaN\n2299.020020\nNaN\n2294.689941\nNaN\n3.135890e+09\nSaturday\n\n\n2017-01-29\nNaN\n2299.020020\nNaN\n2291.620117\nNaN\n2299.020020\nNaN\n2294.689941\nNaN\n3.135890e+09\nSunday\n\n\n2017-02-04\nNaN\n2298.310059\nNaN\n2287.879883\nNaN\n2288.540039\nNaN\n2297.419922\nNaN\n3.597970e+09\nSaturday\n\n\n\n\n\n\n\nThis isn’t perfect though. We probably want 0 for the volume traded and to put the closing price for the open, high, low, and close on the days the market is closed:\nThe reason why we’re using np.where(boolean condition, value if True, value if False) within lambda functions in the example below, is that vectorized operations allow us to be faster and more efficient than utilizing for loops to perform calculations on arrays all at once.\n\nimport numpy as np\n\nsp_reindexed = sp.reindex(bitcoin.index).assign(\n    volume=lambda x: x.volume.fillna(0), # put 0 when market is closed\n    close=lambda x: x.close.fillna(method='ffill'), # carry this forward\n    # take the closing price if these aren't available\n    open=lambda x: np.where(x.open.isnull(), x.close, x.open),\n    high=lambda x: np.where(x.high.isnull(), x.close, x.high),\n    low=lambda x: np.where(x.low.isnull(), x.close, x.low)\n)\nsp_reindexed.head(10).assign(\n    day_of_week=lambda x: x.index.day_name()\n)\n\n\n\n\n\n\n\n\nhigh\nlow\nopen\nclose\nvolume\nday_of_week\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2017-01-01\nNaN\nNaN\nNaN\nNaN\n0.000000e+00\nSunday\n\n\n2017-01-02\nNaN\nNaN\nNaN\nNaN\n0.000000e+00\nMonday\n\n\n2017-01-03\n2263.879883\n2245.129883\n2251.570068\n2257.830078\n3.770530e+09\nTuesday\n\n\n2017-01-04\n2272.820068\n2261.600098\n2261.600098\n2270.750000\n3.764890e+09\nWednesday\n\n\n2017-01-05\n2271.500000\n2260.449951\n2268.179932\n2269.000000\n3.761820e+09\nThursday\n\n\n2017-01-06\n2282.100098\n2264.060059\n2271.139893\n2276.979980\n3.339890e+09\nFriday\n\n\n2017-01-07\n2276.979980\n2276.979980\n2276.979980\n2276.979980\n0.000000e+00\nSaturday\n\n\n2017-01-08\n2276.979980\n2276.979980\n2276.979980\n2276.979980\n0.000000e+00\nSunday\n\n\n2017-01-09\n2275.489990\n2268.899902\n2273.590088\n2268.899902\n3.217610e+09\nMonday\n\n\n2017-01-10\n2279.270020\n2265.270020\n2269.719971\n2268.899902\n3.638790e+09\nTuesday\n\n\n\n\n\n\n\nIf we create a visualization comparing the reindexed data to the first attempt, we see how reindexing helped maintain the asset value when the market was closed:\n\n# every day's closing price = S&P 500 close adjusted for market closure + Bitcoin close (same for other metrics)\nfixed_portfolio = sp_reindexed + bitcoin\n\n# plot the reindexed portfolio's closing price from Q4 2017 through Q2 2018\nax = fixed_portfolio['2017-Q4':'2018-Q2'].plot(\n    y='close', label='reindexed portfolio of S&P 500 + Bitcoin', figsize=(15, 5), linewidth=2, \n    title='Reindexed portfolio vs. portfolio with mismatched indices'\n)\n\n# add line for original portfolio for comparison\nportfolio['2017-Q4':'2018-Q2'].plot(\n    y='close', ax=ax, linestyle='--', label='portfolio of S&P 500 + Bitcoin w/o reindexing'\n)\n\n# formatting\nax.set_ylabel('price')\nax.yaxis.set_major_formatter(StrMethodFormatter('${x:,.0f}'))\nfor spine in ['top', 'right']:\n    ax.spines[spine].set_visible(False)\n\n# show the plot\nplt.show()",
    "crumbs": [
      "Home",
      "Cleaning Data"
    ]
  },
  {
    "objectID": "notebooks/cleaning_data.html#about-the-data",
    "href": "notebooks/cleaning_data.html#about-the-data",
    "title": "Cleaning Data",
    "section": "",
    "text": "In this notebook, we will using daily temperature data from the National Centers for Environmental Information (NCEI) API. We will use the Global Historical Climatology Network - Daily (GHCND) dataset; see the documentation here.\nThis data was collected from the LaGuardia Airport station in New York City for October 2018. It contains: - the daily minimum temperature (TMIN) - the daily maximum temperature (TMAX) - the daily average temperature (TAVG)\nNote: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for “NCEI weather API” to find the updated one.\nIn addition, we will be using S&P 500 stock market data (obtained using the stock_analysis package and data for bitcoin for 2017 through 2018. For the first edition, the bitcoin data was collected from CoinMarketCap using the stock_analysis package; however, changes in the website led to the necessity of changing the data source to Yahoo! Finance. The bitcoin data that was collected before the CoinMarketCap website change should be equivalent to the historical data that can be viewed on this page.",
    "crumbs": [
      "Home",
      "Cleaning Data"
    ]
  },
  {
    "objectID": "notebooks/cleaning_data.html#setup",
    "href": "notebooks/cleaning_data.html#setup",
    "title": "Cleaning Data",
    "section": "",
    "text": "We need to import pandas and read in the temperature data to get started:\n\nimport pandas as pd\n\ndf = pd.read_csv('../data/nyc_temperatures.csv')\ndf.head()\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nattributes\nvalue\n\n\n\n\n0\n2018-10-01T00:00:00\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n\n\n1\n2018-10-01T00:00:00\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n\n\n2\n2018-10-01T00:00:00\nTMIN\nGHCND:USW00014732\n,,W,2400\n18.3\n\n\n3\n2018-10-02T00:00:00\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n\n\n4\n2018-10-02T00:00:00\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1",
    "crumbs": [
      "Home",
      "Cleaning Data"
    ]
  },
  {
    "objectID": "notebooks/cleaning_data.html#renaming-columns",
    "href": "notebooks/cleaning_data.html#renaming-columns",
    "title": "Cleaning Data",
    "section": "",
    "text": "We start out with the following columns:\n\ndf.columns\n\nIndex(['date', 'datatype', 'station', 'attributes', 'value'], dtype='object')\n\n\nWe want to rename the value column to indicate it contains the temperature in Celsius and the attributes column to say flags since each value in the comma-delimited string is a different flag about the data collection. For this task, we use the rename() method and pass in a dictionary mapping the column names to their new names. We pass inplace=True to change our original dataframe instead of getting a new one back:\n\ndf.rename(\n    columns={\n        'value': 'temp_C',\n        'attributes': 'flags'\n    }, inplace=True\n)\n\nThose columns have been successfully renamed:\n\ndf.columns\n\nIndex(['date', 'datatype', 'station', 'flags', 'temp_C'], dtype='object')\n\n\nWe can also perform string operations on the column names with rename():\n\ndf.rename(str.upper, axis='columns').columns\n\nIndex(['DATE', 'DATATYPE', 'STATION', 'FLAGS', 'TEMP_C'], dtype='object')",
    "crumbs": [
      "Home",
      "Cleaning Data"
    ]
  },
  {
    "objectID": "notebooks/cleaning_data.html#type-conversion",
    "href": "notebooks/cleaning_data.html#type-conversion",
    "title": "Cleaning Data",
    "section": "",
    "text": "The date column is not currently being stored as a datetime:\n\ndf.dtypes\n\ndate         object\ndatatype     object\nstation      object\nflags        object\ntemp_C      float64\ndtype: object\n\n\nLet’s perform the conversion with pd.to_datetime():\n\ndf.loc[:,'date'] = pd.to_datetime(df.date)\ndf.dtypes\n\ndate         object\ndatatype     object\nstation      object\nflags        object\ntemp_C      float64\ndtype: object\n\n\nNow we get useful information when we use describe() on this column:\n\ndf.date.describe()\n\ncount                      93\nunique                     31\ntop       2018-10-01 00:00:00\nfreq                        3\nName: date, dtype: object\n\n\nWe can use tz_localize() on a DatetimeIndex object to convert to a desired timezone:\n\npd.date_range(start='2018-10-25', periods=2, freq='D').tz_localize('EST')\n\nDatetimeIndex(['2018-10-25 00:00:00-05:00', '2018-10-26 00:00:00-05:00'], dtype='datetime64[ns, EST]', freq=None)\n\n\nThis also works with Series/DataFrame objects that have an index of type DatetimeIndex. Let’s read in the CSV again for this example and set the date column to be the index and stored as a datetime:\n\neastern = pd.read_csv(\n    '../data/nyc_temperatures.csv', index_col='date', parse_dates=True\n).tz_localize('EST')\neastern.head()\n\n\n\n\n\n\n\n\ndatatype\nstation\nattributes\nvalue\n\n\ndate\n\n\n\n\n\n\n\n\n2018-10-01 00:00:00-05:00\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n\n\n2018-10-01 00:00:00-05:00\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n\n\n2018-10-01 00:00:00-05:00\nTMIN\nGHCND:USW00014732\n,,W,2400\n18.3\n\n\n2018-10-02 00:00:00-05:00\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n\n\n2018-10-02 00:00:00-05:00\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n\n\n\n\n\n\n\nWe can use tz_convert() to convert to another timezone from there. If we convert the Eastern datetimes to UTC, they will now be at 5 AM, since pandas will use the offsets to convert:\n\neastern.tz_convert('UTC').head()\n\n\n\n\n\n\n\n\ndatatype\nstation\nattributes\nvalue\n\n\ndate\n\n\n\n\n\n\n\n\n2018-10-01 05:00:00+00:00\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n\n\n2018-10-01 05:00:00+00:00\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n\n\n2018-10-01 05:00:00+00:00\nTMIN\nGHCND:USW00014732\n,,W,2400\n18.3\n\n\n2018-10-02 05:00:00+00:00\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n\n\n2018-10-02 05:00:00+00:00\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n\n\n\n\n\n\n\nWe can change the period of the index as well. We could change the period to be monthly to make it easier to aggregate later.\nThe reason we have to add the parameter within tz_localize() to None for this, is because we’ll get a warning from pandas that our output class PeriodArray doesn’t have time zone information and we’ll lose it.\n\neastern.tz_localize(None).to_period('M').index\n\nPeriodIndex(['2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10', '2018-10', '2018-10', '2018-10',\n             '2018-10', '2018-10', '2018-10'],\n            dtype='period[M]', name='date')\n\n\nWe now get a PeriodIndex object, which we can change back into a DatetimeIndex object with to_timestamp():\n\neastern.tz_localize(None).to_period('M').to_timestamp().index\n\nDatetimeIndex(['2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01', '2018-10-01', '2018-10-01', '2018-10-01',\n               '2018-10-01'],\n              dtype='datetime64[ns]', name='date', freq=None)\n\n\nWe can use the assign() method for working with multiple columns at once (or creating new ones). Since our date column has already been converted, we need to read in the data again:\n\ndf = pd.read_csv('../data/nyc_temperatures.csv').rename(\n    columns={\n        'value': 'temp_C',\n        'attributes': 'flags'\n    }\n)\n\nnew_df = df.assign(\n    date=pd.to_datetime(df.date),\n    temp_F=(df.temp_C * 9/5) + 32\n)\nnew_df.dtypes\n\ndate        datetime64[ns]\ndatatype            object\nstation             object\nflags               object\ntemp_C             float64\ntemp_F             float64\ndtype: object\n\n\nThe date column now has datetimes and the temp_F column was added:\n\nnew_df.head()\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_F\n\n\n\n\n0\n2018-10-01\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n70.16\n\n\n1\n2018-10-01\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n78.08\n\n\n2\n2018-10-01\nTMIN\nGHCND:USW00014732\n,,W,2400\n18.3\n64.94\n\n\n3\n2018-10-02\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n72.86\n\n\n4\n2018-10-02\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n78.98\n\n\n\n\n\n\n\nWe can also use astype() to perform conversions. Let’s create columns of the integer portion of the temperatures in Celsius and Fahrenheit. We will use lambda functions (first introduced in Chapter 2, Working with Pandas DataFrames), so that we can use the values being created in the temp_F column to calculate the temp_F_whole column. It is very common (and useful) to use lambda functions with assign():\n\ndf = df.assign(\n    date=lambda x: pd.to_datetime(x.date),\n    temp_C_whole=lambda x: x.temp_C.astype('int'),\n    temp_F=lambda x: (x.temp_C * 9/5) + 32,\n    temp_F_whole=lambda x: x.temp_F.astype('int')\n)\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n0\n2018-10-01\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n21\n70.16\n70\n\n\n1\n2018-10-01\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n2\n2018-10-01\nTMIN\nGHCND:USW00014732\n,,W,2400\n18.3\n18\n64.94\n64\n\n\n3\n2018-10-02\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n22\n72.86\n72\n\n\n4\n2018-10-02\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n\n\n\n\n\nCreating categories:\n\ndf_with_categories = df.assign(\n    station=df.station.astype('category'),\n    datatype=df.datatype.astype('category')\n)\ndf_with_categories.dtypes\n\ndate            datetime64[ns]\ndatatype              category\nstation               category\nflags                   object\ntemp_C                 float64\ntemp_C_whole             int32\ntemp_F                 float64\ntemp_F_whole             int32\ndtype: object\n\n\n\ndf_with_categories.describe(include='category')\n\n\n\n\n\n\n\n\ndatatype\nstation\n\n\n\n\ncount\n93\n93\n\n\nunique\n3\n1\n\n\ntop\nTAVG\nGHCND:USW00014732\n\n\nfreq\n31\n93\n\n\n\n\n\n\n\nOur categories have no order, but this is something that pandas supports:\n\npd.Categorical(\n    ['med', 'med', 'low', 'high'], \n    categories=['low', 'med', 'high'],\n    ordered=True\n)\n\n['med', 'med', 'low', 'high']\nCategories (3, object): ['low' &lt; 'med' &lt; 'high']",
    "crumbs": [
      "Home",
      "Cleaning Data"
    ]
  },
  {
    "objectID": "notebooks/cleaning_data.html#reordering-reindexing-and-sorting",
    "href": "notebooks/cleaning_data.html#reordering-reindexing-and-sorting",
    "title": "Cleaning Data",
    "section": "",
    "text": "Say we want to find the days that reached the hottest temperatures in the weather data; we can sort our values by the temp_C column with the largest on top to find this:\n\ndf[df.datatype == 'TMAX'].sort_values(by='temp_C', ascending=False).head(10)\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n19\n2018-10-07\nTMAX\nGHCND:USW00014732\n,,W,2400\n27.8\n27\n82.04\n82\n\n\n28\n2018-10-10\nTMAX\nGHCND:USW00014732\n,,W,2400\n27.8\n27\n82.04\n82\n\n\n31\n2018-10-11\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.7\n26\n80.06\n80\n\n\n10\n2018-10-04\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n4\n2018-10-02\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n1\n2018-10-01\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n25\n2018-10-09\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n7\n2018-10-03\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.0\n25\n77.00\n77\n\n\n13\n2018-10-05\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.8\n22\n73.04\n73\n\n\n22\n2018-10-08\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.8\n22\n73.04\n73\n\n\n\n\n\n\n\nHowever, this isn’t perfect because we have some ties, and they aren’t sorted consistently. In the first tie between the 7th and the 10th, the earlier date comes first, but the opposite is true with the tie between the 4th and the 2nd. We can use other columns to break ties and specify how to sort each with ascending. Let’s break ties with the date column and show earlier dates before later ones:\n\ndf[df.datatype == 'TMAX'].sort_values(by=['temp_C', 'date'], ascending=[False, True]).head(10)\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n19\n2018-10-07\nTMAX\nGHCND:USW00014732\n,,W,2400\n27.8\n27\n82.04\n82\n\n\n28\n2018-10-10\nTMAX\nGHCND:USW00014732\n,,W,2400\n27.8\n27\n82.04\n82\n\n\n31\n2018-10-11\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.7\n26\n80.06\n80\n\n\n4\n2018-10-02\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n10\n2018-10-04\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n1\n2018-10-01\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n25\n2018-10-09\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n7\n2018-10-03\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.0\n25\n77.00\n77\n\n\n13\n2018-10-05\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.8\n22\n73.04\n73\n\n\n22\n2018-10-08\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.8\n22\n73.04\n73\n\n\n\n\n\n\n\nNotice that the index was jumbled in the past 2 results. Here, our index only stores the row number in the original data, but we may not need to keep track of that information. In this case, we can pass in ignore_index=True to get a new index after sorting:\n\ndf[df.datatype == 'TMAX'].sort_values(by=['temp_C', 'date'], ascending=[False, True], ignore_index=True).head(10)\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n0\n2018-10-07\nTMAX\nGHCND:USW00014732\n,,W,2400\n27.8\n27\n82.04\n82\n\n\n1\n2018-10-10\nTMAX\nGHCND:USW00014732\n,,W,2400\n27.8\n27\n82.04\n82\n\n\n2\n2018-10-11\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.7\n26\n80.06\n80\n\n\n3\n2018-10-02\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n4\n2018-10-04\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n5\n2018-10-01\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n6\n2018-10-09\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n7\n2018-10-03\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.0\n25\n77.00\n77\n\n\n8\n2018-10-05\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.8\n22\n73.04\n73\n\n\n9\n2018-10-08\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.8\n22\n73.04\n73\n\n\n\n\n\n\n\nWhen just looking for the n-largest values, rather than wanting to sort all the data, we can use nlargest():\n\ndf[df.datatype == 'TAVG'].nlargest(n=10, columns='temp_C')\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n27\n2018-10-10\nTAVG\nGHCND:USW00014732\nH,,S,\n23.8\n23\n74.84\n74\n\n\n30\n2018-10-11\nTAVG\nGHCND:USW00014732\nH,,S,\n23.4\n23\n74.12\n74\n\n\n18\n2018-10-07\nTAVG\nGHCND:USW00014732\nH,,S,\n22.8\n22\n73.04\n73\n\n\n3\n2018-10-02\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n22\n72.86\n72\n\n\n6\n2018-10-03\nTAVG\nGHCND:USW00014732\nH,,S,\n21.8\n21\n71.24\n71\n\n\n24\n2018-10-09\nTAVG\nGHCND:USW00014732\nH,,S,\n21.8\n21\n71.24\n71\n\n\n9\n2018-10-04\nTAVG\nGHCND:USW00014732\nH,,S,\n21.3\n21\n70.34\n70\n\n\n0\n2018-10-01\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n21\n70.16\n70\n\n\n21\n2018-10-08\nTAVG\nGHCND:USW00014732\nH,,S,\n20.9\n20\n69.62\n69\n\n\n12\n2018-10-05\nTAVG\nGHCND:USW00014732\nH,,S,\n20.3\n20\n68.54\n68\n\n\n\n\n\n\n\nWe use nsmallest() for the n-smallest values.\n\ndf.nsmallest(n=5, columns=['temp_C', 'date'])\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n65\n2018-10-22\nTMIN\nGHCND:USW00014732\n,,W,2400\n5.6\n5\n42.08\n42\n\n\n77\n2018-10-26\nTMIN\nGHCND:USW00014732\n,,W,2400\n5.6\n5\n42.08\n42\n\n\n62\n2018-10-21\nTMIN\nGHCND:USW00014732\n,,W,2400\n6.1\n6\n42.98\n42\n\n\n74\n2018-10-25\nTMIN\nGHCND:USW00014732\n,,W,2400\n6.1\n6\n42.98\n42\n\n\n53\n2018-10-18\nTMIN\nGHCND:USW00014732\n,,W,2400\n6.7\n6\n44.06\n44\n\n\n\n\n\n\n\nThe sample() method will give us rows (or columns with axis=1) at random. We can provide a seed (random_state) to make this reproducible. The index after we do this is jumbled:\n\ndf.sample(5, random_state=0).index\n\nIndex([2, 30, 55, 16, 13], dtype='int64')\n\n\nWe can use sort_index() to order it again:\n\ndf.sample(5, random_state=0).sort_index().index\n\nIndex([2, 13, 16, 30, 55], dtype='int64')\n\n\nThe sort_index() method can also sort columns alphabetically:\n\ndf.sort_index(axis=1).head()\n\n\n\n\n\n\n\n\ndatatype\ndate\nflags\nstation\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n0\nTAVG\n2018-10-01\nH,,S,\nGHCND:USW00014732\n21.2\n21\n70.16\n70\n\n\n1\nTMAX\n2018-10-01\n,,W,2400\nGHCND:USW00014732\n25.6\n25\n78.08\n78\n\n\n2\nTMIN\n2018-10-01\n,,W,2400\nGHCND:USW00014732\n18.3\n18\n64.94\n64\n\n\n3\nTAVG\n2018-10-02\nH,,S,\nGHCND:USW00014732\n22.7\n22\n72.86\n72\n\n\n4\nTMAX\n2018-10-02\n,,W,2400\nGHCND:USW00014732\n26.1\n26\n78.98\n78\n\n\n\n\n\n\n\nThis can make selection with loc easier for many columns:\n\ndf.sort_index(axis=1).head().loc[:,'temp_C':'temp_F_whole']\n\n\n\n\n\n\n\n\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n0\n21.2\n21\n70.16\n70\n\n\n1\n25.6\n25\n78.08\n78\n\n\n2\n18.3\n18\n64.94\n64\n\n\n3\n22.7\n22\n72.86\n72\n\n\n4\n26.1\n26\n78.98\n78\n\n\n\n\n\n\n\nWe must sort the index to compare two dataframes. If the index is different, but the data is the same, they will be marked not-equal:\n\ndf.equals(df.sort_values(by='temp_C'))\n\nFalse\n\n\nSorting the index solves this issue:\n\ndf.equals(df.sort_values(by='temp_C').sort_index())\n\nTrue\n\n\nLet’s set the date column as our index:\n\ndf.set_index('date', inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n2018-10-01\nTAVG\nGHCND:USW00014732\nH,,S,\n21.2\n21\n70.16\n70\n\n\n2018-10-01\nTMAX\nGHCND:USW00014732\n,,W,2400\n25.6\n25\n78.08\n78\n\n\n2018-10-01\nTMIN\nGHCND:USW00014732\n,,W,2400\n18.3\n18\n64.94\n64\n\n\n2018-10-02\nTAVG\nGHCND:USW00014732\nH,,S,\n22.7\n22\n72.86\n72\n\n\n2018-10-02\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.1\n26\n78.98\n78\n\n\n\n\n\n\n\nNow that we have an index of type DatetimeIndex, we can do datetime slicing and indexing. As long as we provide a date format that pandas understands, we can grab the data. To select all of 2018, we simply use df.loc['2018'], for the fourth quarter of 2018 we can use df.loc['2018-Q4'], grabbing October is as simple as using df.loc['2018-10']; these can also be combined to build ranges. Let’s grab October 11, 2018 through October 12, 2018 (inclusive of both endpoints)—note that using loc[] is optional for ranges:\n\ndf['2018-10-11':'2018-10-12']\n\n\n\n\n\n\n\n\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n2018-10-11\nTAVG\nGHCND:USW00014732\nH,,S,\n23.4\n23\n74.12\n74\n\n\n2018-10-11\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.7\n26\n80.06\n80\n\n\n2018-10-11\nTMIN\nGHCND:USW00014732\n,,W,2400\n21.7\n21\n71.06\n71\n\n\n2018-10-12\nTAVG\nGHCND:USW00014732\nH,,S,\n18.3\n18\n64.94\n64\n\n\n2018-10-12\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.2\n22\n71.96\n71\n\n\n2018-10-12\nTMIN\nGHCND:USW00014732\n,,W,2400\n12.2\n12\n53.96\n53\n\n\n\n\n\n\n\nWe can also use reset_index() to get a fresh index and move our current index into a column for safe keeping. This is especially useful if we had data, such as the date, in the index that we don’t want to lose:\n\ndf['2018-10-11':'2018-10-12'].reset_index()\n\n\n\n\n\n\n\n\ndate\ndatatype\nstation\nflags\ntemp_C\ntemp_C_whole\ntemp_F\ntemp_F_whole\n\n\n\n\n0\n2018-10-11\nTAVG\nGHCND:USW00014732\nH,,S,\n23.4\n23\n74.12\n74\n\n\n1\n2018-10-11\nTMAX\nGHCND:USW00014732\n,,W,2400\n26.7\n26\n80.06\n80\n\n\n2\n2018-10-11\nTMIN\nGHCND:USW00014732\n,,W,2400\n21.7\n21\n71.06\n71\n\n\n3\n2018-10-12\nTAVG\nGHCND:USW00014732\nH,,S,\n18.3\n18\n64.94\n64\n\n\n4\n2018-10-12\nTMAX\nGHCND:USW00014732\n,,W,2400\n22.2\n22\n71.96\n71\n\n\n5\n2018-10-12\nTMIN\nGHCND:USW00014732\n,,W,2400\n12.2\n12\n53.96\n53\n\n\n\n\n\n\n\nReindexing allows us to conform our axis to contain a given set of labels. Let’s turn to the S&P 500 stock data in the sp500.csv file to see an example of this. Notice we only have data for trading days (weekdays, excluding holidays):\n\nsp = pd.read_csv(\n    '../data/sp500.csv', index_col='date', parse_dates=True\n).drop(columns=['adj_close'])\n\nsp.head(10).assign(\n    day_of_week=lambda x: x.index.day_name()\n)\n\n\n\n\n\n\n\n\nhigh\nlow\nopen\nclose\nvolume\nday_of_week\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2017-01-03\n2263.879883\n2245.129883\n2251.570068\n2257.830078\n3770530000\nTuesday\n\n\n2017-01-04\n2272.820068\n2261.600098\n2261.600098\n2270.750000\n3764890000\nWednesday\n\n\n2017-01-05\n2271.500000\n2260.449951\n2268.179932\n2269.000000\n3761820000\nThursday\n\n\n2017-01-06\n2282.100098\n2264.060059\n2271.139893\n2276.979980\n3339890000\nFriday\n\n\n2017-01-09\n2275.489990\n2268.899902\n2273.590088\n2268.899902\n3217610000\nMonday\n\n\n2017-01-10\n2279.270020\n2265.270020\n2269.719971\n2268.899902\n3638790000\nTuesday\n\n\n2017-01-11\n2275.320068\n2260.830078\n2268.600098\n2275.320068\n3620410000\nWednesday\n\n\n2017-01-12\n2271.780029\n2254.250000\n2271.139893\n2270.439941\n3462130000\nThursday\n\n\n2017-01-13\n2278.679932\n2271.510010\n2272.739990\n2274.639893\n3081270000\nFriday\n\n\n2017-01-17\n2272.080078\n2262.810059\n2269.139893\n2267.889893\n3584990000\nTuesday\n\n\n\n\n\n\n\nIf we want to look at the value of a portfolio (group of assets) that trade on different days, we need to handle the mismatch in the index. Bitcoin, for example, trades daily. If we sum up all the data we have for each day (aggregations will be covered in chapter 4, so don’t fixate on this part), we get the following:\n\nbitcoin = pd.read_csv(\n    '../data/bitcoin.csv', index_col='date', parse_dates=True\n).drop(columns=['market_cap'])\n\n# every day's closing price = S&P 500 close + Bitcoin close (same for other metrics)\nportfolio = pd.concat([sp, bitcoin], sort=False).groupby(level='date').sum()\n\nportfolio.head(10).assign(\n    day_of_week=lambda x: x.index.day_name()\n)\n\n\n\n\n\n\n\n\nhigh\nlow\nopen\nclose\nvolume\nday_of_week\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2017-01-01\n1003.080000\n958.700000\n963.660000\n998.330000\n147775008\nSunday\n\n\n2017-01-02\n1031.390000\n996.700000\n998.620000\n1021.750000\n222184992\nMonday\n\n\n2017-01-03\n3307.959883\n3266.729883\n3273.170068\n3301.670078\n3955698000\nTuesday\n\n\n2017-01-04\n3432.240068\n3306.000098\n3306.000098\n3425.480000\n4109835984\nWednesday\n\n\n2017-01-05\n3462.600000\n3170.869951\n3424.909932\n3282.380000\n4272019008\nThursday\n\n\n2017-01-06\n3328.910098\n3148.000059\n3285.379893\n3179.179980\n3691766000\nFriday\n\n\n2017-01-07\n908.590000\n823.560000\n903.490000\n908.590000\n279550016\nSaturday\n\n\n2017-01-08\n942.720000\n887.250000\n908.170000\n911.200000\n158715008\nSunday\n\n\n2017-01-09\n3189.179990\n3148.709902\n3186.830088\n3171.729902\n3359486992\nMonday\n\n\n2017-01-10\n3194.140020\n3166.330020\n3172.159971\n3176.579902\n3754598000\nTuesday\n\n\n\n\n\n\n\nIt may not be immediately obvious what is wrong with the previous data, but with a visualization we can easily see the cyclical pattern of drops on the days the stock market is closed. (Don’t worry about the plotting code too much, we will cover it in depth in chapters 5 and 6).\nWe will need to import matplotlib now:\n\nimport matplotlib.pyplot as plt # we use this module for plotting\nfrom matplotlib.ticker import StrMethodFormatter # for formatting the axis\n\nNow we can see why we need to reindex:\n\n# plot the closing price from Q4 2017 through Q2 2018\nax = portfolio['2017-Q4':'2018-Q2'].plot(\n    y='close', figsize=(15, 5), legend=False,\n    title='Bitcoin + S&P 500 value without accounting for different indices'\n)\n\n# formatting\nax.set_ylabel('price')\nax.yaxis.set_major_formatter(StrMethodFormatter('${x:,.0f}'))\nfor spine in ['top', 'right']:\n    ax.spines[spine].set_visible(False)\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\nWe need to align the index of the S&P 500 to match bitcoin in order to fix this. We will use the reindex() method, but by default we get NaN for the values that we don’t have data for:\n\nsp.reindex(bitcoin.index).head(10).assign(\n    day_of_week=lambda x: x.index.day_name()\n)\n\n\n\n\n\n\n\n\nhigh\nlow\nopen\nclose\nvolume\nday_of_week\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2017-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nSunday\n\n\n2017-01-02\nNaN\nNaN\nNaN\nNaN\nNaN\nMonday\n\n\n2017-01-03\n2263.879883\n2245.129883\n2251.570068\n2257.830078\n3.770530e+09\nTuesday\n\n\n2017-01-04\n2272.820068\n2261.600098\n2261.600098\n2270.750000\n3.764890e+09\nWednesday\n\n\n2017-01-05\n2271.500000\n2260.449951\n2268.179932\n2269.000000\n3.761820e+09\nThursday\n\n\n2017-01-06\n2282.100098\n2264.060059\n2271.139893\n2276.979980\n3.339890e+09\nFriday\n\n\n2017-01-07\nNaN\nNaN\nNaN\nNaN\nNaN\nSaturday\n\n\n2017-01-08\nNaN\nNaN\nNaN\nNaN\nNaN\nSunday\n\n\n2017-01-09\n2275.489990\n2268.899902\n2273.590088\n2268.899902\n3.217610e+09\nMonday\n\n\n2017-01-10\n2279.270020\n2265.270020\n2269.719971\n2268.899902\n3.638790e+09\nTuesday\n\n\n\n\n\n\n\nSo now we have rows for every day of the year, but all the weekends and holidays have NaN values. To address this, we can specify how to handle missing values with the method argument. In this case, we want to forward-fill, which will put the weekend and holiday values as the value they had for the Friday (or end of trading week) before:\n\nsp.reindex(bitcoin.index, method='ffill').head(10)\\\n    .assign(day_of_week=lambda x: x.index.day_name())\n\n\n\n\n\n\n\n\nhigh\nlow\nopen\nclose\nvolume\nday_of_week\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2017-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nSunday\n\n\n2017-01-02\nNaN\nNaN\nNaN\nNaN\nNaN\nMonday\n\n\n2017-01-03\n2263.879883\n2245.129883\n2251.570068\n2257.830078\n3.770530e+09\nTuesday\n\n\n2017-01-04\n2272.820068\n2261.600098\n2261.600098\n2270.750000\n3.764890e+09\nWednesday\n\n\n2017-01-05\n2271.500000\n2260.449951\n2268.179932\n2269.000000\n3.761820e+09\nThursday\n\n\n2017-01-06\n2282.100098\n2264.060059\n2271.139893\n2276.979980\n3.339890e+09\nFriday\n\n\n2017-01-07\n2282.100098\n2264.060059\n2271.139893\n2276.979980\n3.339890e+09\nSaturday\n\n\n2017-01-08\n2282.100098\n2264.060059\n2271.139893\n2276.979980\n3.339890e+09\nSunday\n\n\n2017-01-09\n2275.489990\n2268.899902\n2273.590088\n2268.899902\n3.217610e+09\nMonday\n\n\n2017-01-10\n2279.270020\n2265.270020\n2269.719971\n2268.899902\n3.638790e+09\nTuesday\n\n\n\n\n\n\n\nTo isolate the changes happening with the forward-filling, we can use the compare() method. It shows us the values that differ across identically-labeled dataframes (same names and same columns). Here, we can see that only weekends and holidays (Monday, January 16, 2017 was MLK day) have values forward-filled. Notice that consecutive days have the same values.\n\nsp.reindex(bitcoin.index)\\\n    .compare(sp.reindex(bitcoin.index, method='ffill'))\\\n    .head(10).assign(day_of_week=lambda x: x.index.day_name())\n\n\n\n\n\n\n\n\nhigh\nlow\nopen\nclose\nvolume\nday_of_week\n\n\n\nself\nother\nself\nother\nself\nother\nself\nother\nself\nother\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-01-07\nNaN\n2282.100098\nNaN\n2264.060059\nNaN\n2271.139893\nNaN\n2276.979980\nNaN\n3.339890e+09\nSaturday\n\n\n2017-01-08\nNaN\n2282.100098\nNaN\n2264.060059\nNaN\n2271.139893\nNaN\n2276.979980\nNaN\n3.339890e+09\nSunday\n\n\n2017-01-14\nNaN\n2278.679932\nNaN\n2271.510010\nNaN\n2272.739990\nNaN\n2274.639893\nNaN\n3.081270e+09\nSaturday\n\n\n2017-01-15\nNaN\n2278.679932\nNaN\n2271.510010\nNaN\n2272.739990\nNaN\n2274.639893\nNaN\n3.081270e+09\nSunday\n\n\n2017-01-16\nNaN\n2278.679932\nNaN\n2271.510010\nNaN\n2272.739990\nNaN\n2274.639893\nNaN\n3.081270e+09\nMonday\n\n\n2017-01-21\nNaN\n2276.959961\nNaN\n2265.010010\nNaN\n2269.959961\nNaN\n2271.310059\nNaN\n3.524970e+09\nSaturday\n\n\n2017-01-22\nNaN\n2276.959961\nNaN\n2265.010010\nNaN\n2269.959961\nNaN\n2271.310059\nNaN\n3.524970e+09\nSunday\n\n\n2017-01-28\nNaN\n2299.020020\nNaN\n2291.620117\nNaN\n2299.020020\nNaN\n2294.689941\nNaN\n3.135890e+09\nSaturday\n\n\n2017-01-29\nNaN\n2299.020020\nNaN\n2291.620117\nNaN\n2299.020020\nNaN\n2294.689941\nNaN\n3.135890e+09\nSunday\n\n\n2017-02-04\nNaN\n2298.310059\nNaN\n2287.879883\nNaN\n2288.540039\nNaN\n2297.419922\nNaN\n3.597970e+09\nSaturday\n\n\n\n\n\n\n\nThis isn’t perfect though. We probably want 0 for the volume traded and to put the closing price for the open, high, low, and close on the days the market is closed:\nThe reason why we’re using np.where(boolean condition, value if True, value if False) within lambda functions in the example below, is that vectorized operations allow us to be faster and more efficient than utilizing for loops to perform calculations on arrays all at once.\n\nimport numpy as np\n\nsp_reindexed = sp.reindex(bitcoin.index).assign(\n    volume=lambda x: x.volume.fillna(0), # put 0 when market is closed\n    close=lambda x: x.close.fillna(method='ffill'), # carry this forward\n    # take the closing price if these aren't available\n    open=lambda x: np.where(x.open.isnull(), x.close, x.open),\n    high=lambda x: np.where(x.high.isnull(), x.close, x.high),\n    low=lambda x: np.where(x.low.isnull(), x.close, x.low)\n)\nsp_reindexed.head(10).assign(\n    day_of_week=lambda x: x.index.day_name()\n)\n\n\n\n\n\n\n\n\nhigh\nlow\nopen\nclose\nvolume\nday_of_week\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2017-01-01\nNaN\nNaN\nNaN\nNaN\n0.000000e+00\nSunday\n\n\n2017-01-02\nNaN\nNaN\nNaN\nNaN\n0.000000e+00\nMonday\n\n\n2017-01-03\n2263.879883\n2245.129883\n2251.570068\n2257.830078\n3.770530e+09\nTuesday\n\n\n2017-01-04\n2272.820068\n2261.600098\n2261.600098\n2270.750000\n3.764890e+09\nWednesday\n\n\n2017-01-05\n2271.500000\n2260.449951\n2268.179932\n2269.000000\n3.761820e+09\nThursday\n\n\n2017-01-06\n2282.100098\n2264.060059\n2271.139893\n2276.979980\n3.339890e+09\nFriday\n\n\n2017-01-07\n2276.979980\n2276.979980\n2276.979980\n2276.979980\n0.000000e+00\nSaturday\n\n\n2017-01-08\n2276.979980\n2276.979980\n2276.979980\n2276.979980\n0.000000e+00\nSunday\n\n\n2017-01-09\n2275.489990\n2268.899902\n2273.590088\n2268.899902\n3.217610e+09\nMonday\n\n\n2017-01-10\n2279.270020\n2265.270020\n2269.719971\n2268.899902\n3.638790e+09\nTuesday\n\n\n\n\n\n\n\nIf we create a visualization comparing the reindexed data to the first attempt, we see how reindexing helped maintain the asset value when the market was closed:\n\n# every day's closing price = S&P 500 close adjusted for market closure + Bitcoin close (same for other metrics)\nfixed_portfolio = sp_reindexed + bitcoin\n\n# plot the reindexed portfolio's closing price from Q4 2017 through Q2 2018\nax = fixed_portfolio['2017-Q4':'2018-Q2'].plot(\n    y='close', label='reindexed portfolio of S&P 500 + Bitcoin', figsize=(15, 5), linewidth=2, \n    title='Reindexed portfolio vs. portfolio with mismatched indices'\n)\n\n# add line for original portfolio for comparison\nportfolio['2017-Q4':'2018-Q2'].plot(\n    y='close', ax=ax, linestyle='--', label='portfolio of S&P 500 + Bitcoin w/o reindexing'\n)\n\n# formatting\nax.set_ylabel('price')\nax.yaxis.set_major_formatter(StrMethodFormatter('${x:,.0f}'))\nfor spine in ['top', 'right']:\n    ax.spines[spine].set_visible(False)\n\n# show the plot\nplt.show()",
    "crumbs": [
      "Home",
      "Cleaning Data"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to Cary’s Introduction to Python Repository",
    "section": "",
    "text": "Thank you for taking the time to contribute to this project. We’re excited that you’re taking the time to become involved in this project.\n\n\n\nDemonstrations of Python packages\nEnriching documentation to existing files\n\n\n\n\nBefore we get started, here are a few things we expect from you (and that you should expect from others):\n\nBe kind and thoughtful in your conversations around this project. We all come from different backgrounds and projects, which means we likely have different perspectives on “how open source is done.” Try to listen to others rather than convince them that your way is correct.\nWhen adding content, please consider if it is widely valuable. Please don’t add references or links to things you or your employer have created, as others will do so if they appreciate it.\n\n\n\n\nIf you’d like to contribute, start by searching through the pull requests to see whether someone else has raised a similar idea or question.\nIf you don’t see your idea listed, and you think it fits into the goals of this guide, open a pull request.\n\n\n\nDiscussions about the Open Source Guides take place on this repository’s Issues and Pull Requests sections. Anybody is welcome to join these conversations.\nWherever possible, do not take these conversations to private channels, including contacting the maintainers directly. Keeping communication public means everybody can benefit and learn from the conversation."
  },
  {
    "objectID": "CONTRIBUTING.html#types-of-contributions-were-looking-for",
    "href": "CONTRIBUTING.html#types-of-contributions-were-looking-for",
    "title": "Contributing to Cary’s Introduction to Python Repository",
    "section": "",
    "text": "Demonstrations of Python packages\nEnriching documentation to existing files"
  },
  {
    "objectID": "CONTRIBUTING.html#ground-rules-expectations",
    "href": "CONTRIBUTING.html#ground-rules-expectations",
    "title": "Contributing to Cary’s Introduction to Python Repository",
    "section": "",
    "text": "Before we get started, here are a few things we expect from you (and that you should expect from others):\n\nBe kind and thoughtful in your conversations around this project. We all come from different backgrounds and projects, which means we likely have different perspectives on “how open source is done.” Try to listen to others rather than convince them that your way is correct.\nWhen adding content, please consider if it is widely valuable. Please don’t add references or links to things you or your employer have created, as others will do so if they appreciate it."
  },
  {
    "objectID": "CONTRIBUTING.html#how-to-contribute",
    "href": "CONTRIBUTING.html#how-to-contribute",
    "title": "Contributing to Cary’s Introduction to Python Repository",
    "section": "",
    "text": "If you’d like to contribute, start by searching through the pull requests to see whether someone else has raised a similar idea or question.\nIf you don’t see your idea listed, and you think it fits into the goals of this guide, open a pull request."
  },
  {
    "objectID": "CONTRIBUTING.html#community",
    "href": "CONTRIBUTING.html#community",
    "title": "Contributing to Cary’s Introduction to Python Repository",
    "section": "",
    "text": "Discussions about the Open Source Guides take place on this repository’s Issues and Pull Requests sections. Anybody is welcome to join these conversations.\nWherever possible, do not take these conversations to private channels, including contacting the maintainers directly. Keeping communication public means everybody can benefit and learn from the conversation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "For an Introduction to Data Analysis and a Refresher on Statistics\n\nintroduction_to_data_analysis.ipynb\n\nFor an Introduction to Hypothesis Testing\n\nhypothesis_testing.ipynb\n\nFor an Introduction to Python\n\npython_101.ipynb\n\nHandling Common Errors in Python\n\npython_errors.ipynb\n\nReading Local Files with pandas and visualizing datasets with matplotlib\n\nreading_local_files.ipynb\n\nMaking DataFrames from API requests\n\nmaking_dataframes_from_api_requests.ipynb\n\nWhat is wide vs long format data?\n\nwide_vs_long.ipynb\n\nCleaning data in pandas\n\ncleaning_data.ipynb\n\nHandling Data Issues in pandas\n\nhandling_data_issues.ipynb\n\nIntroduction to plotly.express\n\nintro_to_plotly_express.ipynb\n\nIntroduction to matplotlib\n\nintroducing_matplotlib.ipynb\n\nPlotting with plot() method for pandas objects\n\nplotting_with_pandas.ipynb\n\nIntroduction to pandas.plotting() module\n\npandas_plotting_module.ipynb\n\n\n\n\n\nYou can install project dependencies either using out-of-the-box conda CLI commands, or installing conda-lock to ensure dependencies are solved no matter the platform you are on.\nconda install -c conda-forge conda-lock\nconda-lock install --name name-of-your-environment conda-lock.yml\nconda activate name-of-your-environment\nconda env create -n name-of-your-environment --file environment.yml\nconda activate name-of-your-environment\n\n\n\n\nInstall your module using conda install name-of-module in your terminal or Anaconda Prompt\nInstall your module with Anaconda Navigator\n\nOpen Anaconda Navigator\nClick Environments tab\nSelect the Environment you want to install a module into\n\n\nPlease don’t use python -m pip install name-of-module when installing packages without activating your conda environment via conda activate name-of-environment first.\nAnaconda’s Explanation of conda & pip if you want a more in-depth explanation.\n\n\n\n\nGetting Started with Anaconda\nconda Basics\nJupyter Notebook Basics\n\n\n\n\n\nAnaconda Ecosystem Documentation\n\n\n\n\nAnaconda Navigator: Getting Started\nAnaconda Navigator Documentation\n\n\n\nIf your list of “Not Installed” packages is blank, I recommend manually updating Anaconda Navigator.\n\n\n\n\n\nconda: Getting Started\nconda Documenation\nconda Cheat Sheet\n\n\n\nIf you are experimenting with your conda base environment and need to restore a previous version of a conda.\n\nYou can use the conda list command with the --revisions flag to view your conda revision history.\nYou can use the conda install command with the --revision flag with the number that corresponds to the version you want to rollback to.\n\nbash or Powershell conda list --revisions conda install --revision N # Replace N with the number that corresponds to the version you want to rollback to.\n\n\n\n\nAnaconda: Conda is Fast Now. Like, Really Fast\nconda-libmamba-solver: Getting Started\nReplace it with mamba; even for existing conda installations\n\n\n\n\n\n\n\n\n\nPython.org’s ‘Whetting Your Appetite’\nPython.org’s Official Python 3 Tutorial\nPython.org’s Glossary of Terms\nPython.org’s Style Guide for Python Code\n\n\n\n\n\n\n\nFor in-memory analysis of tabular data and introduces the DataFrame for larger-than-memory datasets and want an API similar to pandas.\n\nLanding Page for polars\n\n\n\n\nFor in-memory analysis of tabular data and introduces the DataFrame\n\nLanding page for pandas\npandas Compared to Spreadsheets\npandas Compared to SQL Queries\npandas Compared to SAS\n\n\n\n\nIf you need your DataFrame to be mapped or do geospatial calculations.\n\nLanding page for geopandas\ngeopandas User Guide\n\n\n\n\nIf you need more interactivity from your generated map.\n\nLanding page for folium\n\n\n\n\nIf you need more control over your generated static map projection.\n\nLanding page for cartopy\n\n\n\n\nIf you need your dataset to have more than two dimensions.\n\nLanding Page for xarray\n\n\n\n\nHighly configurable visualization library that other libraries build off of.\n\nLanding page for matplotlib\n\n\n\n\nHigh-level library for generating statistical graphics, especially for long data format.\n\nLanding page for seaborn\n\n\n\n\nGenerate interactive graphics, with a focus on exploratory analysis with visuals.\n\nLanding page for plotly.express\n\n\n\n\n\nI highly recommend going through the official Python 3 tutorial first. It’s a great way to get your feet wet and get a feel for the language. However, here are some books I recommend if you want to go deeper or explore certain topics.\n\nAutomate the Boring Stuff with Python by Al Sweigart\nData Analysis with Pandas: 2nd Edition by Stefanie Molin\nData Science at the Command Line: 2nd Edition by Jeoroen Janssens\n\n\n\n\n\nRob Mulla on YouTube for Data Science with Focus on Python\n\nPlaylist: Medallion Python Data Science Coding Videos\nPlaylist: Working with Data in Python\n\nConference Talk: So you want to be a Python expert?\nConference Talk: 1000x faster data manipulation: vectorizing with pandas and numpy\nConference Talk: No More Sad Pandas: Optimizing Pandas Code for Sped and Efficiency\nConference Talk: Effective Pandas\nConference Tutorial: So You Wanna Be a Pandas Expert?\n\n\n\n\n\nInstalling Anaconda’s Package & Environment Manager conda (Command Line Interface Tool) and Anaconda-Navigator (Graphical User Interface Tool) (Best practice when it comes to dependency management for Python and R)\nPicking an editor of your choice that supports JupyterNotebooks (Visual Studio Code or JupyterLab)\nHow do I get data into Python and get descriptive statistics? (reading files with pandas)\nNow paint me a picture with the data. (introduction to Plotly & Holoviz Ecosystem)\nHow do I share this?\n\nBinder if you want interactivity (a little more setup)\nnbviewer if you value sharing your rendered files (less setup but not as pretty)\n\n\n\n\n\n\nHow to work with the Terminal\n\nUsing Powershell\n\nBasics of version control using Git & pushing to Github\n\nOfficial Git Documentation & Cheatsheets\nGit & Github Crash Course for Beginners\nWant Richer Change History for Notebooks? Try ReviewNB\n\n\n\n\n\nIf you have anything you want to cover, I’m open to suggestions. Feel free to checkout the contributing guidelines for ways to offer feedback and contribute. My previous experience with python covers web scraping, cleaning data, statistical analysis, and moving data into and out of databases."
  },
  {
    "objectID": "index.html#navigating-the-file-tree-in-this-repository",
    "href": "index.html#navigating-the-file-tree-in-this-repository",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "For an Introduction to Data Analysis and a Refresher on Statistics\n\nintroduction_to_data_analysis.ipynb\n\nFor an Introduction to Hypothesis Testing\n\nhypothesis_testing.ipynb\n\nFor an Introduction to Python\n\npython_101.ipynb\n\nHandling Common Errors in Python\n\npython_errors.ipynb\n\nReading Local Files with pandas and visualizing datasets with matplotlib\n\nreading_local_files.ipynb\n\nMaking DataFrames from API requests\n\nmaking_dataframes_from_api_requests.ipynb\n\nWhat is wide vs long format data?\n\nwide_vs_long.ipynb\n\nCleaning data in pandas\n\ncleaning_data.ipynb\n\nHandling Data Issues in pandas\n\nhandling_data_issues.ipynb\n\nIntroduction to plotly.express\n\nintro_to_plotly_express.ipynb\n\nIntroduction to matplotlib\n\nintroducing_matplotlib.ipynb\n\nPlotting with plot() method for pandas objects\n\nplotting_with_pandas.ipynb\n\nIntroduction to pandas.plotting() module\n\npandas_plotting_module.ipynb"
  },
  {
    "objectID": "index.html#installing-this-repositorys-depencies-using-conda-lock.yml-or-environment.yml-files",
    "href": "index.html#installing-this-repositorys-depencies-using-conda-lock.yml-or-environment.yml-files",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "You can install project dependencies either using out-of-the-box conda CLI commands, or installing conda-lock to ensure dependencies are solved no matter the platform you are on.\nconda install -c conda-forge conda-lock\nconda-lock install --name name-of-your-environment conda-lock.yml\nconda activate name-of-your-environment\nconda env create -n name-of-your-environment --file environment.yml\nconda activate name-of-your-environment"
  },
  {
    "objectID": "index.html#missing-an-imported-module",
    "href": "index.html#missing-an-imported-module",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "Install your module using conda install name-of-module in your terminal or Anaconda Prompt\nInstall your module with Anaconda Navigator\n\nOpen Anaconda Navigator\nClick Environments tab\nSelect the Environment you want to install a module into\n\n\nPlease don’t use python -m pip install name-of-module when installing packages without activating your conda environment via conda activate name-of-environment first.\nAnaconda’s Explanation of conda & pip if you want a more in-depth explanation."
  },
  {
    "objectID": "index.html#anacondas-free-with-account-creation-courses",
    "href": "index.html#anacondas-free-with-account-creation-courses",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "Getting Started with Anaconda\nconda Basics\nJupyter Notebook Basics"
  },
  {
    "objectID": "index.html#anaconda-ecosystem",
    "href": "index.html#anaconda-ecosystem",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "Anaconda Ecosystem Documentation\n\n\n\n\nAnaconda Navigator: Getting Started\nAnaconda Navigator Documentation\n\n\n\nIf your list of “Not Installed” packages is blank, I recommend manually updating Anaconda Navigator.\n\n\n\n\n\nconda: Getting Started\nconda Documenation\nconda Cheat Sheet\n\n\n\nIf you are experimenting with your conda base environment and need to restore a previous version of a conda.\n\nYou can use the conda list command with the --revisions flag to view your conda revision history.\nYou can use the conda install command with the --revision flag with the number that corresponds to the version you want to rollback to.\n\nbash or Powershell conda list --revisions conda install --revision N # Replace N with the number that corresponds to the version you want to rollback to.\n\n\n\n\nAnaconda: Conda is Fast Now. Like, Really Fast\nconda-libmamba-solver: Getting Started\nReplace it with mamba; even for existing conda installations"
  },
  {
    "objectID": "index.html#python-3",
    "href": "index.html#python-3",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "Python.org’s ‘Whetting Your Appetite’\nPython.org’s Official Python 3 Tutorial\nPython.org’s Glossary of Terms\nPython.org’s Style Guide for Python Code"
  },
  {
    "objectID": "index.html#python-modules",
    "href": "index.html#python-modules",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "For in-memory analysis of tabular data and introduces the DataFrame for larger-than-memory datasets and want an API similar to pandas.\n\nLanding Page for polars\n\n\n\n\nFor in-memory analysis of tabular data and introduces the DataFrame\n\nLanding page for pandas\npandas Compared to Spreadsheets\npandas Compared to SQL Queries\npandas Compared to SAS\n\n\n\n\nIf you need your DataFrame to be mapped or do geospatial calculations.\n\nLanding page for geopandas\ngeopandas User Guide\n\n\n\n\nIf you need more interactivity from your generated map.\n\nLanding page for folium\n\n\n\n\nIf you need more control over your generated static map projection.\n\nLanding page for cartopy\n\n\n\n\nIf you need your dataset to have more than two dimensions.\n\nLanding Page for xarray\n\n\n\n\nHighly configurable visualization library that other libraries build off of.\n\nLanding page for matplotlib\n\n\n\n\nHigh-level library for generating statistical graphics, especially for long data format.\n\nLanding page for seaborn\n\n\n\n\nGenerate interactive graphics, with a focus on exploratory analysis with visuals.\n\nLanding page for plotly.express"
  },
  {
    "objectID": "index.html#recommended-books",
    "href": "index.html#recommended-books",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "I highly recommend going through the official Python 3 tutorial first. It’s a great way to get your feet wet and get a feel for the language. However, here are some books I recommend if you want to go deeper or explore certain topics.\n\nAutomate the Boring Stuff with Python by Al Sweigart\nData Analysis with Pandas: 2nd Edition by Stefanie Molin\nData Science at the Command Line: 2nd Edition by Jeoroen Janssens"
  },
  {
    "objectID": "index.html#youtube-resources-to-check-out",
    "href": "index.html#youtube-resources-to-check-out",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "Rob Mulla on YouTube for Data Science with Focus on Python\n\nPlaylist: Medallion Python Data Science Coding Videos\nPlaylist: Working with Data in Python\n\nConference Talk: So you want to be a Python expert?\nConference Talk: 1000x faster data manipulation: vectorizing with pandas and numpy\nConference Talk: No More Sad Pandas: Optimizing Pandas Code for Sped and Efficiency\nConference Talk: Effective Pandas\nConference Tutorial: So You Wanna Be a Pandas Expert?"
  },
  {
    "objectID": "index.html#topics-on-the-table",
    "href": "index.html#topics-on-the-table",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "Installing Anaconda’s Package & Environment Manager conda (Command Line Interface Tool) and Anaconda-Navigator (Graphical User Interface Tool) (Best practice when it comes to dependency management for Python and R)\nPicking an editor of your choice that supports JupyterNotebooks (Visual Studio Code or JupyterLab)\nHow do I get data into Python and get descriptive statistics? (reading files with pandas)\nNow paint me a picture with the data. (introduction to Plotly & Holoviz Ecosystem)\nHow do I share this?\n\nBinder if you want interactivity (a little more setup)\nnbviewer if you value sharing your rendered files (less setup but not as pretty)"
  },
  {
    "objectID": "index.html#sidequests",
    "href": "index.html#sidequests",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "How to work with the Terminal\n\nUsing Powershell\n\nBasics of version control using Git & pushing to Github\n\nOfficial Git Documentation & Cheatsheets\nGit & Github Crash Course for Beginners\nWant Richer Change History for Notebooks? Try ReviewNB"
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Cary Introduction to Python & Anaconda",
    "section": "",
    "text": "If you have anything you want to cover, I’m open to suggestions. Feel free to checkout the contributing guidelines for ways to offer feedback and contribute. My previous experience with python covers web scraping, cleaning data, statistical analysis, and moving data into and out of databases."
  },
  {
    "objectID": "notebooks/handling_data_issues.html",
    "href": "notebooks/handling_data_issues.html",
    "title": "Handling duplicate, missing, or invalid data",
    "section": "",
    "text": "In this notebook, we will using daily weather data that was taken from the National Centers for Environmental Information (NCEI) API and altered to introduce many common problems faced when working with data.\nNote: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for “NCEI weather API” to find the updated one.\n\n\n\nData meanings: - PRCP: precipitation in millimeters - SNOW: snowfall in millimeters - SNWD: snow depth in millimeters - TMAX: maximum daily temperature in Celsius - TMIN: minimum daily temperature in Celsius - TOBS: temperature at time of observation in Celsius - WESF: water equivalent of snow in millimeters\nSome important facts to get our bearings: - According to the National Weather Service, the coldest temperature ever recorded in Central Park was -15°F (-26.1°C) on February 9, 1934: source - The temperature of the Sun’s photosphere is approximately 5,505°C: source\n\n\n\nWe need to import pandas and read in the dirty data to get started:\n\nimport pandas as pd\n\ndf = pd.read_csv('../data/dirty_data.csv')\n\n\n\n\nA good first step is to look at some rows:\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\nstation\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\n\n\n0\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n1\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n2\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n3\n2018-01-02T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-8.3\n-16.1\n-12.2\nNaN\nFalse\n\n\n4\n2018-01-03T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n\n\n\n\n\nLooking at summary statistics can reveal strange or missing values:\n\ndf.describe()\n\nc:\\Users\\gpower\\AppData\\Local\\mambaforge\\envs\\cary_dev\\Lib\\site-packages\\numpy\\lib\\function_base.py:4573: RuntimeWarning: invalid value encountered in subtract\n  diff_b_a = subtract(b, a)\n\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\n\n\n\n\ncount\n765.000000\n577.000000\n577.0\n765.000000\n765.000000\n398.000000\n11.000000\n\n\nmean\n5.360392\n4.202773\nNaN\n2649.175294\n-15.914379\n8.632161\n16.290909\n\n\nstd\n10.002138\n25.086077\nNaN\n2744.156281\n24.242849\n9.815054\n9.489832\n\n\nmin\n0.000000\n0.000000\n-inf\n-11.700000\n-40.000000\n-16.100000\n1.800000\n\n\n25%\n0.000000\n0.000000\nNaN\n13.300000\n-40.000000\n0.150000\n8.600000\n\n\n50%\n0.000000\n0.000000\nNaN\n32.800000\n-11.100000\n8.300000\n19.300000\n\n\n75%\n5.800000\n0.000000\nNaN\n5505.000000\n6.700000\n18.300000\n24.900000\n\n\nmax\n61.700000\n229.000000\ninf\n5505.000000\n23.900000\n26.100000\n28.700000\n\n\n\n\n\n\n\nThe info() method can pinpoint missing values and wrong data types:\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 765 entries, 0 to 764\nData columns (total 10 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   date               765 non-null    object \n 1   station            765 non-null    object \n 2   PRCP               765 non-null    float64\n 3   SNOW               577 non-null    float64\n 4   SNWD               577 non-null    float64\n 5   TMAX               765 non-null    float64\n 6   TMIN               765 non-null    float64\n 7   TOBS               398 non-null    float64\n 8   WESF               11 non-null     float64\n 9   inclement_weather  408 non-null    object \ndtypes: float64(7), object(3)\nmemory usage: 59.9+ KB\n\n\nWe can use the isna()/isnull() method of the series to find nulls:\n\ncontain_nulls = df[\n    df.SNOW.isna() | df.SNWD.isna() | df.TOBS.isna()\n    | df.WESF.isna() | df.inclement_weather.isna()\n]\ncontain_nulls.shape[0]\n\n765\n\n\n\ncontain_nulls.head(10)\n\n\n\n\n\n\n\n\ndate\nstation\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\n\n\n0\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n1\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n2\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n3\n2018-01-02T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-8.3\n-16.1\n-12.2\nNaN\nFalse\n\n\n4\n2018-01-03T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n5\n2018-01-03T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n6\n2018-01-03T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n7\n2018-01-04T00:00:00\n?\n20.6\n229.0\ninf\n5505.0\n-40.0\nNaN\n19.3\nTrue\n\n\n8\n2018-01-04T00:00:00\n?\n20.6\n229.0\ninf\n5505.0\n-40.0\nNaN\n19.3\nTrue\n\n\n9\n2018-01-05T00:00:00\n?\n0.3\nNaN\nNaN\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nNote that we can’t check if we have NaN like this:\n\ndf[df.inclement_weather == 'NaN'].shape[0]\n\n0\n\n\nThis is because it is actually np.nan. However, notice this also doesn’t work:\n\nimport numpy as np\ndf[df.inclement_weather == np.nan].shape[0]\n\n0\n\n\nWe have to use one of the methods discussed earlier for this to work:\n\ndf[df.inclement_weather.isna()].shape[0]\n\n357\n\n\nWe can find -inf/inf by comparing to -np.inf/np.inf:\n\ndf[df.SNWD.isin([-np.inf, np.inf])].shape[0]\n\n577\n\n\nRather than do this for each column, we can write a function that will use a dictionary comprehension to check all the columns for us:\n\ndef get_inf_count(df):\n    \"\"\"Find the number of inf/-inf values per column in the dataframe\"\"\"\n    return {\n        col: df[df[col].isin([np.inf, -np.inf])].shape[0] for col in df.columns\n    }\n\nget_inf_count(df)\n\n{'date': 0,\n 'station': 0,\n 'PRCP': 0,\n 'SNOW': 0,\n 'SNWD': 577,\n 'TMAX': 0,\n 'TMIN': 0,\n 'TOBS': 0,\n 'WESF': 0,\n 'inclement_weather': 0}\n\n\nBefore we can decide how to handle the infinite values of snow depth, we should look at the summary statistics for snowfall, which forms a big part in determining the snow depth:\n\npd.DataFrame({\n    'np.inf Snow Depth': df[df.SNWD == np.inf].SNOW.describe(),\n    '-np.inf Snow Depth': df[df.SNWD == -np.inf].SNOW.describe()\n}).T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnp.inf Snow Depth\n24.0\n101.041667\n74.498018\n13.0\n25.0\n120.5\n152.0\n229.0\n\n\n-np.inf Snow Depth\n553.0\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\nLet’s now look into the date and station columns. We saw the ? for station earlier, so we know that was the other unique value. However, we see that some dates are present 8 times in the data and we only have 324 days meaning we are also missing days:\n\ndf.describe(include='object')\n\n\n\n\n\n\n\n\ndate\nstation\ninclement_weather\n\n\n\n\ncount\n765\n765\n408\n\n\nunique\n324\n2\n2\n\n\ntop\n2018-07-05T00:00:00\nGHCND:USC00280907\nFalse\n\n\nfreq\n8\n398\n384\n\n\n\n\n\n\n\nWe can use the duplicated() method to find duplicate rows:\n\ndf[df.duplicated()].shape[0]\n\n284\n\n\nThe default for keep is 'first' meaning it won’t show the first row that the duplicated data was seen in; we can pass in False to see it though:\n\ndf[df.duplicated(keep=False)].shape[0]\n\n482\n\n\nWe can also specify the columns to use:\n\ndf[df.duplicated(['date', 'station'])].shape[0]\n\n284\n\n\nLet’s look at a few duplicates. Just in the few values we see here, we know that the top 4 are actually in the data 6 times because by default we aren’t seeing their first occurrence:\n\ndf[df.duplicated()].head()\n\n\n\n\n\n\n\n\ndate\nstation\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\n\n\n1\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n2\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n5\n2018-01-03T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n6\n2018-01-03T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n8\n2018-01-04T00:00:00\n?\n20.6\n229.0\ninf\n5505.0\n-40.0\nNaN\n19.3\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\nSince we know we have NY weather data and noticed we only had two entries for station, we may decide to drop the station column because we are only interested in the weather data. However, when dealing with duplicate data, we need to think of the ramifications of removing it. Notice we only have data for the WESF column when the station is ?:\n\ndf[df.WESF.notna()].station.unique()\n\narray(['?'], dtype=object)\n\n\nIf we determine it won’t impact our analysis, we can use drop_duplicates() to remove them:\n\n# 1. make the date a datetime\ndf.date = pd.to_datetime(df.date)\n\n# 2. save this information for later\nstation_qm_wesf = df[df.station == '?'].drop_duplicates('date').set_index('date').WESF\n\n# 3. sort ? to the bottom\ndf.sort_values('station', ascending=False, inplace=True)\n\n# 4. drop duplicates based on the date column keeping the first occurrence \n# which will be the valid station if it has data\ndf_deduped = df.drop_duplicates('date')\n\n# 5. remove the station column because we are done with it\ndf_deduped = df_deduped.drop(columns='station').set_index('date').sort_index()\n\n# 6. take valid station's WESF and fall back on station ? if it is null\ndf_deduped = df_deduped.assign(\n    WESF=lambda x: x.WESF.combine_first(station_qm_wesf)\n)\n\ndf_deduped.shape\n\n(324, 8)\n\n\nHere we used the combine_first() method to coalesce the values to the first non-null entry; this means that if we had data from both stations, we would first take the value provided by the named station and if (and only if) that station was null would we take the value from the station named ?. The following table contains some examples of how this would play out:\n\n\n\nstation GHCND:USC00280907\nstation ?\nresult of combine_first()\n\n\n\n\n1\n17\n1\n\n\n1\nNaN\n1\n\n\nNaN\n17\n17\n\n\nNaN\nNaN\nNaN\n\n\n\nCheck out the 4th row—we have WESF in the correct spot thanks to the index:\n\ndf_deduped.head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-inf\n-8.3\n-16.1\n-12.2\nNaN\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n2018-01-04\n20.6\n229.0\ninf\n5505.0\n-40.0\nNaN\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\ninf\n-4.4\n-13.9\n-13.9\nNaN\nTrue\n\n\n\n\n\n\n\n\n\n\nWe could drop nulls, replace them with some arbitrary value, or impute them using the surrounding data. Each of these options may have ramifications, so we must choose wisely.\nWe can use dropna() to drop rows where any column has a null value. The default options leave us hardly any data:\n\ndf_deduped.dropna().shape\n\n(4, 8)\n\n\nIf we pass how='all', we can choose to only drop rows where everything is null, but this removes nothing:\n\ndf_deduped.dropna(how='all').shape\n\n(324, 8)\n\n\nWe can use just a subset of columns to determine what to drop with the subset argument:\n\ndf_deduped.dropna(\n    how='all', subset=['inclement_weather', 'SNOW', 'SNWD']\n).shape\n\n(293, 8)\n\n\nThis can also be performed along columns, and we can also require a certain number of null values before we drop the data:\n\ndf_deduped.dropna(axis='columns', thresh=df_deduped.shape[0] * .75).columns\n\nIndex(['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'TOBS', 'inclement_weather'], dtype='object')\n\n\nWe can choose to fill in the null values instead with fillna():\n\ndf_deduped.loc[:,'WESF'].fillna(0, inplace=True)\ndf_deduped.head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-inf\n-8.3\n-16.1\n-12.2\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\ninf\n5505.0\n-40.0\nNaN\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\ninf\n-4.4\n-13.9\n-13.9\n0.0\nTrue\n\n\n\n\n\n\n\nAt this point we have done everything we can without distorting the data. We know that we are missing dates, but if we reindex, we don’t know how to fill in the NaN data. With the weather data, we can’t assume because it snowed one day that it will snow the next or that the temperature will be the same. For this reason, note that the next few examples are just for illustrative purposes only—just because we can do something doesn’t mean we should.\nThat being said, let’s try to address some of remaining issues with the temperature data. We know that when TMAX is the temperature of the Sun, it must be because there was no measured value, so let’s replace it with NaN. We will also do so for TMIN which currently uses -40°C for its placeholder when we know that the coldest temperature ever recorded in NYC was -15°F (-26.1°C) on February 9, 1934:\n\ndf_deduped = df_deduped.assign(\n    TMAX=lambda x: x.TMAX.replace(5505, np.nan),\n    TMIN=lambda x: x.TMIN.replace(-40, np.nan),\n)\n\nWe will also make an assumption that the temperature won’t change drastically day-to-day. Note that this is actually a big assumption, but it will allow us to understand how fillna() works when we provide a strategy through the method parameter. The fillna() method gives us 2 options for the method parameter: - 'ffill' to forward-fill - 'bfill' to back-fill\nNote that 'nearest' is missing because we are not reindexing.\nHere, we will use 'ffill' to show how this works:\n\ndf_deduped.assign(\n    TMAX=lambda x: x.TMAX.fillna(method='ffill'),\n    TMIN=lambda x: x.TMIN.fillna(method='ffill')\n).head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n-inf\nNaN\nNaN\nNaN\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-inf\n-8.3\n-16.1\n-12.2\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\ninf\n-4.4\n-13.9\nNaN\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\ninf\n-4.4\n-13.9\n-13.9\n0.0\nTrue\n\n\n\n\n\n\n\nWe can use np.nan_to_num() to turn np.nan into 0 and -np.inf/np.inf into large negative or positive finite numbers:\n\ndf_deduped.assign(\n    SNWD=lambda x: np.nan_to_num(x.SNWD)\n).head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n-1.797693e+308\nNaN\nNaN\nNaN\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-1.797693e+308\n-8.3\n-16.1\n-12.2\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-1.797693e+308\n-4.4\n-13.9\n-13.3\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\n1.797693e+308\nNaN\nNaN\nNaN\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\n1.797693e+308\n-4.4\n-13.9\n-13.9\n0.0\nTrue\n\n\n\n\n\n\n\nDepending on the data we are working with, we can use the clip() method as an alternative to np.nan_to_num(). The clip() method makes it possible to cap values at a specific minimum and/or maximum threshold. Since SNWD can’t be negative, let’s use clip() to enforce a lower bound of zero. To show how the upper bound works, let’s use the value of SNOW:\n\ndf_deduped.assign(\n    SNWD=lambda x: x.SNWD.clip(0, x.SNOW)\n).head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n0.0\nNaN\nNaN\nNaN\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n0.0\n-8.3\n-16.1\n-12.2\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n0.0\n-4.4\n-13.9\n-13.3\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\n229.0\nNaN\nNaN\nNaN\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\n127.0\n-4.4\n-13.9\n-13.9\n0.0\nTrue\n\n\n\n\n\n\n\nWe can couple fillna() with other types of calculations. Here we replace missing values of TMAX with the median of all TMAX values, TMIN with the median of all TMIN values, and TOBS to the average of the TMAX and TMIN values. Since we place TOBS last, we have access to the imputed values for TMIN and TMAX in the calculation:\n\ndf_deduped.assign(\n    TMAX=lambda x: x.TMAX.fillna(x.TMAX.median()),\n    TMIN=lambda x: x.TMIN.fillna(x.TMIN.median()),\n    # average of TMAX and TMIN\n    TOBS=lambda x: x.TOBS.fillna((x.TMAX + x.TMIN) / 2)\n).head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n-inf\n14.4\n5.6\n10.0\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-inf\n-8.3\n-16.1\n-12.2\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\ninf\n14.4\n5.6\n10.0\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\ninf\n-4.4\n-13.9\n-13.9\n0.0\nTrue\n\n\n\n\n\n\n\nWe can also use apply() for running the same calculation across columns. For example, let’s fill all missing values with their rolling 7-day median of their values, setting the number of periods required for the calculation to 0 to ensure we don’t introduce more extra NaN values. Rolling calculations will be covered later on, so this is a preview:\n\ndf_deduped.apply(\n    # rolling calculations will be covered later on, this is a rolling 7-day median\n    # we set min_periods (# of periods required for calculation) to 0 so we always get a result \n    lambda x: x.fillna(x.rolling(7, min_periods=0).median())\n).head(10)\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n-inf\nNaN\nNaN\nNaN\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-inf\n-8.30\n-16.1\n-12.20\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-inf\n-4.40\n-13.9\n-13.30\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\ninf\n-6.35\n-15.0\n-12.75\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\ninf\n-4.40\n-13.9\n-13.90\n0.0\nTrue\n\n\n2018-01-06\n0.0\n0.0\n-inf\n-10.00\n-15.6\n-15.00\n0.0\nFalse\n\n\n2018-01-07\n0.0\n0.0\n-inf\n-11.70\n-17.2\n-16.10\n0.0\nFalse\n\n\n2018-01-08\n0.0\n0.0\n-inf\n-7.80\n-16.7\n-8.30\n0.0\nFalse\n\n\n2018-01-10\n0.0\n0.0\n-inf\n5.00\n-7.8\n-7.80\n0.0\nFalse\n\n\n2018-01-11\n0.0\n0.0\n-inf\n4.40\n-7.8\n1.10\n0.0\nFalse\n\n\n\n\n\n\n\nThe last strategy we could try is interpolation with the interpolate() method. We specify the method parameter with the interpolation strategy to use. There are many options, but we will stick with the default of 'linear', which will treat values as evenly spaced and place missing values in the middle of existing ones. We have some missing data, so we will reindex first. Look at January 9th, which we didn’t have before—the values for TMAX, TMIN, and TOBS are the average of values the day prior (January 8th) and the day after (January 10th):\n\ndf_deduped\\\n    .reindex(pd.date_range('2018-01-01', '2018-12-31', freq='D'))\\\n    .apply(lambda x: x.interpolate())\\\n    .head(10)\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\n\n\n2018-01-01\n0.0\n0.0\n-inf\nNaN\nNaN\nNaN\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-inf\n-8.3\n-16.10\n-12.20\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-inf\n-4.4\n-13.90\n-13.30\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\ninf\n-4.4\n-13.90\n-13.60\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\ninf\n-4.4\n-13.90\n-13.90\n0.0\nTrue\n\n\n2018-01-06\n0.0\n0.0\n-inf\n-10.0\n-15.60\n-15.00\n0.0\nFalse\n\n\n2018-01-07\n0.0\n0.0\n-inf\n-11.7\n-17.20\n-16.10\n0.0\nFalse\n\n\n2018-01-08\n0.0\n0.0\n-inf\n-7.8\n-16.70\n-8.30\n0.0\nFalse\n\n\n2018-01-09\n0.0\n0.0\n-inf\n-1.4\n-12.25\n-8.05\n0.0\nNaN\n\n\n2018-01-10\n0.0\n0.0\n-inf\n5.0\n-7.80\n-7.80\n0.0\nFalse",
    "crumbs": [
      "Home",
      "Handling duplicate, missing, or invalid data"
    ]
  },
  {
    "objectID": "notebooks/handling_data_issues.html#about-the-data",
    "href": "notebooks/handling_data_issues.html#about-the-data",
    "title": "Handling duplicate, missing, or invalid data",
    "section": "",
    "text": "In this notebook, we will using daily weather data that was taken from the National Centers for Environmental Information (NCEI) API and altered to introduce many common problems faced when working with data.\nNote: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for “NCEI weather API” to find the updated one.",
    "crumbs": [
      "Home",
      "Handling duplicate, missing, or invalid data"
    ]
  },
  {
    "objectID": "notebooks/handling_data_issues.html#background-on-the-data",
    "href": "notebooks/handling_data_issues.html#background-on-the-data",
    "title": "Handling duplicate, missing, or invalid data",
    "section": "",
    "text": "Data meanings: - PRCP: precipitation in millimeters - SNOW: snowfall in millimeters - SNWD: snow depth in millimeters - TMAX: maximum daily temperature in Celsius - TMIN: minimum daily temperature in Celsius - TOBS: temperature at time of observation in Celsius - WESF: water equivalent of snow in millimeters\nSome important facts to get our bearings: - According to the National Weather Service, the coldest temperature ever recorded in Central Park was -15°F (-26.1°C) on February 9, 1934: source - The temperature of the Sun’s photosphere is approximately 5,505°C: source",
    "crumbs": [
      "Home",
      "Handling duplicate, missing, or invalid data"
    ]
  },
  {
    "objectID": "notebooks/handling_data_issues.html#setup",
    "href": "notebooks/handling_data_issues.html#setup",
    "title": "Handling duplicate, missing, or invalid data",
    "section": "",
    "text": "We need to import pandas and read in the dirty data to get started:\n\nimport pandas as pd\n\ndf = pd.read_csv('../data/dirty_data.csv')",
    "crumbs": [
      "Home",
      "Handling duplicate, missing, or invalid data"
    ]
  },
  {
    "objectID": "notebooks/handling_data_issues.html#finding-problematic-data",
    "href": "notebooks/handling_data_issues.html#finding-problematic-data",
    "title": "Handling duplicate, missing, or invalid data",
    "section": "",
    "text": "A good first step is to look at some rows:\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\nstation\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\n\n\n0\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n1\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n2\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n3\n2018-01-02T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-8.3\n-16.1\n-12.2\nNaN\nFalse\n\n\n4\n2018-01-03T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n\n\n\n\n\nLooking at summary statistics can reveal strange or missing values:\n\ndf.describe()\n\nc:\\Users\\gpower\\AppData\\Local\\mambaforge\\envs\\cary_dev\\Lib\\site-packages\\numpy\\lib\\function_base.py:4573: RuntimeWarning: invalid value encountered in subtract\n  diff_b_a = subtract(b, a)\n\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\n\n\n\n\ncount\n765.000000\n577.000000\n577.0\n765.000000\n765.000000\n398.000000\n11.000000\n\n\nmean\n5.360392\n4.202773\nNaN\n2649.175294\n-15.914379\n8.632161\n16.290909\n\n\nstd\n10.002138\n25.086077\nNaN\n2744.156281\n24.242849\n9.815054\n9.489832\n\n\nmin\n0.000000\n0.000000\n-inf\n-11.700000\n-40.000000\n-16.100000\n1.800000\n\n\n25%\n0.000000\n0.000000\nNaN\n13.300000\n-40.000000\n0.150000\n8.600000\n\n\n50%\n0.000000\n0.000000\nNaN\n32.800000\n-11.100000\n8.300000\n19.300000\n\n\n75%\n5.800000\n0.000000\nNaN\n5505.000000\n6.700000\n18.300000\n24.900000\n\n\nmax\n61.700000\n229.000000\ninf\n5505.000000\n23.900000\n26.100000\n28.700000\n\n\n\n\n\n\n\nThe info() method can pinpoint missing values and wrong data types:\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 765 entries, 0 to 764\nData columns (total 10 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   date               765 non-null    object \n 1   station            765 non-null    object \n 2   PRCP               765 non-null    float64\n 3   SNOW               577 non-null    float64\n 4   SNWD               577 non-null    float64\n 5   TMAX               765 non-null    float64\n 6   TMIN               765 non-null    float64\n 7   TOBS               398 non-null    float64\n 8   WESF               11 non-null     float64\n 9   inclement_weather  408 non-null    object \ndtypes: float64(7), object(3)\nmemory usage: 59.9+ KB\n\n\nWe can use the isna()/isnull() method of the series to find nulls:\n\ncontain_nulls = df[\n    df.SNOW.isna() | df.SNWD.isna() | df.TOBS.isna()\n    | df.WESF.isna() | df.inclement_weather.isna()\n]\ncontain_nulls.shape[0]\n\n765\n\n\n\ncontain_nulls.head(10)\n\n\n\n\n\n\n\n\ndate\nstation\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\n\n\n0\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n1\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n2\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n3\n2018-01-02T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-8.3\n-16.1\n-12.2\nNaN\nFalse\n\n\n4\n2018-01-03T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n5\n2018-01-03T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n6\n2018-01-03T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n7\n2018-01-04T00:00:00\n?\n20.6\n229.0\ninf\n5505.0\n-40.0\nNaN\n19.3\nTrue\n\n\n8\n2018-01-04T00:00:00\n?\n20.6\n229.0\ninf\n5505.0\n-40.0\nNaN\n19.3\nTrue\n\n\n9\n2018-01-05T00:00:00\n?\n0.3\nNaN\nNaN\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nNote that we can’t check if we have NaN like this:\n\ndf[df.inclement_weather == 'NaN'].shape[0]\n\n0\n\n\nThis is because it is actually np.nan. However, notice this also doesn’t work:\n\nimport numpy as np\ndf[df.inclement_weather == np.nan].shape[0]\n\n0\n\n\nWe have to use one of the methods discussed earlier for this to work:\n\ndf[df.inclement_weather.isna()].shape[0]\n\n357\n\n\nWe can find -inf/inf by comparing to -np.inf/np.inf:\n\ndf[df.SNWD.isin([-np.inf, np.inf])].shape[0]\n\n577\n\n\nRather than do this for each column, we can write a function that will use a dictionary comprehension to check all the columns for us:\n\ndef get_inf_count(df):\n    \"\"\"Find the number of inf/-inf values per column in the dataframe\"\"\"\n    return {\n        col: df[df[col].isin([np.inf, -np.inf])].shape[0] for col in df.columns\n    }\n\nget_inf_count(df)\n\n{'date': 0,\n 'station': 0,\n 'PRCP': 0,\n 'SNOW': 0,\n 'SNWD': 577,\n 'TMAX': 0,\n 'TMIN': 0,\n 'TOBS': 0,\n 'WESF': 0,\n 'inclement_weather': 0}\n\n\nBefore we can decide how to handle the infinite values of snow depth, we should look at the summary statistics for snowfall, which forms a big part in determining the snow depth:\n\npd.DataFrame({\n    'np.inf Snow Depth': df[df.SNWD == np.inf].SNOW.describe(),\n    '-np.inf Snow Depth': df[df.SNWD == -np.inf].SNOW.describe()\n}).T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnp.inf Snow Depth\n24.0\n101.041667\n74.498018\n13.0\n25.0\n120.5\n152.0\n229.0\n\n\n-np.inf Snow Depth\n553.0\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\nLet’s now look into the date and station columns. We saw the ? for station earlier, so we know that was the other unique value. However, we see that some dates are present 8 times in the data and we only have 324 days meaning we are also missing days:\n\ndf.describe(include='object')\n\n\n\n\n\n\n\n\ndate\nstation\ninclement_weather\n\n\n\n\ncount\n765\n765\n408\n\n\nunique\n324\n2\n2\n\n\ntop\n2018-07-05T00:00:00\nGHCND:USC00280907\nFalse\n\n\nfreq\n8\n398\n384\n\n\n\n\n\n\n\nWe can use the duplicated() method to find duplicate rows:\n\ndf[df.duplicated()].shape[0]\n\n284\n\n\nThe default for keep is 'first' meaning it won’t show the first row that the duplicated data was seen in; we can pass in False to see it though:\n\ndf[df.duplicated(keep=False)].shape[0]\n\n482\n\n\nWe can also specify the columns to use:\n\ndf[df.duplicated(['date', 'station'])].shape[0]\n\n284\n\n\nLet’s look at a few duplicates. Just in the few values we see here, we know that the top 4 are actually in the data 6 times because by default we aren’t seeing their first occurrence:\n\ndf[df.duplicated()].head()\n\n\n\n\n\n\n\n\ndate\nstation\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\n\n\n1\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n2\n2018-01-01T00:00:00\n?\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n5\n2018-01-03T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n6\n2018-01-03T00:00:00\nGHCND:USC00280907\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n8\n2018-01-04T00:00:00\n?\n20.6\n229.0\ninf\n5505.0\n-40.0\nNaN\n19.3\nTrue",
    "crumbs": [
      "Home",
      "Handling duplicate, missing, or invalid data"
    ]
  },
  {
    "objectID": "notebooks/handling_data_issues.html#mitigating-issues",
    "href": "notebooks/handling_data_issues.html#mitigating-issues",
    "title": "Handling duplicate, missing, or invalid data",
    "section": "",
    "text": "Since we know we have NY weather data and noticed we only had two entries for station, we may decide to drop the station column because we are only interested in the weather data. However, when dealing with duplicate data, we need to think of the ramifications of removing it. Notice we only have data for the WESF column when the station is ?:\n\ndf[df.WESF.notna()].station.unique()\n\narray(['?'], dtype=object)\n\n\nIf we determine it won’t impact our analysis, we can use drop_duplicates() to remove them:\n\n# 1. make the date a datetime\ndf.date = pd.to_datetime(df.date)\n\n# 2. save this information for later\nstation_qm_wesf = df[df.station == '?'].drop_duplicates('date').set_index('date').WESF\n\n# 3. sort ? to the bottom\ndf.sort_values('station', ascending=False, inplace=True)\n\n# 4. drop duplicates based on the date column keeping the first occurrence \n# which will be the valid station if it has data\ndf_deduped = df.drop_duplicates('date')\n\n# 5. remove the station column because we are done with it\ndf_deduped = df_deduped.drop(columns='station').set_index('date').sort_index()\n\n# 6. take valid station's WESF and fall back on station ? if it is null\ndf_deduped = df_deduped.assign(\n    WESF=lambda x: x.WESF.combine_first(station_qm_wesf)\n)\n\ndf_deduped.shape\n\n(324, 8)\n\n\nHere we used the combine_first() method to coalesce the values to the first non-null entry; this means that if we had data from both stations, we would first take the value provided by the named station and if (and only if) that station was null would we take the value from the station named ?. The following table contains some examples of how this would play out:\n\n\n\nstation GHCND:USC00280907\nstation ?\nresult of combine_first()\n\n\n\n\n1\n17\n1\n\n\n1\nNaN\n1\n\n\nNaN\n17\n17\n\n\nNaN\nNaN\nNaN\n\n\n\nCheck out the 4th row—we have WESF in the correct spot thanks to the index:\n\ndf_deduped.head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\nNaN\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-inf\n-8.3\n-16.1\n-12.2\nNaN\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\nNaN\nFalse\n\n\n2018-01-04\n20.6\n229.0\ninf\n5505.0\n-40.0\nNaN\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\ninf\n-4.4\n-13.9\n-13.9\nNaN\nTrue\n\n\n\n\n\n\n\n\n\n\nWe could drop nulls, replace them with some arbitrary value, or impute them using the surrounding data. Each of these options may have ramifications, so we must choose wisely.\nWe can use dropna() to drop rows where any column has a null value. The default options leave us hardly any data:\n\ndf_deduped.dropna().shape\n\n(4, 8)\n\n\nIf we pass how='all', we can choose to only drop rows where everything is null, but this removes nothing:\n\ndf_deduped.dropna(how='all').shape\n\n(324, 8)\n\n\nWe can use just a subset of columns to determine what to drop with the subset argument:\n\ndf_deduped.dropna(\n    how='all', subset=['inclement_weather', 'SNOW', 'SNWD']\n).shape\n\n(293, 8)\n\n\nThis can also be performed along columns, and we can also require a certain number of null values before we drop the data:\n\ndf_deduped.dropna(axis='columns', thresh=df_deduped.shape[0] * .75).columns\n\nIndex(['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'TOBS', 'inclement_weather'], dtype='object')\n\n\nWe can choose to fill in the null values instead with fillna():\n\ndf_deduped.loc[:,'WESF'].fillna(0, inplace=True)\ndf_deduped.head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n-inf\n5505.0\n-40.0\nNaN\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-inf\n-8.3\n-16.1\n-12.2\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\ninf\n5505.0\n-40.0\nNaN\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\ninf\n-4.4\n-13.9\n-13.9\n0.0\nTrue\n\n\n\n\n\n\n\nAt this point we have done everything we can without distorting the data. We know that we are missing dates, but if we reindex, we don’t know how to fill in the NaN data. With the weather data, we can’t assume because it snowed one day that it will snow the next or that the temperature will be the same. For this reason, note that the next few examples are just for illustrative purposes only—just because we can do something doesn’t mean we should.\nThat being said, let’s try to address some of remaining issues with the temperature data. We know that when TMAX is the temperature of the Sun, it must be because there was no measured value, so let’s replace it with NaN. We will also do so for TMIN which currently uses -40°C for its placeholder when we know that the coldest temperature ever recorded in NYC was -15°F (-26.1°C) on February 9, 1934:\n\ndf_deduped = df_deduped.assign(\n    TMAX=lambda x: x.TMAX.replace(5505, np.nan),\n    TMIN=lambda x: x.TMIN.replace(-40, np.nan),\n)\n\nWe will also make an assumption that the temperature won’t change drastically day-to-day. Note that this is actually a big assumption, but it will allow us to understand how fillna() works when we provide a strategy through the method parameter. The fillna() method gives us 2 options for the method parameter: - 'ffill' to forward-fill - 'bfill' to back-fill\nNote that 'nearest' is missing because we are not reindexing.\nHere, we will use 'ffill' to show how this works:\n\ndf_deduped.assign(\n    TMAX=lambda x: x.TMAX.fillna(method='ffill'),\n    TMIN=lambda x: x.TMIN.fillna(method='ffill')\n).head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n-inf\nNaN\nNaN\nNaN\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-inf\n-8.3\n-16.1\n-12.2\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\ninf\n-4.4\n-13.9\nNaN\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\ninf\n-4.4\n-13.9\n-13.9\n0.0\nTrue\n\n\n\n\n\n\n\nWe can use np.nan_to_num() to turn np.nan into 0 and -np.inf/np.inf into large negative or positive finite numbers:\n\ndf_deduped.assign(\n    SNWD=lambda x: np.nan_to_num(x.SNWD)\n).head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n-1.797693e+308\nNaN\nNaN\nNaN\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-1.797693e+308\n-8.3\n-16.1\n-12.2\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-1.797693e+308\n-4.4\n-13.9\n-13.3\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\n1.797693e+308\nNaN\nNaN\nNaN\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\n1.797693e+308\n-4.4\n-13.9\n-13.9\n0.0\nTrue\n\n\n\n\n\n\n\nDepending on the data we are working with, we can use the clip() method as an alternative to np.nan_to_num(). The clip() method makes it possible to cap values at a specific minimum and/or maximum threshold. Since SNWD can’t be negative, let’s use clip() to enforce a lower bound of zero. To show how the upper bound works, let’s use the value of SNOW:\n\ndf_deduped.assign(\n    SNWD=lambda x: x.SNWD.clip(0, x.SNOW)\n).head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n0.0\nNaN\nNaN\nNaN\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n0.0\n-8.3\n-16.1\n-12.2\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n0.0\n-4.4\n-13.9\n-13.3\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\n229.0\nNaN\nNaN\nNaN\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\n127.0\n-4.4\n-13.9\n-13.9\n0.0\nTrue\n\n\n\n\n\n\n\nWe can couple fillna() with other types of calculations. Here we replace missing values of TMAX with the median of all TMAX values, TMIN with the median of all TMIN values, and TOBS to the average of the TMAX and TMIN values. Since we place TOBS last, we have access to the imputed values for TMIN and TMAX in the calculation:\n\ndf_deduped.assign(\n    TMAX=lambda x: x.TMAX.fillna(x.TMAX.median()),\n    TMIN=lambda x: x.TMIN.fillna(x.TMIN.median()),\n    # average of TMAX and TMIN\n    TOBS=lambda x: x.TOBS.fillna((x.TMAX + x.TMIN) / 2)\n).head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n-inf\n14.4\n5.6\n10.0\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-inf\n-8.3\n-16.1\n-12.2\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-inf\n-4.4\n-13.9\n-13.3\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\ninf\n14.4\n5.6\n10.0\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\ninf\n-4.4\n-13.9\n-13.9\n0.0\nTrue\n\n\n\n\n\n\n\nWe can also use apply() for running the same calculation across columns. For example, let’s fill all missing values with their rolling 7-day median of their values, setting the number of periods required for the calculation to 0 to ensure we don’t introduce more extra NaN values. Rolling calculations will be covered later on, so this is a preview:\n\ndf_deduped.apply(\n    # rolling calculations will be covered later on, this is a rolling 7-day median\n    # we set min_periods (# of periods required for calculation) to 0 so we always get a result \n    lambda x: x.fillna(x.rolling(7, min_periods=0).median())\n).head(10)\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n0.0\n0.0\n-inf\nNaN\nNaN\nNaN\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-inf\n-8.30\n-16.1\n-12.20\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-inf\n-4.40\n-13.9\n-13.30\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\ninf\n-6.35\n-15.0\n-12.75\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\ninf\n-4.40\n-13.9\n-13.90\n0.0\nTrue\n\n\n2018-01-06\n0.0\n0.0\n-inf\n-10.00\n-15.6\n-15.00\n0.0\nFalse\n\n\n2018-01-07\n0.0\n0.0\n-inf\n-11.70\n-17.2\n-16.10\n0.0\nFalse\n\n\n2018-01-08\n0.0\n0.0\n-inf\n-7.80\n-16.7\n-8.30\n0.0\nFalse\n\n\n2018-01-10\n0.0\n0.0\n-inf\n5.00\n-7.8\n-7.80\n0.0\nFalse\n\n\n2018-01-11\n0.0\n0.0\n-inf\n4.40\n-7.8\n1.10\n0.0\nFalse\n\n\n\n\n\n\n\nThe last strategy we could try is interpolation with the interpolate() method. We specify the method parameter with the interpolation strategy to use. There are many options, but we will stick with the default of 'linear', which will treat values as evenly spaced and place missing values in the middle of existing ones. We have some missing data, so we will reindex first. Look at January 9th, which we didn’t have before—the values for TMAX, TMIN, and TOBS are the average of values the day prior (January 8th) and the day after (January 10th):\n\ndf_deduped\\\n    .reindex(pd.date_range('2018-01-01', '2018-12-31', freq='D'))\\\n    .apply(lambda x: x.interpolate())\\\n    .head(10)\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nSNWD\nTMAX\nTMIN\nTOBS\nWESF\ninclement_weather\n\n\n\n\n2018-01-01\n0.0\n0.0\n-inf\nNaN\nNaN\nNaN\n0.0\nNaN\n\n\n2018-01-02\n0.0\n0.0\n-inf\n-8.3\n-16.10\n-12.20\n0.0\nFalse\n\n\n2018-01-03\n0.0\n0.0\n-inf\n-4.4\n-13.90\n-13.30\n0.0\nFalse\n\n\n2018-01-04\n20.6\n229.0\ninf\n-4.4\n-13.90\n-13.60\n19.3\nTrue\n\n\n2018-01-05\n14.2\n127.0\ninf\n-4.4\n-13.90\n-13.90\n0.0\nTrue\n\n\n2018-01-06\n0.0\n0.0\n-inf\n-10.0\n-15.60\n-15.00\n0.0\nFalse\n\n\n2018-01-07\n0.0\n0.0\n-inf\n-11.7\n-17.20\n-16.10\n0.0\nFalse\n\n\n2018-01-08\n0.0\n0.0\n-inf\n-7.8\n-16.70\n-8.30\n0.0\nFalse\n\n\n2018-01-09\n0.0\n0.0\n-inf\n-1.4\n-12.25\n-8.05\n0.0\nNaN\n\n\n2018-01-10\n0.0\n0.0\n-inf\n5.0\n-7.80\n-7.80\n0.0\nFalse",
    "crumbs": [
      "Home",
      "Handling duplicate, missing, or invalid data"
    ]
  },
  {
    "objectID": "notebooks/introducing_matplotlib.html",
    "href": "notebooks/introducing_matplotlib.html",
    "title": "Getting Started with Matplotlib",
    "section": "",
    "text": "Pandas uses matplotlib to create visualizations. Therefore, before we learn how to plot with pandas, it’s important to understand how matplotlib works at a high-level, which is the focus of this notebook.\n\n\nIn this notebook, we will be working with 2 datasets: - Facebook’s stock price throughout 2018 (obtained using the stock_analysis package) - Earthquake data from September 18, 2018 - October 13, 2018 (obtained from the US Geological Survey (USGS) using the USGS API)\n\n\n\nWe need to import matplotlib.pyplot for plotting.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\n\n\nfb = pd.read_csv(\n    '../data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True\n)\n\nplt.plot(fb.index, fb.open)\nplt.show()\n\n\n\n\n\n\n\n\nSince we are working in a Jupyter notebook, we can use the magic command %matplotlib inline once and not have to call plt.show() for each plot.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfb = pd.read_csv(\n    '../data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True\n)\nplt.plot(fb.index, fb.open)\n\n\n\n\n\n\n\n\n\n\n\nWe can pass in a string specifying the style of the plot. This is of the form [marker][linestyle][color]. For example, we can make a black dashed line with '--k' or a red scatter plot with 'or':\n\nplt.plot('high', 'low', 'or', data=fb.head(20))\n\n\n\n\n\n\n\n\nHere are some examples of how you make a format string:\n\n\n\nMarker\nLinestyle\nColor\nFormat String\nResult\n\n\n\n\n\n-\nb\n-b\nblue solid line\n\n\n.\n\nk\n.k\nblack points\n\n\n\n--\nr\n--r\nred dashed line\n\n\no\n-\ng\no-g\ngreen solid line with circles\n\n\n\n:\nm\n:m\nmagenta dotted line\n\n\nx\n-.\nc\nx-.c\ncyan dot-dashed line with x’s\n\n\n\nNote that we can also use format strings of the form [color][marker][linestyle], but the parsing by matplotlib (in rare cases) might not be what we were aiming for. Consult the Notes section in the documentation for the complete list of options. ## Histograms\n\nquakes = pd.read_csv('../data/earthquakes.csv')\nplt.hist(quakes.query('magType == \"ml\"').mag)\n\n(array([6.400e+01, 4.450e+02, 1.137e+03, 1.853e+03, 2.114e+03, 8.070e+02,\n        2.800e+02, 9.200e+01, 9.000e+00, 2.000e+00]),\n array([-1.26 , -0.624,  0.012,  0.648,  1.284,  1.92 ,  2.556,  3.192,\n         3.828,  4.464,  5.1  ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n\nNotice how our assumptions of the distribution of the data can change based on the number of bins (look at the drop between the two highest peaks on the righthand plot):\n\nx = quakes.query('magType == \"ml\"').mag\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\nfor ax, bins in zip(axes, [7, 35]):\n    ax.hist(x, bins=bins)\n    ax.set_title(f'bins param: {bins}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop-level object that holds the other plot components.\n\nfig = plt.figure()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\nIndividual plots contained within the Figure.\n\n\n\n\nSimply specify the number of rows and columns to create:\n\nfig, axes = plt.subplots(1, 2)\n\n\n\n\n\n\n\n\nAs an alternative to using plt.subplots() we can add Axes objects to the Figure object on our own. This allows for some more complex layouts, such as picture in picture:\n\nfig = plt.figure(figsize=(3, 3))\noutside = fig.add_axes([0.1, 0.1, 0.9, 0.9])\ninside = fig.add_axes([0.7, 0.7, 0.25, 0.25])\n\n\n\n\n\n\n\n\n\n\n\nWe can create subplots with varying sizes as well:\n\nfig = plt.figure(figsize=(8, 8))\ngs = fig.add_gridspec(3, 3)\ntop_left = fig.add_subplot(gs[0, 0])\nmid_left = fig.add_subplot(gs[1, 0])\ntop_right = fig.add_subplot(gs[:2, 1:])\nbottom = fig.add_subplot(gs[2,:])\n\n\n\n\n\n\n\n\n\n\n\nUse plt.savefig() to save the last created plot. To save a specific Figure object, use its savefig() method. Which supports ‘png’, ‘pdf’, ‘svg’, and ‘eps’ filetypes.\n\nfig.savefig('empty.png')\nfig.savefig('empty.pdf')\nfig.savefig('empty.svg')\nfig.savefig('empty.eps')\n\n\n\n\nIt’s important to close resources when we are done with them. We use plt.close() to do so. If we pass in nothing, it will close the last plot, but we can pass in the specific Figure object to close or say 'all' to close all Figure objects that are open. Let’s close all the Figure objects that are open with plt.close():\n\nplt.close('all')\n\n\n\n\n\n\nJust pass the figsize argument to plt.figure(). It’s a tuple of (width, height):\n\nfig = plt.figure(figsize=(10, 4))\n\n&lt;Figure size 1000x400 with 0 Axes&gt;\n\n\nThis can be specified when creating subplots as well:\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n\n\n\n\n\n\n\n\n\n\nA small subset of all the available plot settings (shuffling to get a good variation of options):\n\nimport random\nimport matplotlib as mpl\n\nrcparams_list = list(mpl.rcParams.keys())\nrandom.seed(20) # make this repeatable\nrandom.shuffle(rcparams_list)\nsorted(rcparams_list[:20])\n\n['animation.convert_args',\n 'axes.edgecolor',\n 'axes.formatter.use_locale',\n 'axes.spines.right',\n 'boxplot.meanprops.markersize',\n 'boxplot.showfliers',\n 'keymap.home',\n 'lines.markerfacecolor',\n 'lines.scale_dashes',\n 'mathtext.rm',\n 'patch.force_edgecolor',\n 'savefig.facecolor',\n 'svg.fonttype',\n 'text.hinting_factor',\n 'xtick.alignment',\n 'xtick.minor.top',\n 'xtick.minor.width',\n 'ytick.left',\n 'ytick.major.left',\n 'ytick.minor.width']\n\n\nWe can check the current default figsize using rcParams:\n\nmpl.rcParams['figure.figsize']\n\n[6.4, 4.8]\n\n\nWe can also update this value to change the default (until the kernel is restarted):\n\nmpl.rcParams['figure.figsize'] = (300, 10)\nmpl.rcParams['figure.figsize']\n\n[300.0, 10.0]\n\n\nUse rcdefaults() to restore the defaults. Note this is slightly different than before because running %matplotlib inline sets a different value for figsize (see more). After we reset, we are going back to the default value of figsize before that import:\n\nmpl.rcdefaults()\nmpl.rcParams['figure.figsize']\n\n[6.4, 4.8]\n\n\nThis can also be done via pyplot:\n\nplt.rc('figure', figsize=(20, 20)) # change `figsize` default to (20, 20)\nplt.rcdefaults() # reset the default",
    "crumbs": [
      "Home",
      "Getting Started with Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introducing_matplotlib.html#about-the-data",
    "href": "notebooks/introducing_matplotlib.html#about-the-data",
    "title": "Getting Started with Matplotlib",
    "section": "",
    "text": "In this notebook, we will be working with 2 datasets: - Facebook’s stock price throughout 2018 (obtained using the stock_analysis package) - Earthquake data from September 18, 2018 - October 13, 2018 (obtained from the US Geological Survey (USGS) using the USGS API)",
    "crumbs": [
      "Home",
      "Getting Started with Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introducing_matplotlib.html#setup",
    "href": "notebooks/introducing_matplotlib.html#setup",
    "title": "Getting Started with Matplotlib",
    "section": "",
    "text": "We need to import matplotlib.pyplot for plotting.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd",
    "crumbs": [
      "Home",
      "Getting Started with Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introducing_matplotlib.html#plotting-lines",
    "href": "notebooks/introducing_matplotlib.html#plotting-lines",
    "title": "Getting Started with Matplotlib",
    "section": "",
    "text": "fb = pd.read_csv(\n    '../data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True\n)\n\nplt.plot(fb.index, fb.open)\nplt.show()\n\n\n\n\n\n\n\n\nSince we are working in a Jupyter notebook, we can use the magic command %matplotlib inline once and not have to call plt.show() for each plot.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfb = pd.read_csv(\n    '../data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True\n)\nplt.plot(fb.index, fb.open)",
    "crumbs": [
      "Home",
      "Getting Started with Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introducing_matplotlib.html#scatter-plots",
    "href": "notebooks/introducing_matplotlib.html#scatter-plots",
    "title": "Getting Started with Matplotlib",
    "section": "",
    "text": "We can pass in a string specifying the style of the plot. This is of the form [marker][linestyle][color]. For example, we can make a black dashed line with '--k' or a red scatter plot with 'or':\n\nplt.plot('high', 'low', 'or', data=fb.head(20))\n\n\n\n\n\n\n\n\nHere are some examples of how you make a format string:\n\n\n\nMarker\nLinestyle\nColor\nFormat String\nResult\n\n\n\n\n\n-\nb\n-b\nblue solid line\n\n\n.\n\nk\n.k\nblack points\n\n\n\n--\nr\n--r\nred dashed line\n\n\no\n-\ng\no-g\ngreen solid line with circles\n\n\n\n:\nm\n:m\nmagenta dotted line\n\n\nx\n-.\nc\nx-.c\ncyan dot-dashed line with x’s\n\n\n\nNote that we can also use format strings of the form [color][marker][linestyle], but the parsing by matplotlib (in rare cases) might not be what we were aiming for. Consult the Notes section in the documentation for the complete list of options. ## Histograms\n\nquakes = pd.read_csv('../data/earthquakes.csv')\nplt.hist(quakes.query('magType == \"ml\"').mag)\n\n(array([6.400e+01, 4.450e+02, 1.137e+03, 1.853e+03, 2.114e+03, 8.070e+02,\n        2.800e+02, 9.200e+01, 9.000e+00, 2.000e+00]),\n array([-1.26 , -0.624,  0.012,  0.648,  1.284,  1.92 ,  2.556,  3.192,\n         3.828,  4.464,  5.1  ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n\nNotice how our assumptions of the distribution of the data can change based on the number of bins (look at the drop between the two highest peaks on the righthand plot):\n\nx = quakes.query('magType == \"ml\"').mag\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\nfor ax, bins in zip(axes, [7, 35]):\n    ax.hist(x, bins=bins)\n    ax.set_title(f'bins param: {bins}')",
    "crumbs": [
      "Home",
      "Getting Started with Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introducing_matplotlib.html#plot-components",
    "href": "notebooks/introducing_matplotlib.html#plot-components",
    "title": "Getting Started with Matplotlib",
    "section": "",
    "text": "Top-level object that holds the other plot components.\n\nfig = plt.figure()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\nIndividual plots contained within the Figure.",
    "crumbs": [
      "Home",
      "Getting Started with Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introducing_matplotlib.html#creating-subplots",
    "href": "notebooks/introducing_matplotlib.html#creating-subplots",
    "title": "Getting Started with Matplotlib",
    "section": "",
    "text": "Simply specify the number of rows and columns to create:\n\nfig, axes = plt.subplots(1, 2)\n\n\n\n\n\n\n\n\nAs an alternative to using plt.subplots() we can add Axes objects to the Figure object on our own. This allows for some more complex layouts, such as picture in picture:\n\nfig = plt.figure(figsize=(3, 3))\noutside = fig.add_axes([0.1, 0.1, 0.9, 0.9])\ninside = fig.add_axes([0.7, 0.7, 0.25, 0.25])",
    "crumbs": [
      "Home",
      "Getting Started with Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introducing_matplotlib.html#creating-plot-layouts-with-gridspec",
    "href": "notebooks/introducing_matplotlib.html#creating-plot-layouts-with-gridspec",
    "title": "Getting Started with Matplotlib",
    "section": "",
    "text": "We can create subplots with varying sizes as well:\n\nfig = plt.figure(figsize=(8, 8))\ngs = fig.add_gridspec(3, 3)\ntop_left = fig.add_subplot(gs[0, 0])\nmid_left = fig.add_subplot(gs[1, 0])\ntop_right = fig.add_subplot(gs[:2, 1:])\nbottom = fig.add_subplot(gs[2,:])",
    "crumbs": [
      "Home",
      "Getting Started with Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introducing_matplotlib.html#saving-plots",
    "href": "notebooks/introducing_matplotlib.html#saving-plots",
    "title": "Getting Started with Matplotlib",
    "section": "",
    "text": "Use plt.savefig() to save the last created plot. To save a specific Figure object, use its savefig() method. Which supports ‘png’, ‘pdf’, ‘svg’, and ‘eps’ filetypes.\n\nfig.savefig('empty.png')\nfig.savefig('empty.pdf')\nfig.savefig('empty.svg')\nfig.savefig('empty.eps')",
    "crumbs": [
      "Home",
      "Getting Started with Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introducing_matplotlib.html#cleaning-up",
    "href": "notebooks/introducing_matplotlib.html#cleaning-up",
    "title": "Getting Started with Matplotlib",
    "section": "",
    "text": "It’s important to close resources when we are done with them. We use plt.close() to do so. If we pass in nothing, it will close the last plot, but we can pass in the specific Figure object to close or say 'all' to close all Figure objects that are open. Let’s close all the Figure objects that are open with plt.close():\n\nplt.close('all')",
    "crumbs": [
      "Home",
      "Getting Started with Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introducing_matplotlib.html#additional-plotting-options",
    "href": "notebooks/introducing_matplotlib.html#additional-plotting-options",
    "title": "Getting Started with Matplotlib",
    "section": "",
    "text": "Just pass the figsize argument to plt.figure(). It’s a tuple of (width, height):\n\nfig = plt.figure(figsize=(10, 4))\n\n&lt;Figure size 1000x400 with 0 Axes&gt;\n\n\nThis can be specified when creating subplots as well:\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n\n\n\n\n\n\n\n\n\n\nA small subset of all the available plot settings (shuffling to get a good variation of options):\n\nimport random\nimport matplotlib as mpl\n\nrcparams_list = list(mpl.rcParams.keys())\nrandom.seed(20) # make this repeatable\nrandom.shuffle(rcparams_list)\nsorted(rcparams_list[:20])\n\n['animation.convert_args',\n 'axes.edgecolor',\n 'axes.formatter.use_locale',\n 'axes.spines.right',\n 'boxplot.meanprops.markersize',\n 'boxplot.showfliers',\n 'keymap.home',\n 'lines.markerfacecolor',\n 'lines.scale_dashes',\n 'mathtext.rm',\n 'patch.force_edgecolor',\n 'savefig.facecolor',\n 'svg.fonttype',\n 'text.hinting_factor',\n 'xtick.alignment',\n 'xtick.minor.top',\n 'xtick.minor.width',\n 'ytick.left',\n 'ytick.major.left',\n 'ytick.minor.width']\n\n\nWe can check the current default figsize using rcParams:\n\nmpl.rcParams['figure.figsize']\n\n[6.4, 4.8]\n\n\nWe can also update this value to change the default (until the kernel is restarted):\n\nmpl.rcParams['figure.figsize'] = (300, 10)\nmpl.rcParams['figure.figsize']\n\n[300.0, 10.0]\n\n\nUse rcdefaults() to restore the defaults. Note this is slightly different than before because running %matplotlib inline sets a different value for figsize (see more). After we reset, we are going back to the default value of figsize before that import:\n\nmpl.rcdefaults()\nmpl.rcParams['figure.figsize']\n\n[6.4, 4.8]\n\n\nThis can also be done via pyplot:\n\nplt.rc('figure', figsize=(20, 20)) # change `figsize` default to (20, 20)\nplt.rcdefaults() # reset the default",
    "crumbs": [
      "Home",
      "Getting Started with Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/intro_to_plotly_express.html",
    "href": "notebooks/intro_to_plotly_express.html",
    "title": "Introduction to Plotly Express",
    "section": "",
    "text": "The plotly.express module (usually imported as px) contains functions that can create entire figures at once, and is referred to as Plotly Express or PX. Plotly Express is a built-in part of the plotly library, and is the recommended starting point for creating most common figures. Every Plotly Express function uses graph objects internally and returns a plotly.graph_objects.Figure instance. Throughout the plotly documentation, you will find the Plotly Express way of building figures at the top of any applicable page, followed by a section on how to use graph objects to build similar figures. Any figure created in a single function call with Plotly Express could be created using graph objects alone, but with between 5 and 100 times more code.\n\nimport plotly.express as px\n\ndf = px.data.gapminder().query(\"year==2007\")\n\n\ndf.columns\n\nIndex(['country', 'continent', 'year', 'lifeExp', 'pop', 'gdpPercap',\n       'iso_alpha', 'iso_num'],\n      dtype='object')\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nyear\nlifeExp\npop\ngdpPercap\niso_num\n\n\n\n\ncount\n142.0\n142.000000\n1.420000e+02\n142.000000\n142.000000\n\n\nmean\n2007.0\n67.007423\n4.402122e+07\n11680.071820\n425.880282\n\n\nstd\n0.0\n12.073021\n1.476214e+08\n12859.937337\n249.111541\n\n\nmin\n2007.0\n39.613000\n1.995790e+05\n277.551859\n4.000000\n\n\n25%\n2007.0\n57.160250\n4.508034e+06\n1624.842248\n209.500000\n\n\n50%\n2007.0\n71.935500\n1.051753e+07\n6124.371109\n410.000000\n\n\n75%\n2007.0\n76.413250\n3.121004e+07\n18008.835640\n636.000000\n\n\nmax\n2007.0\n82.603000\n1.318683e+09\n49357.190170\n894.000000\n\n\n\n\n\n\n\n\npx.strip(df, x='lifeExp', hover_name=\"country\")\n\n                                                \n\n\n\npx.strip(df, x='lifeExp', color=\"continent\", hover_name=\"country\")\n\n                                                \n\n\n\npx.histogram(df, x='lifeExp', color=\"continent\", hover_name=\"country\")\n\n                                                \n\n\n\npx.histogram(df, x='lifeExp', color=\"continent\", hover_name=\"country\", marginal=\"rug\")\n\n                                                \n\n\n\npx.histogram(df, x='lifeExp', y=\"pop\", color=\"continent\", hover_name=\"country\", marginal=\"rug\")\n\n                                                \n\n\n\npx.histogram(df, x='lifeExp', y=\"pop\", color=\"continent\", hover_name=\"country\", marginal=\"rug\", facet_col=\"continent\")\n\n                                                \n\n\n\npx.bar(df, color='lifeExp', x=\"pop\", y=\"continent\", hover_name=\"country\")\n\n                                                \n\n\n\npx.sunburst(df, color='lifeExp', values=\"pop\", path=[\"continent\", \"country\"], hover_name=\"country\", height=500)\n\n                                                \n\n\n\npx.treemap(df, color='lifeExp', values=\"pop\", path=[\"continent\", \"country\"], hover_name=\"country\", height=500)\n\n                                                \n\n\n\npx.choropleth(df, color='lifeExp', locations=\"iso_alpha\", hover_name=\"country\", height=500)\n\n                                                \n\n\n\npx.scatter(df, x=\"gdpPercap\", y='lifeExp', hover_name=\"country\", height=500)\n\n                                                \n\n\n\npx.scatter(df, x=\"gdpPercap\", y='lifeExp', hover_name=\"country\", color=\"continent\",size=\"pop\", height=500)\n\n                                                \n\n\nWe can see that the curve follows a logarithmic path, so make log_x=True to straighten out the line to view the relationships in an easier manner. In the graph below we can view the monotic and nonmonotonic relationships in the dataset.\n\npx.scatter(df, x=\"gdpPercap\", y='lifeExp', hover_name=\"country\", color=\"continent\",size=\"pop\", size_max=60, log_x=True, height=500)\n\n                                                \n\n\n\nfig = px.scatter(df, x=\"gdpPercap\", y='lifeExp', hover_name=\"country\", color=\"continent\",size=\"pop\", size_max=60, log_x=True, height=500)\n\nThis will allow you to inspect the values for each of these cells, unfortunately this is a great deal easier to see in JupyterLab.\n\nfig.show(\"json\")\n\nUnable to display output for mime type(s): application/json\n\n\n\nimport plotly.express as px\n\ndf = px.data.gapminder().query(\"year == 2007\")\n\nfig = px.scatter(df, x=\"gdpPercap\",y=\"lifeExp\", color=\"continent\", log_x=True, size=\"pop\", size_max=60,\n                 hover_name=\"country\", height=600, width=1000, template=\"simple_white\", \n                 color_discrete_sequence=px.colors.qualitative.G10,\n                 title=\"Health vs Wealth 2007\",\n                 labels=dict(\n                     continent=\"Continent\", pop=\"Population\",\n                     gdpPercap=\"GDP per Capita (US$, price-adjusted)\", \n                     lifeExp=\"Life Expectancy (years)\"))\n\nfig.update_layout(font_family=\"Rockwell\",\n                  legend=dict(orientation=\"h\", title=\"\", y=1.1, x=1, xanchor=\"right\", yanchor=\"bottom\"))\nfig.update_xaxes(tickprefix=\"$\", range=[2,5], dtick=1)\nfig.update_yaxes(range=[30,90])\nfig.add_hline((df[\"lifeExp\"]*df[\"pop\"]).sum()/df[\"pop\"].sum(), line_width=1, line_dash=\"dot\")\nfig.add_vline((df[\"gdpPercap\"]*df[\"pop\"]).sum()/df[\"pop\"].sum(), line_width=1, line_dash=\"dot\")\nfig.show()\n\n# fig.write_image(\"gapminder_2007.svg\") # static export\n# fig.write_html(\"gapminder_2007.html\") # interactive export\n# fig.write_json(\"gapminder_2007.json\") # serialized export\n\n                                                \n\n\n\n\n\ndf_animation = px.data.gapminder()\n\nanim_fig = px.scatter(df_animation, x=\"gdpPercap\", y=\"lifeExp\",\n                      title=\"Health vs Wealth from 1952 to 2007\",\n                      labels=dict(continent=\"Continent\", pop=\"Population\", gdpPercap=\"GDP per Capita (US$, price-adjusted)\", lifeExp=\"Life Expectancy (years)\"),\n                      animation_frame=\"year\", animation_group=\"country\",\n                      size=\"pop\",\n                      color=\"continent\",\n                      hover_name=\"country\",\n                      height=600,width=1000,\n                      template=\"simple_white\",\n                      color_discrete_sequence=px.colors.qualitative.G10,\n                      log_x=True,\n                      size_max=60,\n                      range_x=[100,100000],\n                      range_y=[25,90])\n\nanim_fig.update_layout(font_family=\"Rockwell\",\n                  legend=dict(orientation=\"h\", title=\"\", y=1.1, x=1, xanchor=\"right\", yanchor=\"bottom\"))\nanim_fig.update_xaxes(tickprefix=\"$\", range=[2,5], dtick=1)\n\n                                                \n\n\n\nanim_fig.write_html(\"gapminder_animation.html\", auto_play=False) # You're able to export this animation.\n\n\npx.defaults.height=600\n\n\nimport plotly.express as px\n\nz = [[.1, .3, .5, .7, .9],\n     [1, .8, .6, .4, .2],\n     [.2, 0, .5, .7, .9],\n     [.9, .8, .4, .2, 0],\n     [.3, .4, .5, .7, 1]]\n\nfig = px.imshow(z, text_auto=True)\nfig.show()\n\n                                                \n\n\n\nimport plotly.express as px\ndf = px.data.wind()\nfig = px.bar_polar(df, r=\"frequency\", theta=\"direction\", height=600,\n                   color=\"strength\", template=\"plotly_dark\",\n                   color_discrete_sequence= px.colors.sequential.Plasma_r)\nfig.show()\n\n                                                \n\n\n\ndf = px.data.election()\nfig = px.scatter_ternary(df, a=\"Joly\", b=\"Coderre\", c=\"Bergeron\", color=\"winner\", size=\"total\", hover_name=\"district\",\n                   size_max=15, color_discrete_map = {\"Joly\": \"blue\", \"Bergeron\": \"green\", \"Coderre\":\"red\"} )\nfig.show()\n\n                                                \n\n\n\ndf = px.data.election()\nfig = px.scatter_3d(df, x=\"Joly\", y=\"Coderre\", z=\"Bergeron\", color=\"winner\", size=\"total\", hover_name=\"district\",\n                  symbol=\"result\", color_discrete_map = {\"Joly\": \"blue\", \"Bergeron\": \"green\", \"Coderre\":\"red\"})\nfig.show()",
    "crumbs": [
      "Home",
      "Introduction to Plotly Express"
    ]
  },
  {
    "objectID": "notebooks/intro_to_plotly_express.html#animations-in-plotly-express",
    "href": "notebooks/intro_to_plotly_express.html#animations-in-plotly-express",
    "title": "Introduction to Plotly Express",
    "section": "",
    "text": "df_animation = px.data.gapminder()\n\nanim_fig = px.scatter(df_animation, x=\"gdpPercap\", y=\"lifeExp\",\n                      title=\"Health vs Wealth from 1952 to 2007\",\n                      labels=dict(continent=\"Continent\", pop=\"Population\", gdpPercap=\"GDP per Capita (US$, price-adjusted)\", lifeExp=\"Life Expectancy (years)\"),\n                      animation_frame=\"year\", animation_group=\"country\",\n                      size=\"pop\",\n                      color=\"continent\",\n                      hover_name=\"country\",\n                      height=600,width=1000,\n                      template=\"simple_white\",\n                      color_discrete_sequence=px.colors.qualitative.G10,\n                      log_x=True,\n                      size_max=60,\n                      range_x=[100,100000],\n                      range_y=[25,90])\n\nanim_fig.update_layout(font_family=\"Rockwell\",\n                  legend=dict(orientation=\"h\", title=\"\", y=1.1, x=1, xanchor=\"right\", yanchor=\"bottom\"))\nanim_fig.update_xaxes(tickprefix=\"$\", range=[2,5], dtick=1)\n\n                                                \n\n\n\nanim_fig.write_html(\"gapminder_animation.html\", auto_play=False) # You're able to export this animation.\n\n\npx.defaults.height=600\n\n\nimport plotly.express as px\n\nz = [[.1, .3, .5, .7, .9],\n     [1, .8, .6, .4, .2],\n     [.2, 0, .5, .7, .9],\n     [.9, .8, .4, .2, 0],\n     [.3, .4, .5, .7, 1]]\n\nfig = px.imshow(z, text_auto=True)\nfig.show()\n\n                                                \n\n\n\nimport plotly.express as px\ndf = px.data.wind()\nfig = px.bar_polar(df, r=\"frequency\", theta=\"direction\", height=600,\n                   color=\"strength\", template=\"plotly_dark\",\n                   color_discrete_sequence= px.colors.sequential.Plasma_r)\nfig.show()\n\n                                                \n\n\n\ndf = px.data.election()\nfig = px.scatter_ternary(df, a=\"Joly\", b=\"Coderre\", c=\"Bergeron\", color=\"winner\", size=\"total\", hover_name=\"district\",\n                   size_max=15, color_discrete_map = {\"Joly\": \"blue\", \"Bergeron\": \"green\", \"Coderre\":\"red\"} )\nfig.show()\n\n                                                \n\n\n\ndf = px.data.election()\nfig = px.scatter_3d(df, x=\"Joly\", y=\"Coderre\", z=\"Bergeron\", color=\"winner\", size=\"total\", hover_name=\"district\",\n                  symbol=\"result\", color_discrete_map = {\"Joly\": \"blue\", \"Bergeron\": \"green\", \"Coderre\":\"red\"})\nfig.show()",
    "crumbs": [
      "Home",
      "Introduction to Plotly Express"
    ]
  },
  {
    "objectID": "notebooks/one_hot_encoding.html",
    "href": "notebooks/one_hot_encoding.html",
    "title": "One-Hot Encoding",
    "section": "",
    "text": "It’s useful for feeding categorical data into machine-learning algorithms since integers are computationally less expensive than strings.\n\nimport pandas as pd\nprint(pd.__version__)\n\n2.0.3\n\n\n\ndisengagements = pd.read_excel(\"../data/cassi-autonomous-shuttle/autonomous_shuttle_disengagement.xlsx\",usecols=[\"Incident Datetime\", \"Location\",\"Weather\",\"Vehicle Speed in Miles per Hour\", \"Initiated by\",\"Cause\"], parse_dates=True)\ndisengagements\n\n\n\n\n\n\n\n\nIncident Datetime\nLocation\nWeather\nVehicle Speed in Miles per Hour\nInitiated by\nCause\n\n\n\n\n0\n2023-03-07T10:00:00-05:00\n35.7849964, -78.8268094\nSunny;\n2\nOperator\nFault Code/Error Code\n\n\n1\n2023-03-07T14:00:00-05:00\n35.7847312, -78.8245051\nSunny;\n5\nOperator\nStation Blocked\n\n\n2\n2023-03-07T14:30:00-05:00\n35.7824658, -78.8244159\nSunny;\n5\nOperator\nStation Blocked\n\n\n3\n2023-03-07T15:15:00-05:00\n35.7824658, -78.8244159\nSunny;\n4\nOperator\nStation Blocked\n\n\n4\n2023-03-08T10:00:00-05:00\n35.7852558, -78.8273737\nSunny;\n2\nOperator\nShuttle Manually Deviated from Approved Path\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n174\n2023-06-01T16:00:00-04:00\n35.783456, -78.821639\nSunny;\n5\nOperator\nSignal Loss\n\n\n175\n2023-06-02T10:32:00-04:00\n35.7819145, -78.8235603\nSunny;\n4\nOperator\nStation Blocked\n\n\n176\n2023-06-02T10:35:00-04:00\n35.7813188, -78.8256601\nSunny;\n3\nOperator\nStation Blocked\n\n\n177\n2023-06-02T10:44:00-04:00\n35.7847325, -78.824496\nSunny;\n4\nOperator\nObstacle Detection\n\n\n178\n2023-06-02T11:01:00-04:00\n35.7841086, -78.8261962\nSunny;\n3\nOperator\nSignalized Intersection\n\n\n\n\n179 rows × 6 columns\n\n\n\n\ndisengagements.dtypes\n\nIncident Datetime                  object\nLocation                           object\nWeather                            object\nVehicle Speed in Miles per Hour     int64\nInitiated by                       object\nCause                              object\ndtype: object\n\n\n\ndisengagements['Incident Datetime'] = pd.to_datetime(disengagements['Incident Datetime'], utc=True)\ndisengagements['Initiated by'] = disengagements['Initiated by'].astype('category')\ndisengagements['Cause'] = disengagements['Cause'].astype('category')\ndisengagements.dtypes\n\nIncident Datetime                  datetime64[ns, UTC]\nLocation                                        object\nWeather                                         object\nVehicle Speed in Miles per Hour                  int64\nInitiated by                                  category\nCause                                         category\ndtype: object\n\n\n\ndisengagements = disengagements.assign(week_of_year = disengagements['Incident Datetime'].dt.isocalendar().week, week_of_pilot = lambda x: disengagements['Incident Datetime'].dt.isocalendar().week - 9)\ndisengagements\n\n\n\n\n\n\n\n\nIncident Datetime\nLocation\nWeather\nVehicle Speed in Miles per Hour\nInitiated by\nCause\nweek_of_year\nweek_of_pilot\n\n\n\n\n0\n2023-03-07 15:00:00+00:00\n35.7849964, -78.8268094\nSunny;\n2\nOperator\nFault Code/Error Code\n10\n1\n\n\n1\n2023-03-07 19:00:00+00:00\n35.7847312, -78.8245051\nSunny;\n5\nOperator\nStation Blocked\n10\n1\n\n\n2\n2023-03-07 19:30:00+00:00\n35.7824658, -78.8244159\nSunny;\n5\nOperator\nStation Blocked\n10\n1\n\n\n3\n2023-03-07 20:15:00+00:00\n35.7824658, -78.8244159\nSunny;\n4\nOperator\nStation Blocked\n10\n1\n\n\n4\n2023-03-08 15:00:00+00:00\n35.7852558, -78.8273737\nSunny;\n2\nOperator\nShuttle Manually Deviated from Approved Path\n10\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\n2023-06-01 20:00:00+00:00\n35.783456, -78.821639\nSunny;\n5\nOperator\nSignal Loss\n22\n13\n\n\n175\n2023-06-02 14:32:00+00:00\n35.7819145, -78.8235603\nSunny;\n4\nOperator\nStation Blocked\n22\n13\n\n\n176\n2023-06-02 14:35:00+00:00\n35.7813188, -78.8256601\nSunny;\n3\nOperator\nStation Blocked\n22\n13\n\n\n177\n2023-06-02 14:44:00+00:00\n35.7847325, -78.824496\nSunny;\n4\nOperator\nObstacle Detection\n22\n13\n\n\n178\n2023-06-02 15:01:00+00:00\n35.7841086, -78.8261962\nSunny;\n3\nOperator\nSignalized Intersection\n22\n13\n\n\n\n\n179 rows × 8 columns\n\n\n\n\ndisengagements['Cause']\n\n0                             Fault Code/Error Code\n1                                   Station Blocked\n2                                   Station Blocked\n3                                   Station Blocked\n4      Shuttle Manually Deviated from Approved Path\n                           ...                     \n174                                     Signal Loss\n175                                 Station Blocked\n176                                 Station Blocked\n177                              Obstacle Detection\n178                         Signalized Intersection\nName: Cause, Length: 179, dtype: category\nCategories (9, object): ['Fault Code/Error Code', 'Obstacle Detection', 'Other Road Users', 'Priority Zone', ..., 'Signal Loss', 'Signalized Intersection', 'Station Blocked', 'Vegetation']\n\n\n\ndisengagements['Cause'].cat.categories\n\nIndex(['Fault Code/Error Code', 'Obstacle Detection', 'Other Road Users',\n       'Priority Zone', 'Shuttle Manually Deviated from Approved Path',\n       'Signal Loss', 'Signalized Intersection', 'Station Blocked',\n       'Vegetation'],\n      dtype='object')\n\n\n\ndisengagements_datetime_is_index = disengagements.set_index('Incident Datetime')\ndisengagements_datetime_is_index\n\n\n\n\n\n\n\n\nLocation\nWeather\nVehicle Speed in Miles per Hour\nInitiated by\nCause\nweek_of_year\nweek_of_pilot\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n\n\n2023-03-07 15:00:00+00:00\n35.7849964, -78.8268094\nSunny;\n2\nOperator\nFault Code/Error Code\n10\n1\n\n\n2023-03-07 19:00:00+00:00\n35.7847312, -78.8245051\nSunny;\n5\nOperator\nStation Blocked\n10\n1\n\n\n2023-03-07 19:30:00+00:00\n35.7824658, -78.8244159\nSunny;\n5\nOperator\nStation Blocked\n10\n1\n\n\n2023-03-07 20:15:00+00:00\n35.7824658, -78.8244159\nSunny;\n4\nOperator\nStation Blocked\n10\n1\n\n\n2023-03-08 15:00:00+00:00\n35.7852558, -78.8273737\nSunny;\n2\nOperator\nShuttle Manually Deviated from Approved Path\n10\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 20:00:00+00:00\n35.783456, -78.821639\nSunny;\n5\nOperator\nSignal Loss\n22\n13\n\n\n2023-06-02 14:32:00+00:00\n35.7819145, -78.8235603\nSunny;\n4\nOperator\nStation Blocked\n22\n13\n\n\n2023-06-02 14:35:00+00:00\n35.7813188, -78.8256601\nSunny;\n3\nOperator\nStation Blocked\n22\n13\n\n\n2023-06-02 14:44:00+00:00\n35.7847325, -78.824496\nSunny;\n4\nOperator\nObstacle Detection\n22\n13\n\n\n2023-06-02 15:01:00+00:00\n35.7841086, -78.8261962\nSunny;\n3\nOperator\nSignalized Intersection\n22\n13\n\n\n\n\n179 rows × 7 columns\n\n\n\n\ndisengagements_datetime_is_index.index=disengagements_datetime_is_index.index.tz_convert(tz='US/Eastern')\ndisengagements_datetime_is_index\n\n\n\n\n\n\n\n\nLocation\nWeather\nVehicle Speed in Miles per Hour\nInitiated by\nCause\nweek_of_year\nweek_of_pilot\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n\n\n2023-03-07 10:00:00-05:00\n35.7849964, -78.8268094\nSunny;\n2\nOperator\nFault Code/Error Code\n10\n1\n\n\n2023-03-07 14:00:00-05:00\n35.7847312, -78.8245051\nSunny;\n5\nOperator\nStation Blocked\n10\n1\n\n\n2023-03-07 14:30:00-05:00\n35.7824658, -78.8244159\nSunny;\n5\nOperator\nStation Blocked\n10\n1\n\n\n2023-03-07 15:15:00-05:00\n35.7824658, -78.8244159\nSunny;\n4\nOperator\nStation Blocked\n10\n1\n\n\n2023-03-08 10:00:00-05:00\n35.7852558, -78.8273737\nSunny;\n2\nOperator\nShuttle Manually Deviated from Approved Path\n10\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 16:00:00-04:00\n35.783456, -78.821639\nSunny;\n5\nOperator\nSignal Loss\n22\n13\n\n\n2023-06-02 10:32:00-04:00\n35.7819145, -78.8235603\nSunny;\n4\nOperator\nStation Blocked\n22\n13\n\n\n2023-06-02 10:35:00-04:00\n35.7813188, -78.8256601\nSunny;\n3\nOperator\nStation Blocked\n22\n13\n\n\n2023-06-02 10:44:00-04:00\n35.7847325, -78.824496\nSunny;\n4\nOperator\nObstacle Detection\n22\n13\n\n\n2023-06-02 11:01:00-04:00\n35.7841086, -78.8261962\nSunny;\n3\nOperator\nSignalized Intersection\n22\n13\n\n\n\n\n179 rows × 7 columns\n\n\n\n\ndisengagements_datetime_is_index.dtypes\n\nLocation                             object\nWeather                              object\nVehicle Speed in Miles per Hour       int64\nInitiated by                       category\nCause                              category\nweek_of_year                         UInt32\nweek_of_pilot                        UInt32\ndtype: object\n\n\n\none_hot = disengagements_datetime_is_index.Weather.str.get_dummies(sep=';')\none_hot\n\n\n\n\n\n\n\n\nCloudy\nPartly Cloudy\nRain\nSunny\nWindy\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n2023-03-07 10:00:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-07 14:00:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-07 14:30:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-07 15:15:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-08 10:00:00-05:00\n0\n0\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 16:00:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:32:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:35:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:44:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 11:01:00-04:00\n0\n0\n0\n1\n0\n\n\n\n\n179 rows × 5 columns\n\n\n\n\none_hot.columns = 'Weather_' + one_hot.columns\none_hot\n\n\n\n\n\n\n\n\nWeather_Cloudy\nWeather_Partly Cloudy\nWeather_Rain\nWeather_Sunny\nWeather_Windy\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n2023-03-07 10:00:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-07 14:00:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-07 14:30:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-07 15:15:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-08 10:00:00-05:00\n0\n0\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 16:00:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:32:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:35:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:44:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 11:01:00-04:00\n0\n0\n0\n1\n0\n\n\n\n\n179 rows × 5 columns\n\n\n\n\none_hot_cause = disengagements_datetime_is_index.Cause.str.get_dummies()\none_hot_cause.columns = 'Cause_' + one_hot_cause.columns\none_hot_cause\n\n\n\n\n\n\n\n\nCause_Fault Code/Error Code\nCause_Obstacle Detection\nCause_Other Road Users\nCause_Priority Zone\nCause_Shuttle Manually Deviated from Approved Path\nCause_Signal Loss\nCause_Signalized Intersection\nCause_Station Blocked\nCause_Vegetation\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-03-07 10:00:00-05:00\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2023-03-07 14:00:00-05:00\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-07 14:30:00-05:00\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-07 15:15:00-05:00\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-08 10:00:00-05:00\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 16:00:00-04:00\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2023-06-02 10:32:00-04:00\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:35:00-04:00\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:44:00-04:00\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n2023-06-02 11:01:00-04:00\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n179 rows × 9 columns\n\n\n\n\ndisengagements_datetime_is_index = disengagements_datetime_is_index.drop(['Weather', 'Initiated by', 'Cause'], axis=1)\ncassi_data_one_hot_encoded = pd.concat([disengagements_datetime_is_index, one_hot, one_hot_cause], axis=1)\ncassi_data_one_hot_encoded\n\n\n\n\n\n\n\n\nLocation\nVehicle Speed in Miles per Hour\nweek_of_year\nweek_of_pilot\nWeather_Cloudy\nWeather_Partly Cloudy\nWeather_Rain\nWeather_Sunny\nWeather_Windy\nCause_Fault Code/Error Code\nCause_Obstacle Detection\nCause_Other Road Users\nCause_Priority Zone\nCause_Shuttle Manually Deviated from Approved Path\nCause_Signal Loss\nCause_Signalized Intersection\nCause_Station Blocked\nCause_Vegetation\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-03-07 10:00:00-05:00\n35.7849964, -78.8268094\n2\n10\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2023-03-07 14:00:00-05:00\n35.7847312, -78.8245051\n5\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-07 14:30:00-05:00\n35.7824658, -78.8244159\n5\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-07 15:15:00-05:00\n35.7824658, -78.8244159\n4\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-08 10:00:00-05:00\n35.7852558, -78.8273737\n2\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 16:00:00-04:00\n35.783456, -78.821639\n5\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2023-06-02 10:32:00-04:00\n35.7819145, -78.8235603\n4\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:35:00-04:00\n35.7813188, -78.8256601\n3\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:44:00-04:00\n35.7847325, -78.824496\n4\n22\n13\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n2023-06-02 11:01:00-04:00\n35.7841086, -78.8261962\n3\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n179 rows × 18 columns\n\n\n\n\ncassi_data_one_hot_encoded.index = cassi_data_one_hot_encoded.index.tz_convert(tz='UTC')\n\n\ncassi_data_one_hot_encoded\n\n\n\n\n\n\n\n\nLocation\nVehicle Speed in Miles per Hour\nweek_of_year\nweek_of_pilot\nWeather_Cloudy\nWeather_Partly Cloudy\nWeather_Rain\nWeather_Sunny\nWeather_Windy\nCause_Fault Code/Error Code\nCause_Obstacle Detection\nCause_Other Road Users\nCause_Priority Zone\nCause_Shuttle Manually Deviated from Approved Path\nCause_Signal Loss\nCause_Signalized Intersection\nCause_Station Blocked\nCause_Vegetation\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-03-07 15:00:00+00:00\n35.7849964, -78.8268094\n2\n10\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2023-03-07 19:00:00+00:00\n35.7847312, -78.8245051\n5\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-07 19:30:00+00:00\n35.7824658, -78.8244159\n5\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-07 20:15:00+00:00\n35.7824658, -78.8244159\n4\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-08 15:00:00+00:00\n35.7852558, -78.8273737\n2\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 20:00:00+00:00\n35.783456, -78.821639\n5\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2023-06-02 14:32:00+00:00\n35.7819145, -78.8235603\n4\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-06-02 14:35:00+00:00\n35.7813188, -78.8256601\n3\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-06-02 14:44:00+00:00\n35.7847325, -78.824496\n4\n22\n13\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n2023-06-02 15:01:00+00:00\n35.7841086, -78.8261962\n3\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n179 rows × 18 columns"
  },
  {
    "objectID": "notebooks/one_hot_encoding.html#why-bother-with-one-hot-encoding",
    "href": "notebooks/one_hot_encoding.html#why-bother-with-one-hot-encoding",
    "title": "One-Hot Encoding",
    "section": "",
    "text": "It’s useful for feeding categorical data into machine-learning algorithms since integers are computationally less expensive than strings.\n\nimport pandas as pd\nprint(pd.__version__)\n\n2.0.3\n\n\n\ndisengagements = pd.read_excel(\"../data/cassi-autonomous-shuttle/autonomous_shuttle_disengagement.xlsx\",usecols=[\"Incident Datetime\", \"Location\",\"Weather\",\"Vehicle Speed in Miles per Hour\", \"Initiated by\",\"Cause\"], parse_dates=True)\ndisengagements\n\n\n\n\n\n\n\n\nIncident Datetime\nLocation\nWeather\nVehicle Speed in Miles per Hour\nInitiated by\nCause\n\n\n\n\n0\n2023-03-07T10:00:00-05:00\n35.7849964, -78.8268094\nSunny;\n2\nOperator\nFault Code/Error Code\n\n\n1\n2023-03-07T14:00:00-05:00\n35.7847312, -78.8245051\nSunny;\n5\nOperator\nStation Blocked\n\n\n2\n2023-03-07T14:30:00-05:00\n35.7824658, -78.8244159\nSunny;\n5\nOperator\nStation Blocked\n\n\n3\n2023-03-07T15:15:00-05:00\n35.7824658, -78.8244159\nSunny;\n4\nOperator\nStation Blocked\n\n\n4\n2023-03-08T10:00:00-05:00\n35.7852558, -78.8273737\nSunny;\n2\nOperator\nShuttle Manually Deviated from Approved Path\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n174\n2023-06-01T16:00:00-04:00\n35.783456, -78.821639\nSunny;\n5\nOperator\nSignal Loss\n\n\n175\n2023-06-02T10:32:00-04:00\n35.7819145, -78.8235603\nSunny;\n4\nOperator\nStation Blocked\n\n\n176\n2023-06-02T10:35:00-04:00\n35.7813188, -78.8256601\nSunny;\n3\nOperator\nStation Blocked\n\n\n177\n2023-06-02T10:44:00-04:00\n35.7847325, -78.824496\nSunny;\n4\nOperator\nObstacle Detection\n\n\n178\n2023-06-02T11:01:00-04:00\n35.7841086, -78.8261962\nSunny;\n3\nOperator\nSignalized Intersection\n\n\n\n\n179 rows × 6 columns\n\n\n\n\ndisengagements.dtypes\n\nIncident Datetime                  object\nLocation                           object\nWeather                            object\nVehicle Speed in Miles per Hour     int64\nInitiated by                       object\nCause                              object\ndtype: object\n\n\n\ndisengagements['Incident Datetime'] = pd.to_datetime(disengagements['Incident Datetime'], utc=True)\ndisengagements['Initiated by'] = disengagements['Initiated by'].astype('category')\ndisengagements['Cause'] = disengagements['Cause'].astype('category')\ndisengagements.dtypes\n\nIncident Datetime                  datetime64[ns, UTC]\nLocation                                        object\nWeather                                         object\nVehicle Speed in Miles per Hour                  int64\nInitiated by                                  category\nCause                                         category\ndtype: object\n\n\n\ndisengagements = disengagements.assign(week_of_year = disengagements['Incident Datetime'].dt.isocalendar().week, week_of_pilot = lambda x: disengagements['Incident Datetime'].dt.isocalendar().week - 9)\ndisengagements\n\n\n\n\n\n\n\n\nIncident Datetime\nLocation\nWeather\nVehicle Speed in Miles per Hour\nInitiated by\nCause\nweek_of_year\nweek_of_pilot\n\n\n\n\n0\n2023-03-07 15:00:00+00:00\n35.7849964, -78.8268094\nSunny;\n2\nOperator\nFault Code/Error Code\n10\n1\n\n\n1\n2023-03-07 19:00:00+00:00\n35.7847312, -78.8245051\nSunny;\n5\nOperator\nStation Blocked\n10\n1\n\n\n2\n2023-03-07 19:30:00+00:00\n35.7824658, -78.8244159\nSunny;\n5\nOperator\nStation Blocked\n10\n1\n\n\n3\n2023-03-07 20:15:00+00:00\n35.7824658, -78.8244159\nSunny;\n4\nOperator\nStation Blocked\n10\n1\n\n\n4\n2023-03-08 15:00:00+00:00\n35.7852558, -78.8273737\nSunny;\n2\nOperator\nShuttle Manually Deviated from Approved Path\n10\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\n2023-06-01 20:00:00+00:00\n35.783456, -78.821639\nSunny;\n5\nOperator\nSignal Loss\n22\n13\n\n\n175\n2023-06-02 14:32:00+00:00\n35.7819145, -78.8235603\nSunny;\n4\nOperator\nStation Blocked\n22\n13\n\n\n176\n2023-06-02 14:35:00+00:00\n35.7813188, -78.8256601\nSunny;\n3\nOperator\nStation Blocked\n22\n13\n\n\n177\n2023-06-02 14:44:00+00:00\n35.7847325, -78.824496\nSunny;\n4\nOperator\nObstacle Detection\n22\n13\n\n\n178\n2023-06-02 15:01:00+00:00\n35.7841086, -78.8261962\nSunny;\n3\nOperator\nSignalized Intersection\n22\n13\n\n\n\n\n179 rows × 8 columns\n\n\n\n\ndisengagements['Cause']\n\n0                             Fault Code/Error Code\n1                                   Station Blocked\n2                                   Station Blocked\n3                                   Station Blocked\n4      Shuttle Manually Deviated from Approved Path\n                           ...                     \n174                                     Signal Loss\n175                                 Station Blocked\n176                                 Station Blocked\n177                              Obstacle Detection\n178                         Signalized Intersection\nName: Cause, Length: 179, dtype: category\nCategories (9, object): ['Fault Code/Error Code', 'Obstacle Detection', 'Other Road Users', 'Priority Zone', ..., 'Signal Loss', 'Signalized Intersection', 'Station Blocked', 'Vegetation']\n\n\n\ndisengagements['Cause'].cat.categories\n\nIndex(['Fault Code/Error Code', 'Obstacle Detection', 'Other Road Users',\n       'Priority Zone', 'Shuttle Manually Deviated from Approved Path',\n       'Signal Loss', 'Signalized Intersection', 'Station Blocked',\n       'Vegetation'],\n      dtype='object')\n\n\n\ndisengagements_datetime_is_index = disengagements.set_index('Incident Datetime')\ndisengagements_datetime_is_index\n\n\n\n\n\n\n\n\nLocation\nWeather\nVehicle Speed in Miles per Hour\nInitiated by\nCause\nweek_of_year\nweek_of_pilot\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n\n\n2023-03-07 15:00:00+00:00\n35.7849964, -78.8268094\nSunny;\n2\nOperator\nFault Code/Error Code\n10\n1\n\n\n2023-03-07 19:00:00+00:00\n35.7847312, -78.8245051\nSunny;\n5\nOperator\nStation Blocked\n10\n1\n\n\n2023-03-07 19:30:00+00:00\n35.7824658, -78.8244159\nSunny;\n5\nOperator\nStation Blocked\n10\n1\n\n\n2023-03-07 20:15:00+00:00\n35.7824658, -78.8244159\nSunny;\n4\nOperator\nStation Blocked\n10\n1\n\n\n2023-03-08 15:00:00+00:00\n35.7852558, -78.8273737\nSunny;\n2\nOperator\nShuttle Manually Deviated from Approved Path\n10\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 20:00:00+00:00\n35.783456, -78.821639\nSunny;\n5\nOperator\nSignal Loss\n22\n13\n\n\n2023-06-02 14:32:00+00:00\n35.7819145, -78.8235603\nSunny;\n4\nOperator\nStation Blocked\n22\n13\n\n\n2023-06-02 14:35:00+00:00\n35.7813188, -78.8256601\nSunny;\n3\nOperator\nStation Blocked\n22\n13\n\n\n2023-06-02 14:44:00+00:00\n35.7847325, -78.824496\nSunny;\n4\nOperator\nObstacle Detection\n22\n13\n\n\n2023-06-02 15:01:00+00:00\n35.7841086, -78.8261962\nSunny;\n3\nOperator\nSignalized Intersection\n22\n13\n\n\n\n\n179 rows × 7 columns\n\n\n\n\ndisengagements_datetime_is_index.index=disengagements_datetime_is_index.index.tz_convert(tz='US/Eastern')\ndisengagements_datetime_is_index\n\n\n\n\n\n\n\n\nLocation\nWeather\nVehicle Speed in Miles per Hour\nInitiated by\nCause\nweek_of_year\nweek_of_pilot\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n\n\n2023-03-07 10:00:00-05:00\n35.7849964, -78.8268094\nSunny;\n2\nOperator\nFault Code/Error Code\n10\n1\n\n\n2023-03-07 14:00:00-05:00\n35.7847312, -78.8245051\nSunny;\n5\nOperator\nStation Blocked\n10\n1\n\n\n2023-03-07 14:30:00-05:00\n35.7824658, -78.8244159\nSunny;\n5\nOperator\nStation Blocked\n10\n1\n\n\n2023-03-07 15:15:00-05:00\n35.7824658, -78.8244159\nSunny;\n4\nOperator\nStation Blocked\n10\n1\n\n\n2023-03-08 10:00:00-05:00\n35.7852558, -78.8273737\nSunny;\n2\nOperator\nShuttle Manually Deviated from Approved Path\n10\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 16:00:00-04:00\n35.783456, -78.821639\nSunny;\n5\nOperator\nSignal Loss\n22\n13\n\n\n2023-06-02 10:32:00-04:00\n35.7819145, -78.8235603\nSunny;\n4\nOperator\nStation Blocked\n22\n13\n\n\n2023-06-02 10:35:00-04:00\n35.7813188, -78.8256601\nSunny;\n3\nOperator\nStation Blocked\n22\n13\n\n\n2023-06-02 10:44:00-04:00\n35.7847325, -78.824496\nSunny;\n4\nOperator\nObstacle Detection\n22\n13\n\n\n2023-06-02 11:01:00-04:00\n35.7841086, -78.8261962\nSunny;\n3\nOperator\nSignalized Intersection\n22\n13\n\n\n\n\n179 rows × 7 columns\n\n\n\n\ndisengagements_datetime_is_index.dtypes\n\nLocation                             object\nWeather                              object\nVehicle Speed in Miles per Hour       int64\nInitiated by                       category\nCause                              category\nweek_of_year                         UInt32\nweek_of_pilot                        UInt32\ndtype: object\n\n\n\none_hot = disengagements_datetime_is_index.Weather.str.get_dummies(sep=';')\none_hot\n\n\n\n\n\n\n\n\nCloudy\nPartly Cloudy\nRain\nSunny\nWindy\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n2023-03-07 10:00:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-07 14:00:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-07 14:30:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-07 15:15:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-08 10:00:00-05:00\n0\n0\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 16:00:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:32:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:35:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:44:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 11:01:00-04:00\n0\n0\n0\n1\n0\n\n\n\n\n179 rows × 5 columns\n\n\n\n\none_hot.columns = 'Weather_' + one_hot.columns\none_hot\n\n\n\n\n\n\n\n\nWeather_Cloudy\nWeather_Partly Cloudy\nWeather_Rain\nWeather_Sunny\nWeather_Windy\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n2023-03-07 10:00:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-07 14:00:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-07 14:30:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-07 15:15:00-05:00\n0\n0\n0\n1\n0\n\n\n2023-03-08 10:00:00-05:00\n0\n0\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 16:00:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:32:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:35:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:44:00-04:00\n0\n0\n0\n1\n0\n\n\n2023-06-02 11:01:00-04:00\n0\n0\n0\n1\n0\n\n\n\n\n179 rows × 5 columns\n\n\n\n\none_hot_cause = disengagements_datetime_is_index.Cause.str.get_dummies()\none_hot_cause.columns = 'Cause_' + one_hot_cause.columns\none_hot_cause\n\n\n\n\n\n\n\n\nCause_Fault Code/Error Code\nCause_Obstacle Detection\nCause_Other Road Users\nCause_Priority Zone\nCause_Shuttle Manually Deviated from Approved Path\nCause_Signal Loss\nCause_Signalized Intersection\nCause_Station Blocked\nCause_Vegetation\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-03-07 10:00:00-05:00\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2023-03-07 14:00:00-05:00\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-07 14:30:00-05:00\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-07 15:15:00-05:00\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-08 10:00:00-05:00\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 16:00:00-04:00\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2023-06-02 10:32:00-04:00\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:35:00-04:00\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:44:00-04:00\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n2023-06-02 11:01:00-04:00\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n179 rows × 9 columns\n\n\n\n\ndisengagements_datetime_is_index = disengagements_datetime_is_index.drop(['Weather', 'Initiated by', 'Cause'], axis=1)\ncassi_data_one_hot_encoded = pd.concat([disengagements_datetime_is_index, one_hot, one_hot_cause], axis=1)\ncassi_data_one_hot_encoded\n\n\n\n\n\n\n\n\nLocation\nVehicle Speed in Miles per Hour\nweek_of_year\nweek_of_pilot\nWeather_Cloudy\nWeather_Partly Cloudy\nWeather_Rain\nWeather_Sunny\nWeather_Windy\nCause_Fault Code/Error Code\nCause_Obstacle Detection\nCause_Other Road Users\nCause_Priority Zone\nCause_Shuttle Manually Deviated from Approved Path\nCause_Signal Loss\nCause_Signalized Intersection\nCause_Station Blocked\nCause_Vegetation\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-03-07 10:00:00-05:00\n35.7849964, -78.8268094\n2\n10\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2023-03-07 14:00:00-05:00\n35.7847312, -78.8245051\n5\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-07 14:30:00-05:00\n35.7824658, -78.8244159\n5\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-07 15:15:00-05:00\n35.7824658, -78.8244159\n4\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-08 10:00:00-05:00\n35.7852558, -78.8273737\n2\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 16:00:00-04:00\n35.783456, -78.821639\n5\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2023-06-02 10:32:00-04:00\n35.7819145, -78.8235603\n4\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:35:00-04:00\n35.7813188, -78.8256601\n3\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-06-02 10:44:00-04:00\n35.7847325, -78.824496\n4\n22\n13\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n2023-06-02 11:01:00-04:00\n35.7841086, -78.8261962\n3\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n179 rows × 18 columns\n\n\n\n\ncassi_data_one_hot_encoded.index = cassi_data_one_hot_encoded.index.tz_convert(tz='UTC')\n\n\ncassi_data_one_hot_encoded\n\n\n\n\n\n\n\n\nLocation\nVehicle Speed in Miles per Hour\nweek_of_year\nweek_of_pilot\nWeather_Cloudy\nWeather_Partly Cloudy\nWeather_Rain\nWeather_Sunny\nWeather_Windy\nCause_Fault Code/Error Code\nCause_Obstacle Detection\nCause_Other Road Users\nCause_Priority Zone\nCause_Shuttle Manually Deviated from Approved Path\nCause_Signal Loss\nCause_Signalized Intersection\nCause_Station Blocked\nCause_Vegetation\n\n\nIncident Datetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-03-07 15:00:00+00:00\n35.7849964, -78.8268094\n2\n10\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2023-03-07 19:00:00+00:00\n35.7847312, -78.8245051\n5\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-07 19:30:00+00:00\n35.7824658, -78.8244159\n5\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-07 20:15:00+00:00\n35.7824658, -78.8244159\n4\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-03-08 15:00:00+00:00\n35.7852558, -78.8273737\n2\n10\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-01 20:00:00+00:00\n35.783456, -78.821639\n5\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2023-06-02 14:32:00+00:00\n35.7819145, -78.8235603\n4\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-06-02 14:35:00+00:00\n35.7813188, -78.8256601\n3\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2023-06-02 14:44:00+00:00\n35.7847325, -78.824496\n4\n22\n13\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n2023-06-02 15:01:00+00:00\n35.7841086, -78.8261962\n3\n22\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n179 rows × 18 columns"
  },
  {
    "objectID": "notebooks/plotting_with_pandas.html",
    "href": "notebooks/plotting_with_pandas.html",
    "title": "Plotting with Pandas",
    "section": "",
    "text": "The plot() method is available on Series and DataFrame objects. Many of the parameters get passed down to matplotlib. The kind argument let’s us vary the plot type. Here are some commonly used parameters:\n\n\n\n\n\n\n\n\nParameter\nPurpose\nData Type\n\n\n\n\nkind\nDetermines the plot type\nString\n\n\nx/y\nColumn(s) to plot on the x-axis/y-axis\nString or list\n\n\nax\nDraws the plot on the Axes object provided\nAxes\n\n\nsubplots\nDetermines whether to make subplots\nBoolean\n\n\nlayout\nSpecifies how to arrange the subplots\nTuple of (rows, columns)\n\n\nfigsize\nSize to make the Figure object\nTuple of (width, height)\n\n\ntitle\nThe title of the plot or subplots\nString for the plot title or a list of strings for subplot titles\n\n\nlegend\nDetermines whether to show the legend\nBoolean\n\n\nlabel\nWhat to call an item in the legend\nString if a single column is being plotted; otherwise, a list of strings\n\n\nstyle\nmatplotlib style strings for each item being plotted\nString if a single column is being plotted; otherwise, a list of strings\n\n\ncolor\nThe color to plot the item in\nString or red, green, blue tuple if a single column is being plotted; otherwise, a list\n\n\ncolormap\nThe colormap to use\nString or matplotlib colormap object\n\n\nlogx/logy/loglog\nDetermines whether to use a logarithmic scale for the x-axis, y-axis, or both\nBoolean\n\n\nxticks/yticks\nDetermines where to draw the ticks on the x-axis/y-axis\nList of values\n\n\nxlim/ylim\nThe axis limits for the x-axis/y-axis\nTuple of the form (min, max)\n\n\nrot\nThe angle to write the tick labels at\nInteger\n\n\nsharex/sharey\nDetermines whether to have subplots share the x-axis/y-axis\nBoolean\n\n\nfontsize\nControls the size of the tick labels\nInteger\n\n\ngrid\nTurns on/off the grid lines\nBoolean\n\n\n\n\n\nIn this notebook, we will be working with 3 datasets: - Facebook’s stock price throughout 2018 (obtained using the stock_analysis package) - Earthquake data from September 18, 2018 - October 13, 2018 (obtained from the US Geological Survey (USGS) using the USGS API) - European Centre for Disease Prevention and Control’s (ECDC) daily number of new reported cases of COVID-19 by country worldwide dataset collected on September 19, 2020 via this link\n\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfb = pd.read_csv(\n    '../data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True\n)\nquakes = pd.read_csv('../data/earthquakes.csv')\ncovid = pd.read_csv('../data/covid19_cases.csv').assign(\n    date=lambda x: pd.to_datetime(x.dateRep, format='%d/%m/%Y')\n).set_index('date').replace(\n    'United_States_of_America', 'USA'\n).sort_index()['2020-01-18':'2020-09-18']\n\n\n\n\nLine plots help us see how a variable changes over time. They are the default for the kind argument, but we can pass kind='line' to be explicit in our intent:\n\nfb.plot(\n    kind='line',\n    y='open',\n    figsize=(10, 5),\n    style='-b',\n    legend=False,\n    title='Evolution of Facebook Open Price'\n)\n\n\n\n\n\n\n\n\nWe provided the style argument in the previous example; however, we can use the color and linestyle arguments to get the same result:\n\nfb.plot(\n    kind='line',\n    y='open',\n    figsize=(10, 5),\n    color='blue',\n    linestyle='solid',\n    legend=False,\n    title='Evolution of Facebook Open Price'\n)\n\n\n\n\n\n\n\n\nWe can also plot many lines at once by simply passing a list of the columns to plot:\n\nfb.first('1W').plot(\n    y=['open', 'high', 'low', 'close'],\n    style=['o-b', '--r', ':k', '.-g'],\n    title='Facebook OHLC Prices during 1st Week of Trading 2018'\n).autoscale()\n\n\n\n\n\n\n\n\n\n\nWhen plotting with pandas, creating subplots is simply a matter of passing subplots=True to the plot() method, and (optionally) specifying the layout in a tuple of (rows, columns):\n\nfb.plot(\n    kind='line',\n    subplots=True,\n    layout=(3, 2),\n    figsize=(15, 10),\n    title='Facebook Stock 2018'\n)\n\narray([[&lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;],\n       [&lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;],\n       [&lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nNote that we didn’t provide a specific column to plot and pandas plotted all of them for us.\nSometimes we want to make subplots that each have a few variables in them for comparison. This can be achieved using the ax parameter. To illustrate this, let’s take a look at daily new COVID-19 cases in China, Spain, Italy, the USA, Brazil, and India:\n\nnew_cases_rolling_average = covid.pivot_table(\n    index=covid.index, \n    columns='countriesAndTerritories', \n    values='cases'\n).rolling(7).mean()\n\nSince there is a lot of fluctuation in these values, we will plot the 7-day moving average of new cases using the rolling() method (discussed in chapter 4). Rather than create a separate plot for each country (which makes it harder to compare) or plot them all together (which will make it difficult to see the smaller values), we will plot countries that have had a similar number of cases in the same subplot:\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nnew_cases_rolling_average[['China']].plot(ax=axes[0], style='-.c')\nnew_cases_rolling_average[['Italy', 'Spain']].plot(\n    ax=axes[1], style=['-', '--'], \n    title='7-day rolling average of new COVID-19 cases\\n(source: ECDC)'\n)\nnew_cases_rolling_average[['Brazil', 'India', 'USA']]\\\n    .plot(ax=axes[2], style=['--', ':', '-'])\n\n\n\n\n\n\n\n\nNOTE: we specified the line styles here so that the lines can be distinguished in the text as a black and white image.\nIn the previous figure, we were able to compare countries with similar levels of new COVID-19 cases, but we couldn’t compare all of them in the same plot due to scale. One way around this is to use an area plot, which makes it possible for us to visualize the overall 7-day rolling average of new COVID-19 cases and at the same time how much each country is contributing to the total. In the interest of readability, we will group Italy and Spain together and create another category for countries other than the USA, Brazil, and India. The combined height of the plot areas is the overall value, and the height of given shaded region is the value for the individual country.\n\nplot_cols = ['Brazil', 'India', 'Italy & Spain', 'USA', 'Other']\ngrouped = ['Italy', 'Spain']\nother_cols = [\n    col for col in new_cases_rolling_average.columns \n    if col not in plot_cols\n]\n\nnew_cases_rolling_average.sort_index(axis=1).assign(\n    **{\n        'Italy & Spain': lambda x: x[grouped].sum(axis=1),\n        'Other': lambda x: x[other_cols].drop(columns=grouped).sum(axis=1)\n    }\n)[plot_cols].plot(\n    kind='area', figsize=(15, 5), \n    title='7-day rolling average of new COVID-19 cases\\n(source: ECDC)'\n)\n\n\n\n\n\n\n\n\nAnother way to visualize evolution over time is to look at the cumulative sum over time. Let’s plot the cumulative number of COVID-19 cases in China, Spain, Italy, the USA, Brazil, and India, using ax to create subplots as we did in the previous example.\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 3))\n\ncumulative_covid_cases = covid.groupby(\n    ['countriesAndTerritories', pd.Grouper(freq='1D')]\n).cases.sum().unstack(0).apply('cumsum')\n\ncumulative_covid_cases[['China']].plot(ax=axes[0], style='-.c')\ncumulative_covid_cases[['Italy', 'Spain']].plot(\n    ax=axes[1], style=['-', '--'], \n    title='Cumulative COVID-19 Cases\\n(source: ECDC)'\n)\ncumulative_covid_cases[['Brazil', 'India', 'USA']]\\\n    .plot(ax=axes[2], style=['--', ':', '-'])\n\n\n\n\n\n\n\n\nNOTE: we specified the line styles here so that the lines can be distinguished in the text as a black and white image.\n\n\n\n\n\n\nWe make scatter plots to help visualize the relationship between two variables. Creating scatter plots requires we pass in kind='scatter' along with a column for the x-axis and a column for the y-axis:\n\nfb.assign(\n    max_abs_change=fb.high - fb.low\n).plot(\n    kind='scatter', x='volume', y='max_abs_change',\n    title='Facebook Daily High - Low vs. Volume Traded'\n)\n\n\n\n\n\n\n\n\nThe relationship doesn’t seem to be linear, but we can try a log transform on the x-axis since the scales of the axes are very different. With pandas, we simply pass in logx=True:\n\nfb.assign(\n    max_abs_change=fb.high - fb.low\n).plot(\n    kind='scatter', x='volume', y='max_abs_change',\n    title='Facebook Daily High - Low vs. log(Volume Traded)', \n    logx=True\n)\n\n\n\n\n\n\n\n\nWith matplotlib, we could use plt.xscale('log') to do the same thing.\n\n\n\nSometimes our plots have many overlapping values, but this can be impossible to see. This can be addressed by increasing the transparency of what we are plotting using the alpha parameter. It is a float in the range [0, 1] where 0 is completely transparent and 1 is completely opaque. By default this is 1, so let’s put in a lower value and re-plot the scatter plot:\n\nfb.assign(\n    max_abs_change=fb.high - fb.low\n).plot(\n    kind='scatter', x='volume', y='max_abs_change',\n    title='Facebook Daily High - Low vs. log(Volume Traded)',\n    logx=True, alpha=0.25\n)\n\n\n\n\n\n\n\n\n\n\n\nIn the previous example, we can start to see the overlaps, but it is still difficult. Hexbins are another plot type that divide up the plot into hexagons, which are shaded according to the density of points there. With pandas, this is the hexbin value for the kind argument. It may also be necessary to tweak the gridsize, which determines the number of hexagons along the y-axis:\n\nfb.assign(\n    log_volume=np.log(fb.volume),\n    max_abs_change=fb.high - fb.low\n).plot(\n    kind='hexbin',\n    x='log_volume',\n    y='max_abs_change',\n    title='Facebook Daily High - Low vs. log(Volume Traded)',\n    colormap='gray_r',\n    gridsize=20, \n    sharex=False # we have to pass this to see the x-axis\n)\n\n\n\n\n\n\n\n\n\n\n\nPandas doesn’t offer heatmaps; however, if we are able to get our data into a matrix, we can use matshow() from matplotlib:\n\nfig, ax = plt.subplots(figsize=(20, 10))\n\n# calculate the correlation matrix\nfb_corr = fb.assign(\n    log_volume=np.log(fb.volume),\n    max_abs_change=fb.high - fb.low\n).corr()\n\n# create the heatmap and colorbar\nim = ax.matshow(fb_corr, cmap='seismic')\nim.set_clim(-1, 1)\nfig.colorbar(im)\n\n# label the ticks with the column names\nlabels = [col.lower() for col in fb_corr.columns]\nax.set_xticks(ax.get_xticks()[1:-1]) # to handle bug in matplotlib\nax.set_xticklabels(labels, rotation=45)\nax.set_yticks(ax.get_yticks()[1:-1]) # to handle bug in matplotlib\nax.set_yticklabels(labels)\n\n# include the value of the correlation coefficient in the boxes\nfor (i, j), coef in np.ndenumerate(fb_corr):\n    ax.text(\n        i, j, fr'$\\rho$ = {coef:.2f}', # raw (r), format (f) string\n        ha='center', va='center', \n        color='white', fontsize=14\n    )\n\n\n\n\n\n\n\n\nAccessing the values in the correlation matrix can be done with loc[]:\n\nfb_corr.loc['max_abs_change', ['volume', 'log_volume']]\n\nvolume        0.642027\nlog_volume    0.731542\nName: max_abs_change, dtype: float64\n\n\n\n\n\n\n\n\nWith the pandas, making histograms is as easy as passing kind='hist' to the plot() method:\n\nfb.volume.plot(\n    kind='hist', \n    title='Histogram of Daily Volume Traded in Facebook Stock'\n)\nplt.xlabel('Volume traded') # label the x-axis (discussed in chapter 6)\n\nText(0.5, 0, 'Volume traded')\n\n\n\n\n\n\n\n\n\nWe can overlap histograms to compare distributions provided we use the alpha parameter. For example, let’s compare the usage and magnitude of the various measurement techniques (the magType column) in the data:\n\nfig, axes = plt.subplots(figsize=(8, 5))\n\nfor magtype in quakes.magType.unique():\n    data = quakes.query(f'magType == \"{magtype}\"').mag\n    if not data.empty:\n        data.plot(\n            kind='hist', ax=axes, alpha=0.4, \n            label=magtype, legend=True,\n            title='Comparing histograms of earthquake magnitude by magType'\n        )\n\nplt.xlabel('magnitude') # label the x-axis (discussed in chapter 6)\n\nText(0.5, 0, 'magnitude')\n\n\n\n\n\n\n\n\n\n\n\n\nWe can pass kind='kde' for an estimate of the probability density function (PDF), which tells us the probability of getting a particular value:\n\nfb.high.plot(\n    kind='kde', \n    title='KDE of Daily High Price for Facebook Stock'\n)\nplt.xlabel('Price ($)') # label the x-axis (discussed in chapter 6)\n\nText(0.5, 0, 'Price ($)')\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot() method returns an Axes object. We can store this for additional customization of the plot, or we can pass this into another call to plot() as the ax argument to add to the original plot.\nIt can often be helpful to view the KDE superimposed on top of the histogram, which can be achieved with this strategy:\n\nax = fb.high.plot(kind='hist', density=True, alpha=0.5)\nfb.high.plot(\n    ax=ax, kind='kde', color='blue', \n    title='Distribution of Facebook Stock\\'s Daily High Price in 2018'\n)\nplt.xlabel('Price ($)') # label the x-axis (discussed in chapter 6)\n\nText(0.5, 0, 'Price ($)')\n\n\n\n\n\n\n\n\n\n\n\n\nIn some cases, we are more interested in the probability of getting less than or equal to that value (or greater than or equal), which we can see with the cumulative disribution function (CDF). Using the statsmodels package, we can estimate the CDF giving us the empirical cumulative distribution function (ECDF):\n\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\necdf = ECDF(quakes.query('magType == \"ml\"').mag)\nplt.plot(ecdf.x, ecdf.y)\n\n# axis labels (we will cover this in chapter 6)\nplt.xlabel('mag') # add x-axis label \nplt.ylabel('cumulative probability') # add y-axis label\n\n# add title (we will cover this in chapter 6)\nplt.title('ECDF of earthquake magnitude with magType ml')\n\nText(0.5, 1.0, 'ECDF of earthquake magnitude with magType ml')\n\n\n\n\n\n\n\n\n\nThis ECDF tells us the probability of getting an earthquake with magnitude of 3 or less using the ml scale is 98%:\n\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\necdf = ECDF(quakes.query('magType == \"ml\"').mag)\nplt.plot(ecdf.x, ecdf.y)\n\n# formatting below will all be covered in chapter 6\n# axis labels\nplt.xlabel('mag') # add x-axis label \nplt.ylabel('cumulative probability') # add y-axis label\n\n# add reference lines for interpreting the ECDF for mag &lt;= 3 \nplt.plot(\n    [3, 3], [0, .98], '--k', \n    [-1.5, 3], [0.98, 0.98], '--k', alpha=0.4\n)\n\n# set axis ranges\nplt.ylim(0, None)\nplt.xlim(-1.25, None)\n\n# add a title\nplt.title('P(mag &lt;= 3) = 98%')\n\nText(0.5, 1.0, 'P(mag &lt;= 3) = 98%')\n\n\n\n\n\n\n\n\n\n\n\n\nTo make box plots with pandas, we pass kind='box' to the plot() method:\n\nfb.iloc[:,:4].plot(kind='box', title='Facebook OHLC Prices Box Plot')\nplt.ylabel('price ($)') # label the y-axis (discussed in chapter 6)\n\nText(0, 0.5, 'price ($)')\n\n\n\n\n\n\n\n\n\nIf we pass in notch=True, we get a notched box plot. The notch represents a 95% confidence interval around the median, which can be helpful when comparing differences. For an introduction to interpreting a notched box plot, see this Google sites page and this Towards Data Science article.\n\nfb.iloc[:,:4].plot(kind='box', title='Facebook OHLC Prices Box Plot', notch=True)\nplt.ylabel('price ($)') # label the y-axis (discussed in chapter 6)\n\nText(0, 0.5, 'price ($)')\n\n\n\n\n\n\n\n\n\nThis can also be combined with a call to groupby():\n\nfb.assign(\n    volume_bin=pd.cut(fb.volume, 3, labels=['low', 'med', 'high'])\n).groupby('volume_bin').boxplot(\n    column=['open', 'high', 'low', 'close'],\n    layout=(1, 3), figsize=(12, 3)\n)\nplt.suptitle('Facebook OHLC Box Plots by Volume Traded', y=1.1)\n\nText(0.5, 1.1, 'Facebook OHLC Box Plots by Volume Traded')\n\n\n\n\n\n\n\n\n\nWe can use this to see the distribution of magnitudes across the different measurement methods for earthquakes:\n\nquakes[['mag', 'magType']].groupby('magType').boxplot(\n    figsize=(15, 8), subplots=False\n)\nplt.title('Earthquake Magnitude Box Plots by magType')\nplt.ylabel('magnitude') # label the y-axis (discussed in chapter 6)\n\nText(0, 0.5, 'magnitude')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPassing kind='barh' gives us horizontal bars while kind='bar' gives us vertical ones. Let’s use horizontal bars to look at the top 15 places for earthquakes in our data:\n\nquakes.parsed_place.value_counts().iloc[14::-1,].plot(\n    kind='barh', figsize=(10, 5),\n    title='Top 15 Places for Earthquakes '\n          '(September 18, 2018 - October 13, 2018)'\n)\nplt.xlabel('earthquakes') # label the x-axis (discussed in chapter 6)\n\nText(0.5, 0, 'earthquakes')\n\n\n\n\n\n\n\n\n\nWe also have data on whether earthquakes were accompanied by tsunamis. Let’s see what the top places for tsunamis are:\n\nquakes.groupby('parsed_place').tsunami.sum().sort_values().iloc[-10:,].plot(\n    kind='barh', figsize=(10, 5), \n    title='Top 10 Places for Tsunamis '\n          '(September 18, 2018 - October 13, 2018)'\n)\nplt.xlabel('tsunamis') # label the x-axis (discussed in chapter 6)\n\nText(0.5, 0, 'tsunamis')\n\n\n\n\n\n\n\n\n\nSeeing that Indonesia is the top place for tsunamis during the time period we are looking at, we may want to look how many earthquakes and tsunamis Indonesia gets on a daily basis. We could show this as a line plot or with bars; since we don’t want to interpolate, we will use bars here:\n\nindonesia_quakes = quakes.query('parsed_place == \"Indonesia\"').assign(\n    time=lambda x: pd.to_datetime(x.time, unit='ms'),\n    earthquake=1\n).set_index('time').resample('1D').sum()\n\n# format the datetimes in the index for the x-axis\nindonesia_quakes.index = indonesia_quakes.index.strftime('%b\\n%d')\n\nindonesia_quakes.plot(\n    y=['earthquake', 'tsunami'], kind='bar', figsize=(15, 3), \n    rot=0, label=['earthquakes', 'tsunamis'], \n    title='Earthquakes and Tsunamis in Indonesia '\n          '(September 18, 2018 - October 13, 2018)'\n)\n\n# label the axes (discussed in chapter 6)\nplt.xlabel('date')\nplt.ylabel('count')\n\nC:\\Users\\gpower\\AppData\\Local\\Temp\\ipykernel_13112\\3940988219.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  ).set_index('time').resample('1D').sum()\n\n\nText(0, 0.5, 'count')\n\n\n\n\n\n\n\n\n\n\n\n\n\nquakes.groupby(['parsed_place', 'tsunami']).mag.count()\\\n    .unstack().apply(lambda x: x / x.sum(), axis=1)\\\n    .rename(columns={0: 'no', 1: 'yes'})\\\n    .sort_values('yes', ascending=False)[7::-1]\\\n    .plot.barh(\n        title='Frequency of a tsunami accompanying an earthquake'\n    )\n\n# move legend to the right of the plot\nplt.legend(title='tsunami?', bbox_to_anchor=(1, 0.65))\n\n# label the axes (discussed in chapter 6)\nplt.xlabel('percentage of earthquakes')\nplt.ylabel('')\n\nText(0, 0.5, '')\n\n\n\n\n\n\n\n\n\nUsing the kind arugment for vertical bars when the labels for each bar are shorter:\n\nquakes.magType.value_counts().plot(\n    kind='bar', title='Earthquakes Recorded per magType', rot=0\n)\n\n# label the axes (discussed in chapter 6)\nplt.xlabel('magType')\nplt.ylabel('earthquakes')\n\nText(0, 0.5, 'earthquakes')\n\n\n\n\n\n\n\n\n\n\n\n\n\npivot = quakes.assign(\n    mag_bin=lambda x: np.floor(x.mag)\n).pivot_table(\n    index='mag_bin', columns='magType', values='mag', aggfunc='count'\n)\npivot.plot.bar(\n    stacked=True, rot=0, ylabel='earthquakes', \n    title='Earthquakes by integer magnitude and magType'\n)\n\n\n\n\n\n\n\n\n\n\nPlot the percentages to be better able to see the different magTypes.\n\nnormalized_pivot = pivot.fillna(0).apply(lambda x: x / x.sum(), axis=1)\nax = normalized_pivot.plot.bar(\n    stacked=True, rot=0, figsize=(10, 5),\n    title='Percentage of earthquakes by integer magnitude for each magType'\n)\nax.legend(bbox_to_anchor=(1, 0.8)) # move legend to the right of the plot\nplt.ylabel('percentage') # label the y-axis (discussed in chapter 6)\n\nText(0, 0.5, 'percentage')\n\n\n\n\n\n\n\n\n\nWe can also create horizontal stacked bars and do so using groupby() and unstack():\n\nquakes.groupby(['parsed_place', 'tsunami']).mag.count()\\\n    .unstack().apply(lambda x: x / x.sum(), axis=1)\\\n    .rename(columns={0: 'no', 1: 'yes'})\\\n    .sort_values('yes', ascending=False)[7::-1]\\\n    .plot.barh(\n        title='Frequency of a tsunami accompanying an earthquake', \n        stacked=True\n    )\n\n# move legend to the right of the plot\nplt.legend(title='tsunami?', bbox_to_anchor=(1, 0.65))\n\n# label the axes (discussed in chapter 6)\nplt.xlabel('percentage of earthquakes')\nplt.ylabel('')\n\nText(0, 0.5, '')",
    "crumbs": [
      "Home",
      "Plotting with Pandas"
    ]
  },
  {
    "objectID": "notebooks/plotting_with_pandas.html#about-the-data",
    "href": "notebooks/plotting_with_pandas.html#about-the-data",
    "title": "Plotting with Pandas",
    "section": "",
    "text": "In this notebook, we will be working with 3 datasets: - Facebook’s stock price throughout 2018 (obtained using the stock_analysis package) - Earthquake data from September 18, 2018 - October 13, 2018 (obtained from the US Geological Survey (USGS) using the USGS API) - European Centre for Disease Prevention and Control’s (ECDC) daily number of new reported cases of COVID-19 by country worldwide dataset collected on September 19, 2020 via this link",
    "crumbs": [
      "Home",
      "Plotting with Pandas"
    ]
  },
  {
    "objectID": "notebooks/plotting_with_pandas.html#setup",
    "href": "notebooks/plotting_with_pandas.html#setup",
    "title": "Plotting with Pandas",
    "section": "",
    "text": "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfb = pd.read_csv(\n    '../data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True\n)\nquakes = pd.read_csv('../data/earthquakes.csv')\ncovid = pd.read_csv('../data/covid19_cases.csv').assign(\n    date=lambda x: pd.to_datetime(x.dateRep, format='%d/%m/%Y')\n).set_index('date').replace(\n    'United_States_of_America', 'USA'\n).sort_index()['2020-01-18':'2020-09-18']",
    "crumbs": [
      "Home",
      "Plotting with Pandas"
    ]
  },
  {
    "objectID": "notebooks/plotting_with_pandas.html#evolution-over-time",
    "href": "notebooks/plotting_with_pandas.html#evolution-over-time",
    "title": "Plotting with Pandas",
    "section": "",
    "text": "Line plots help us see how a variable changes over time. They are the default for the kind argument, but we can pass kind='line' to be explicit in our intent:\n\nfb.plot(\n    kind='line',\n    y='open',\n    figsize=(10, 5),\n    style='-b',\n    legend=False,\n    title='Evolution of Facebook Open Price'\n)\n\n\n\n\n\n\n\n\nWe provided the style argument in the previous example; however, we can use the color and linestyle arguments to get the same result:\n\nfb.plot(\n    kind='line',\n    y='open',\n    figsize=(10, 5),\n    color='blue',\n    linestyle='solid',\n    legend=False,\n    title='Evolution of Facebook Open Price'\n)\n\n\n\n\n\n\n\n\nWe can also plot many lines at once by simply passing a list of the columns to plot:\n\nfb.first('1W').plot(\n    y=['open', 'high', 'low', 'close'],\n    style=['o-b', '--r', ':k', '.-g'],\n    title='Facebook OHLC Prices during 1st Week of Trading 2018'\n).autoscale()\n\n\n\n\n\n\n\n\n\n\nWhen plotting with pandas, creating subplots is simply a matter of passing subplots=True to the plot() method, and (optionally) specifying the layout in a tuple of (rows, columns):\n\nfb.plot(\n    kind='line',\n    subplots=True,\n    layout=(3, 2),\n    figsize=(15, 10),\n    title='Facebook Stock 2018'\n)\n\narray([[&lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;],\n       [&lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;],\n       [&lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nNote that we didn’t provide a specific column to plot and pandas plotted all of them for us.\nSometimes we want to make subplots that each have a few variables in them for comparison. This can be achieved using the ax parameter. To illustrate this, let’s take a look at daily new COVID-19 cases in China, Spain, Italy, the USA, Brazil, and India:\n\nnew_cases_rolling_average = covid.pivot_table(\n    index=covid.index, \n    columns='countriesAndTerritories', \n    values='cases'\n).rolling(7).mean()\n\nSince there is a lot of fluctuation in these values, we will plot the 7-day moving average of new cases using the rolling() method (discussed in chapter 4). Rather than create a separate plot for each country (which makes it harder to compare) or plot them all together (which will make it difficult to see the smaller values), we will plot countries that have had a similar number of cases in the same subplot:\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nnew_cases_rolling_average[['China']].plot(ax=axes[0], style='-.c')\nnew_cases_rolling_average[['Italy', 'Spain']].plot(\n    ax=axes[1], style=['-', '--'], \n    title='7-day rolling average of new COVID-19 cases\\n(source: ECDC)'\n)\nnew_cases_rolling_average[['Brazil', 'India', 'USA']]\\\n    .plot(ax=axes[2], style=['--', ':', '-'])\n\n\n\n\n\n\n\n\nNOTE: we specified the line styles here so that the lines can be distinguished in the text as a black and white image.\nIn the previous figure, we were able to compare countries with similar levels of new COVID-19 cases, but we couldn’t compare all of them in the same plot due to scale. One way around this is to use an area plot, which makes it possible for us to visualize the overall 7-day rolling average of new COVID-19 cases and at the same time how much each country is contributing to the total. In the interest of readability, we will group Italy and Spain together and create another category for countries other than the USA, Brazil, and India. The combined height of the plot areas is the overall value, and the height of given shaded region is the value for the individual country.\n\nplot_cols = ['Brazil', 'India', 'Italy & Spain', 'USA', 'Other']\ngrouped = ['Italy', 'Spain']\nother_cols = [\n    col for col in new_cases_rolling_average.columns \n    if col not in plot_cols\n]\n\nnew_cases_rolling_average.sort_index(axis=1).assign(\n    **{\n        'Italy & Spain': lambda x: x[grouped].sum(axis=1),\n        'Other': lambda x: x[other_cols].drop(columns=grouped).sum(axis=1)\n    }\n)[plot_cols].plot(\n    kind='area', figsize=(15, 5), \n    title='7-day rolling average of new COVID-19 cases\\n(source: ECDC)'\n)\n\n\n\n\n\n\n\n\nAnother way to visualize evolution over time is to look at the cumulative sum over time. Let’s plot the cumulative number of COVID-19 cases in China, Spain, Italy, the USA, Brazil, and India, using ax to create subplots as we did in the previous example.\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 3))\n\ncumulative_covid_cases = covid.groupby(\n    ['countriesAndTerritories', pd.Grouper(freq='1D')]\n).cases.sum().unstack(0).apply('cumsum')\n\ncumulative_covid_cases[['China']].plot(ax=axes[0], style='-.c')\ncumulative_covid_cases[['Italy', 'Spain']].plot(\n    ax=axes[1], style=['-', '--'], \n    title='Cumulative COVID-19 Cases\\n(source: ECDC)'\n)\ncumulative_covid_cases[['Brazil', 'India', 'USA']]\\\n    .plot(ax=axes[2], style=['--', ':', '-'])\n\n\n\n\n\n\n\n\nNOTE: we specified the line styles here so that the lines can be distinguished in the text as a black and white image.",
    "crumbs": [
      "Home",
      "Plotting with Pandas"
    ]
  },
  {
    "objectID": "notebooks/plotting_with_pandas.html#visualizing-relationships-between-variables",
    "href": "notebooks/plotting_with_pandas.html#visualizing-relationships-between-variables",
    "title": "Plotting with Pandas",
    "section": "",
    "text": "We make scatter plots to help visualize the relationship between two variables. Creating scatter plots requires we pass in kind='scatter' along with a column for the x-axis and a column for the y-axis:\n\nfb.assign(\n    max_abs_change=fb.high - fb.low\n).plot(\n    kind='scatter', x='volume', y='max_abs_change',\n    title='Facebook Daily High - Low vs. Volume Traded'\n)\n\n\n\n\n\n\n\n\nThe relationship doesn’t seem to be linear, but we can try a log transform on the x-axis since the scales of the axes are very different. With pandas, we simply pass in logx=True:\n\nfb.assign(\n    max_abs_change=fb.high - fb.low\n).plot(\n    kind='scatter', x='volume', y='max_abs_change',\n    title='Facebook Daily High - Low vs. log(Volume Traded)', \n    logx=True\n)\n\n\n\n\n\n\n\n\nWith matplotlib, we could use plt.xscale('log') to do the same thing.\n\n\n\nSometimes our plots have many overlapping values, but this can be impossible to see. This can be addressed by increasing the transparency of what we are plotting using the alpha parameter. It is a float in the range [0, 1] where 0 is completely transparent and 1 is completely opaque. By default this is 1, so let’s put in a lower value and re-plot the scatter plot:\n\nfb.assign(\n    max_abs_change=fb.high - fb.low\n).plot(\n    kind='scatter', x='volume', y='max_abs_change',\n    title='Facebook Daily High - Low vs. log(Volume Traded)',\n    logx=True, alpha=0.25\n)\n\n\n\n\n\n\n\n\n\n\n\nIn the previous example, we can start to see the overlaps, but it is still difficult. Hexbins are another plot type that divide up the plot into hexagons, which are shaded according to the density of points there. With pandas, this is the hexbin value for the kind argument. It may also be necessary to tweak the gridsize, which determines the number of hexagons along the y-axis:\n\nfb.assign(\n    log_volume=np.log(fb.volume),\n    max_abs_change=fb.high - fb.low\n).plot(\n    kind='hexbin',\n    x='log_volume',\n    y='max_abs_change',\n    title='Facebook Daily High - Low vs. log(Volume Traded)',\n    colormap='gray_r',\n    gridsize=20, \n    sharex=False # we have to pass this to see the x-axis\n)\n\n\n\n\n\n\n\n\n\n\n\nPandas doesn’t offer heatmaps; however, if we are able to get our data into a matrix, we can use matshow() from matplotlib:\n\nfig, ax = plt.subplots(figsize=(20, 10))\n\n# calculate the correlation matrix\nfb_corr = fb.assign(\n    log_volume=np.log(fb.volume),\n    max_abs_change=fb.high - fb.low\n).corr()\n\n# create the heatmap and colorbar\nim = ax.matshow(fb_corr, cmap='seismic')\nim.set_clim(-1, 1)\nfig.colorbar(im)\n\n# label the ticks with the column names\nlabels = [col.lower() for col in fb_corr.columns]\nax.set_xticks(ax.get_xticks()[1:-1]) # to handle bug in matplotlib\nax.set_xticklabels(labels, rotation=45)\nax.set_yticks(ax.get_yticks()[1:-1]) # to handle bug in matplotlib\nax.set_yticklabels(labels)\n\n# include the value of the correlation coefficient in the boxes\nfor (i, j), coef in np.ndenumerate(fb_corr):\n    ax.text(\n        i, j, fr'$\\rho$ = {coef:.2f}', # raw (r), format (f) string\n        ha='center', va='center', \n        color='white', fontsize=14\n    )\n\n\n\n\n\n\n\n\nAccessing the values in the correlation matrix can be done with loc[]:\n\nfb_corr.loc['max_abs_change', ['volume', 'log_volume']]\n\nvolume        0.642027\nlog_volume    0.731542\nName: max_abs_change, dtype: float64",
    "crumbs": [
      "Home",
      "Plotting with Pandas"
    ]
  },
  {
    "objectID": "notebooks/plotting_with_pandas.html#visualizing-distributions",
    "href": "notebooks/plotting_with_pandas.html#visualizing-distributions",
    "title": "Plotting with Pandas",
    "section": "",
    "text": "With the pandas, making histograms is as easy as passing kind='hist' to the plot() method:\n\nfb.volume.plot(\n    kind='hist', \n    title='Histogram of Daily Volume Traded in Facebook Stock'\n)\nplt.xlabel('Volume traded') # label the x-axis (discussed in chapter 6)\n\nText(0.5, 0, 'Volume traded')\n\n\n\n\n\n\n\n\n\nWe can overlap histograms to compare distributions provided we use the alpha parameter. For example, let’s compare the usage and magnitude of the various measurement techniques (the magType column) in the data:\n\nfig, axes = plt.subplots(figsize=(8, 5))\n\nfor magtype in quakes.magType.unique():\n    data = quakes.query(f'magType == \"{magtype}\"').mag\n    if not data.empty:\n        data.plot(\n            kind='hist', ax=axes, alpha=0.4, \n            label=magtype, legend=True,\n            title='Comparing histograms of earthquake magnitude by magType'\n        )\n\nplt.xlabel('magnitude') # label the x-axis (discussed in chapter 6)\n\nText(0.5, 0, 'magnitude')\n\n\n\n\n\n\n\n\n\n\n\n\nWe can pass kind='kde' for an estimate of the probability density function (PDF), which tells us the probability of getting a particular value:\n\nfb.high.plot(\n    kind='kde', \n    title='KDE of Daily High Price for Facebook Stock'\n)\nplt.xlabel('Price ($)') # label the x-axis (discussed in chapter 6)\n\nText(0.5, 0, 'Price ($)')\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot() method returns an Axes object. We can store this for additional customization of the plot, or we can pass this into another call to plot() as the ax argument to add to the original plot.\nIt can often be helpful to view the KDE superimposed on top of the histogram, which can be achieved with this strategy:\n\nax = fb.high.plot(kind='hist', density=True, alpha=0.5)\nfb.high.plot(\n    ax=ax, kind='kde', color='blue', \n    title='Distribution of Facebook Stock\\'s Daily High Price in 2018'\n)\nplt.xlabel('Price ($)') # label the x-axis (discussed in chapter 6)\n\nText(0.5, 0, 'Price ($)')\n\n\n\n\n\n\n\n\n\n\n\n\nIn some cases, we are more interested in the probability of getting less than or equal to that value (or greater than or equal), which we can see with the cumulative disribution function (CDF). Using the statsmodels package, we can estimate the CDF giving us the empirical cumulative distribution function (ECDF):\n\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\necdf = ECDF(quakes.query('magType == \"ml\"').mag)\nplt.plot(ecdf.x, ecdf.y)\n\n# axis labels (we will cover this in chapter 6)\nplt.xlabel('mag') # add x-axis label \nplt.ylabel('cumulative probability') # add y-axis label\n\n# add title (we will cover this in chapter 6)\nplt.title('ECDF of earthquake magnitude with magType ml')\n\nText(0.5, 1.0, 'ECDF of earthquake magnitude with magType ml')\n\n\n\n\n\n\n\n\n\nThis ECDF tells us the probability of getting an earthquake with magnitude of 3 or less using the ml scale is 98%:\n\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\necdf = ECDF(quakes.query('magType == \"ml\"').mag)\nplt.plot(ecdf.x, ecdf.y)\n\n# formatting below will all be covered in chapter 6\n# axis labels\nplt.xlabel('mag') # add x-axis label \nplt.ylabel('cumulative probability') # add y-axis label\n\n# add reference lines for interpreting the ECDF for mag &lt;= 3 \nplt.plot(\n    [3, 3], [0, .98], '--k', \n    [-1.5, 3], [0.98, 0.98], '--k', alpha=0.4\n)\n\n# set axis ranges\nplt.ylim(0, None)\nplt.xlim(-1.25, None)\n\n# add a title\nplt.title('P(mag &lt;= 3) = 98%')\n\nText(0.5, 1.0, 'P(mag &lt;= 3) = 98%')\n\n\n\n\n\n\n\n\n\n\n\n\nTo make box plots with pandas, we pass kind='box' to the plot() method:\n\nfb.iloc[:,:4].plot(kind='box', title='Facebook OHLC Prices Box Plot')\nplt.ylabel('price ($)') # label the y-axis (discussed in chapter 6)\n\nText(0, 0.5, 'price ($)')\n\n\n\n\n\n\n\n\n\nIf we pass in notch=True, we get a notched box plot. The notch represents a 95% confidence interval around the median, which can be helpful when comparing differences. For an introduction to interpreting a notched box plot, see this Google sites page and this Towards Data Science article.\n\nfb.iloc[:,:4].plot(kind='box', title='Facebook OHLC Prices Box Plot', notch=True)\nplt.ylabel('price ($)') # label the y-axis (discussed in chapter 6)\n\nText(0, 0.5, 'price ($)')\n\n\n\n\n\n\n\n\n\nThis can also be combined with a call to groupby():\n\nfb.assign(\n    volume_bin=pd.cut(fb.volume, 3, labels=['low', 'med', 'high'])\n).groupby('volume_bin').boxplot(\n    column=['open', 'high', 'low', 'close'],\n    layout=(1, 3), figsize=(12, 3)\n)\nplt.suptitle('Facebook OHLC Box Plots by Volume Traded', y=1.1)\n\nText(0.5, 1.1, 'Facebook OHLC Box Plots by Volume Traded')\n\n\n\n\n\n\n\n\n\nWe can use this to see the distribution of magnitudes across the different measurement methods for earthquakes:\n\nquakes[['mag', 'magType']].groupby('magType').boxplot(\n    figsize=(15, 8), subplots=False\n)\nplt.title('Earthquake Magnitude Box Plots by magType')\nplt.ylabel('magnitude') # label the y-axis (discussed in chapter 6)\n\nText(0, 0.5, 'magnitude')",
    "crumbs": [
      "Home",
      "Plotting with Pandas"
    ]
  },
  {
    "objectID": "notebooks/plotting_with_pandas.html#counts-and-frequencies",
    "href": "notebooks/plotting_with_pandas.html#counts-and-frequencies",
    "title": "Plotting with Pandas",
    "section": "",
    "text": "Passing kind='barh' gives us horizontal bars while kind='bar' gives us vertical ones. Let’s use horizontal bars to look at the top 15 places for earthquakes in our data:\n\nquakes.parsed_place.value_counts().iloc[14::-1,].plot(\n    kind='barh', figsize=(10, 5),\n    title='Top 15 Places for Earthquakes '\n          '(September 18, 2018 - October 13, 2018)'\n)\nplt.xlabel('earthquakes') # label the x-axis (discussed in chapter 6)\n\nText(0.5, 0, 'earthquakes')\n\n\n\n\n\n\n\n\n\nWe also have data on whether earthquakes were accompanied by tsunamis. Let’s see what the top places for tsunamis are:\n\nquakes.groupby('parsed_place').tsunami.sum().sort_values().iloc[-10:,].plot(\n    kind='barh', figsize=(10, 5), \n    title='Top 10 Places for Tsunamis '\n          '(September 18, 2018 - October 13, 2018)'\n)\nplt.xlabel('tsunamis') # label the x-axis (discussed in chapter 6)\n\nText(0.5, 0, 'tsunamis')\n\n\n\n\n\n\n\n\n\nSeeing that Indonesia is the top place for tsunamis during the time period we are looking at, we may want to look how many earthquakes and tsunamis Indonesia gets on a daily basis. We could show this as a line plot or with bars; since we don’t want to interpolate, we will use bars here:\n\nindonesia_quakes = quakes.query('parsed_place == \"Indonesia\"').assign(\n    time=lambda x: pd.to_datetime(x.time, unit='ms'),\n    earthquake=1\n).set_index('time').resample('1D').sum()\n\n# format the datetimes in the index for the x-axis\nindonesia_quakes.index = indonesia_quakes.index.strftime('%b\\n%d')\n\nindonesia_quakes.plot(\n    y=['earthquake', 'tsunami'], kind='bar', figsize=(15, 3), \n    rot=0, label=['earthquakes', 'tsunamis'], \n    title='Earthquakes and Tsunamis in Indonesia '\n          '(September 18, 2018 - October 13, 2018)'\n)\n\n# label the axes (discussed in chapter 6)\nplt.xlabel('date')\nplt.ylabel('count')\n\nC:\\Users\\gpower\\AppData\\Local\\Temp\\ipykernel_13112\\3940988219.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  ).set_index('time').resample('1D').sum()\n\n\nText(0, 0.5, 'count')\n\n\n\n\n\n\n\n\n\n\n\n\n\nquakes.groupby(['parsed_place', 'tsunami']).mag.count()\\\n    .unstack().apply(lambda x: x / x.sum(), axis=1)\\\n    .rename(columns={0: 'no', 1: 'yes'})\\\n    .sort_values('yes', ascending=False)[7::-1]\\\n    .plot.barh(\n        title='Frequency of a tsunami accompanying an earthquake'\n    )\n\n# move legend to the right of the plot\nplt.legend(title='tsunami?', bbox_to_anchor=(1, 0.65))\n\n# label the axes (discussed in chapter 6)\nplt.xlabel('percentage of earthquakes')\nplt.ylabel('')\n\nText(0, 0.5, '')\n\n\n\n\n\n\n\n\n\nUsing the kind arugment for vertical bars when the labels for each bar are shorter:\n\nquakes.magType.value_counts().plot(\n    kind='bar', title='Earthquakes Recorded per magType', rot=0\n)\n\n# label the axes (discussed in chapter 6)\nplt.xlabel('magType')\nplt.ylabel('earthquakes')\n\nText(0, 0.5, 'earthquakes')\n\n\n\n\n\n\n\n\n\n\n\n\n\npivot = quakes.assign(\n    mag_bin=lambda x: np.floor(x.mag)\n).pivot_table(\n    index='mag_bin', columns='magType', values='mag', aggfunc='count'\n)\npivot.plot.bar(\n    stacked=True, rot=0, ylabel='earthquakes', \n    title='Earthquakes by integer magnitude and magType'\n)\n\n\n\n\n\n\n\n\n\n\nPlot the percentages to be better able to see the different magTypes.\n\nnormalized_pivot = pivot.fillna(0).apply(lambda x: x / x.sum(), axis=1)\nax = normalized_pivot.plot.bar(\n    stacked=True, rot=0, figsize=(10, 5),\n    title='Percentage of earthquakes by integer magnitude for each magType'\n)\nax.legend(bbox_to_anchor=(1, 0.8)) # move legend to the right of the plot\nplt.ylabel('percentage') # label the y-axis (discussed in chapter 6)\n\nText(0, 0.5, 'percentage')\n\n\n\n\n\n\n\n\n\nWe can also create horizontal stacked bars and do so using groupby() and unstack():\n\nquakes.groupby(['parsed_place', 'tsunami']).mag.count()\\\n    .unstack().apply(lambda x: x / x.sum(), axis=1)\\\n    .rename(columns={0: 'no', 1: 'yes'})\\\n    .sort_values('yes', ascending=False)[7::-1]\\\n    .plot.barh(\n        title='Frequency of a tsunami accompanying an earthquake', \n        stacked=True\n    )\n\n# move legend to the right of the plot\nplt.legend(title='tsunami?', bbox_to_anchor=(1, 0.65))\n\n# label the axes (discussed in chapter 6)\nplt.xlabel('percentage of earthquakes')\nplt.ylabel('')\n\nText(0, 0.5, '')",
    "crumbs": [
      "Home",
      "Plotting with Pandas"
    ]
  },
  {
    "objectID": "notebooks/python_errors.html",
    "href": "notebooks/python_errors.html",
    "title": "Python Errors",
    "section": "",
    "text": "When an error arises, there will be an error message with the type of error and the line the error occured on. This notebook goes over how to handle the common types of errors and exceptions in Python.\nI recommend looking at the Python tutorial page for more information on errors. Searching for the error message directly on Google can help the debugging process if there is an error not discussed in this page.\n\n\nA SyntaxError occurs when the syntax of your code is incorrect.\n\nif True \nprint(\"Hello World\")\n\nSyntaxError: expected ':' (975521850.py, line 1)\n\n\nA colon is expected after the if statement, which arises the syntax error. The error goes away after adding the colon.\n\nif True:\n    print(\"Hello World\")\n\nHello World\n\n\n\n\n\nA NameError occurs when a variable, function, or module used does not exist. When this happens, it is usually because of a spelling error.\n\nadd\n\nNameError: name 'add' is not defined\n\n\n\nstring(9)\n\nNameError: name 'string' is not defined\n\n\n\n\n\nA TypeError occurs when you input an incorrect data type for an operation or function.\n\n\"abc\" + 9\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\nIn python, you cannot add strings to integers. You can add, however, an integer to an integer or a string to a string with a +.\n\n9 + 9\n\n18\n\n\n\n\"abc\" + \"def\"\n\n'abcdef'\n\n\n\n\n\nA ZeroDivisionError occurs when you try to divide by zero. To fix this, recheck your computation.\n\n2 / (9 * 0)\n\nZeroDivisionError: division by zero\n\n\n\n#code corrected to no longer divide by zero\n(2 / 9) * 0\n\n0.0\n\n\n\n\n\nA ValueError occurs when an input for a function is the correct data type but is invalid in regards to the domain of the function. This is most common with mathematical operations.\n\nimport math\n\nmath.sqrt(-10)\n\nValueError: math domain error\n\n\nIn the example above, you must input a positive number into the sqrt() function. The negative number is still an integer, but it is not in the function’s domain.\n\nmath.sqrt(10)\n\n3.1622776601683795\n\n\n\n\n\nAn IndexError occurs when you try to access an item in a list with an index out of bounds.\n\nlist = [1,2,3,4,5]\nlist[5]\n\nIndexError: list index out of range\n\n\nThe range of a list is [0, n-1], where “n” is the length of the list. So, the list [1,2,3,4,5] has index elements in the range 0-4.\n\nlist[4]\n\n5\n\n\n\n\n\nA ModuleNotFoundError occurs when you try to import a module that does not exist. It is a type of ImportError. To fix this error, check if you have installed the module in your python environment from the terminal command-line.\n\nimport pillow\n\nModuleNotFoundError: No module named 'pillow'\n\n\n\n\n\nYou can use a try statement to catch errors. A try clause includes the code you want to run that might cause an error. If no error occurs, the try clause runs successfully. If an error does occur, the except clause runs after the line in the try clause that caused an error.\n\ntry:\n    \"abc\" + 9\n    print(\"Success\")\nexcept:\n    print(\"Failure to execute\")\n\nFailure to execute\n\n\nThe except clause above can catch any type of error. However, an except clause can also catch a specific type of error. There can be mulptile except clauses in a try statement to catch the different types of errors.\n\ntry:\n    hello\n    \"abc\" + 9\n    print(\"Success\")\nexcept TypeError:\n    print(\"TypeError failure to execute\")\nexcept NameError:\n    print(\"NameError failure to execute\")\n\nNameError failure to execute\n\n\n\ntry:\n    list = [1,2,3,4,5]\n    list[5]\n    print(\"Success\")\nexcept TypeError:\n    print(\"TypeError failure to execute\")\nexcept NameError:\n    print(\"NameError failure to execute\")\nexcept IndexError:\n    print(\"IndexError failure to execute\")\n\nIndexError failure to execute\n\n\n\n\n\nThese are not all the errors that might come up in your coding. If another type of error occurs, you can search the error type on Google to learn more about what has caused it. As always, remember to look at the line resulting in the error for hints on what could have gone wrong!",
    "crumbs": [
      "Home",
      "Python Errors"
    ]
  },
  {
    "objectID": "notebooks/python_errors.html#syntax-error",
    "href": "notebooks/python_errors.html#syntax-error",
    "title": "Python Errors",
    "section": "",
    "text": "A SyntaxError occurs when the syntax of your code is incorrect.\n\nif True \nprint(\"Hello World\")\n\nSyntaxError: expected ':' (975521850.py, line 1)\n\n\nA colon is expected after the if statement, which arises the syntax error. The error goes away after adding the colon.\n\nif True:\n    print(\"Hello World\")\n\nHello World",
    "crumbs": [
      "Home",
      "Python Errors"
    ]
  },
  {
    "objectID": "notebooks/python_errors.html#name-error",
    "href": "notebooks/python_errors.html#name-error",
    "title": "Python Errors",
    "section": "",
    "text": "A NameError occurs when a variable, function, or module used does not exist. When this happens, it is usually because of a spelling error.\n\nadd\n\nNameError: name 'add' is not defined\n\n\n\nstring(9)\n\nNameError: name 'string' is not defined",
    "crumbs": [
      "Home",
      "Python Errors"
    ]
  },
  {
    "objectID": "notebooks/python_errors.html#type-error",
    "href": "notebooks/python_errors.html#type-error",
    "title": "Python Errors",
    "section": "",
    "text": "A TypeError occurs when you input an incorrect data type for an operation or function.\n\n\"abc\" + 9\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\nIn python, you cannot add strings to integers. You can add, however, an integer to an integer or a string to a string with a +.\n\n9 + 9\n\n18\n\n\n\n\"abc\" + \"def\"\n\n'abcdef'",
    "crumbs": [
      "Home",
      "Python Errors"
    ]
  },
  {
    "objectID": "notebooks/python_errors.html#zero-division-error",
    "href": "notebooks/python_errors.html#zero-division-error",
    "title": "Python Errors",
    "section": "",
    "text": "A ZeroDivisionError occurs when you try to divide by zero. To fix this, recheck your computation.\n\n2 / (9 * 0)\n\nZeroDivisionError: division by zero\n\n\n\n#code corrected to no longer divide by zero\n(2 / 9) * 0\n\n0.0",
    "crumbs": [
      "Home",
      "Python Errors"
    ]
  },
  {
    "objectID": "notebooks/python_errors.html#value-error",
    "href": "notebooks/python_errors.html#value-error",
    "title": "Python Errors",
    "section": "",
    "text": "A ValueError occurs when an input for a function is the correct data type but is invalid in regards to the domain of the function. This is most common with mathematical operations.\n\nimport math\n\nmath.sqrt(-10)\n\nValueError: math domain error\n\n\nIn the example above, you must input a positive number into the sqrt() function. The negative number is still an integer, but it is not in the function’s domain.\n\nmath.sqrt(10)\n\n3.1622776601683795",
    "crumbs": [
      "Home",
      "Python Errors"
    ]
  },
  {
    "objectID": "notebooks/python_errors.html#index-error",
    "href": "notebooks/python_errors.html#index-error",
    "title": "Python Errors",
    "section": "",
    "text": "An IndexError occurs when you try to access an item in a list with an index out of bounds.\n\nlist = [1,2,3,4,5]\nlist[5]\n\nIndexError: list index out of range\n\n\nThe range of a list is [0, n-1], where “n” is the length of the list. So, the list [1,2,3,4,5] has index elements in the range 0-4.\n\nlist[4]\n\n5",
    "crumbs": [
      "Home",
      "Python Errors"
    ]
  },
  {
    "objectID": "notebooks/python_errors.html#module-not-found-error",
    "href": "notebooks/python_errors.html#module-not-found-error",
    "title": "Python Errors",
    "section": "",
    "text": "A ModuleNotFoundError occurs when you try to import a module that does not exist. It is a type of ImportError. To fix this error, check if you have installed the module in your python environment from the terminal command-line.\n\nimport pillow\n\nModuleNotFoundError: No module named 'pillow'",
    "crumbs": [
      "Home",
      "Python Errors"
    ]
  },
  {
    "objectID": "notebooks/python_errors.html#catching-exceptions-with-try-statements",
    "href": "notebooks/python_errors.html#catching-exceptions-with-try-statements",
    "title": "Python Errors",
    "section": "",
    "text": "You can use a try statement to catch errors. A try clause includes the code you want to run that might cause an error. If no error occurs, the try clause runs successfully. If an error does occur, the except clause runs after the line in the try clause that caused an error.\n\ntry:\n    \"abc\" + 9\n    print(\"Success\")\nexcept:\n    print(\"Failure to execute\")\n\nFailure to execute\n\n\nThe except clause above can catch any type of error. However, an except clause can also catch a specific type of error. There can be mulptile except clauses in a try statement to catch the different types of errors.\n\ntry:\n    hello\n    \"abc\" + 9\n    print(\"Success\")\nexcept TypeError:\n    print(\"TypeError failure to execute\")\nexcept NameError:\n    print(\"NameError failure to execute\")\n\nNameError failure to execute\n\n\n\ntry:\n    list = [1,2,3,4,5]\n    list[5]\n    print(\"Success\")\nexcept TypeError:\n    print(\"TypeError failure to execute\")\nexcept NameError:\n    print(\"NameError failure to execute\")\nexcept IndexError:\n    print(\"IndexError failure to execute\")\n\nIndexError failure to execute",
    "crumbs": [
      "Home",
      "Python Errors"
    ]
  },
  {
    "objectID": "notebooks/python_errors.html#next-steps",
    "href": "notebooks/python_errors.html#next-steps",
    "title": "Python Errors",
    "section": "",
    "text": "These are not all the errors that might come up in your coding. If another type of error occurs, you can search the error type on Google to learn more about what has caused it. As always, remember to look at the line resulting in the error for hints on what could have gone wrong!",
    "crumbs": [
      "Home",
      "Python Errors"
    ]
  },
  {
    "objectID": "notebooks/wide_vs_long.html",
    "href": "notebooks/wide_vs_long.html",
    "title": "Wide vs. Long Format Data",
    "section": "",
    "text": "In this notebook, we will be using daily temperature data from the National Centers for Environmental Information (NCEI) API. We will use the Global Historical Climatology Network - Daily (GHCND) dataset for the Boonton 1 station (GHCND:USC00280907); see the documentation here.\nNote: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for “NCEI weather API” to find the updated one.\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nwide_df = pd.read_csv('../data/wide_data.csv', parse_dates=['date'])\nlong_df = pd.read_csv(\n    '../data/long_data.csv', \n    usecols=['date', 'datatype', 'value'], \n    parse_dates=['date']\n)[['date', 'datatype', 'value']] # sort columns\n\n\n\n\nOur variables each have their own column:\n\nwide_df.head(6)\n\n\n\n\n\n\n\n\ndate\nTMAX\nTMIN\nTOBS\n\n\n\n\n0\n2018-10-01\n21.1\n8.9\n13.9\n\n\n1\n2018-10-02\n23.9\n13.9\n17.2\n\n\n2\n2018-10-03\n25.0\n15.6\n16.1\n\n\n3\n2018-10-04\n22.8\n11.7\n11.7\n\n\n4\n2018-10-05\n23.3\n11.7\n18.9\n\n\n5\n2018-10-06\n20.0\n13.3\n16.1\n\n\n\n\n\n\n\nDescribing all the columns is easy:\n\nwide_df.describe(include='all')\n\n\n\n\n\n\n\n\ndate\nTMAX\nTMIN\nTOBS\n\n\n\n\ncount\n31\n31.000000\n31.000000\n31.000000\n\n\nmean\n2018-10-16 00:00:00\n16.829032\n7.561290\n10.022581\n\n\nmin\n2018-10-01 00:00:00\n7.800000\n-1.100000\n-1.100000\n\n\n25%\n2018-10-08 12:00:00\n12.750000\n2.500000\n5.550000\n\n\n50%\n2018-10-16 00:00:00\n16.100000\n6.700000\n8.300000\n\n\n75%\n2018-10-23 12:00:00\n21.950000\n13.600000\n16.100000\n\n\nmax\n2018-10-31 00:00:00\n26.700000\n17.800000\n21.700000\n\n\nstd\nNaN\n5.714962\n6.513252\n6.596550\n\n\n\n\n\n\n\nIt’s easy to graph with pandas:\n\nwide_df.plot(\n    x='date', y=['TMAX', 'TMIN', 'TOBS'], figsize=(15, 5), \n    title='Temperature in NYC in October 2018'\n).set_ylabel('Temperature in Celsius')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nOur variable names are now in the datatype column and their values are in the value column. We now have 3 rows for each date, since we have 3 different datatypes:\n\nlong_df.head(6)\n\n\n\n\n\n\n\n\ndate\ndatatype\nvalue\n\n\n\n\n0\n2018-10-01\nTMAX\n21.1\n\n\n1\n2018-10-01\nTMIN\n8.9\n\n\n2\n2018-10-01\nTOBS\n13.9\n\n\n3\n2018-10-02\nTMAX\n23.9\n\n\n4\n2018-10-02\nTMIN\n13.9\n\n\n5\n2018-10-02\nTOBS\n17.2\n\n\n\n\n\n\n\nSince we have many rows for the same date, using describe() is not that helpful:\n\nlong_df.describe(include='all')\n\n\n\n\n\n\n\n\ndate\ndatatype\nvalue\n\n\n\n\ncount\n93\n93\n93.000000\n\n\nunique\nNaN\n3\nNaN\n\n\ntop\nNaN\nTMAX\nNaN\n\n\nfreq\nNaN\n31\nNaN\n\n\nmean\n2018-10-16 00:00:00\nNaN\n11.470968\n\n\nmin\n2018-10-01 00:00:00\nNaN\n-1.100000\n\n\n25%\n2018-10-08 00:00:00\nNaN\n6.700000\n\n\n50%\n2018-10-16 00:00:00\nNaN\n11.700000\n\n\n75%\n2018-10-24 00:00:00\nNaN\n17.200000\n\n\nmax\n2018-10-31 00:00:00\nNaN\n26.700000\n\n\nstd\nNaN\nNaN\n7.362354\n\n\n\n\n\n\n\nPlotting long format data in pandas can be rather tricky. Instead we use seaborn:\n\nimport seaborn as sns\n\nsns.set(rc={'figure.figsize': (15, 5)}, style='white')\n\nax = sns.lineplot(\n    data=long_df, x='date', y='value', hue='datatype'\n)\nax.set_ylabel('Temperature in Celsius')\nax.set_title('Temperature in NYC in October 2018')\nplt.show()\n\n\n\n\n\n\n\n\nWith long data and seaborn, we can easily facet our plots:\n\nsns.set(\n    rc={'figure.figsize': (20, 10)}, style='white', font_scale=2\n)\n\ng = sns.FacetGrid(long_df, col='datatype', height=10)\ng = g.map(plt.plot, 'date', 'value')\ng.set_titles(size=25)\ng.set_xticklabels(rotation=45)\nplt.show()",
    "crumbs": [
      "Home",
      "Wide vs. Long Format Data"
    ]
  },
  {
    "objectID": "notebooks/wide_vs_long.html#about-the-data",
    "href": "notebooks/wide_vs_long.html#about-the-data",
    "title": "Wide vs. Long Format Data",
    "section": "",
    "text": "In this notebook, we will be using daily temperature data from the National Centers for Environmental Information (NCEI) API. We will use the Global Historical Climatology Network - Daily (GHCND) dataset for the Boonton 1 station (GHCND:USC00280907); see the documentation here.\nNote: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for “NCEI weather API” to find the updated one.",
    "crumbs": [
      "Home",
      "Wide vs. Long Format Data"
    ]
  },
  {
    "objectID": "notebooks/wide_vs_long.html#setup",
    "href": "notebooks/wide_vs_long.html#setup",
    "title": "Wide vs. Long Format Data",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\n\nwide_df = pd.read_csv('../data/wide_data.csv', parse_dates=['date'])\nlong_df = pd.read_csv(\n    '../data/long_data.csv', \n    usecols=['date', 'datatype', 'value'], \n    parse_dates=['date']\n)[['date', 'datatype', 'value']] # sort columns",
    "crumbs": [
      "Home",
      "Wide vs. Long Format Data"
    ]
  },
  {
    "objectID": "notebooks/wide_vs_long.html#wide-format",
    "href": "notebooks/wide_vs_long.html#wide-format",
    "title": "Wide vs. Long Format Data",
    "section": "",
    "text": "Our variables each have their own column:\n\nwide_df.head(6)\n\n\n\n\n\n\n\n\ndate\nTMAX\nTMIN\nTOBS\n\n\n\n\n0\n2018-10-01\n21.1\n8.9\n13.9\n\n\n1\n2018-10-02\n23.9\n13.9\n17.2\n\n\n2\n2018-10-03\n25.0\n15.6\n16.1\n\n\n3\n2018-10-04\n22.8\n11.7\n11.7\n\n\n4\n2018-10-05\n23.3\n11.7\n18.9\n\n\n5\n2018-10-06\n20.0\n13.3\n16.1\n\n\n\n\n\n\n\nDescribing all the columns is easy:\n\nwide_df.describe(include='all')\n\n\n\n\n\n\n\n\ndate\nTMAX\nTMIN\nTOBS\n\n\n\n\ncount\n31\n31.000000\n31.000000\n31.000000\n\n\nmean\n2018-10-16 00:00:00\n16.829032\n7.561290\n10.022581\n\n\nmin\n2018-10-01 00:00:00\n7.800000\n-1.100000\n-1.100000\n\n\n25%\n2018-10-08 12:00:00\n12.750000\n2.500000\n5.550000\n\n\n50%\n2018-10-16 00:00:00\n16.100000\n6.700000\n8.300000\n\n\n75%\n2018-10-23 12:00:00\n21.950000\n13.600000\n16.100000\n\n\nmax\n2018-10-31 00:00:00\n26.700000\n17.800000\n21.700000\n\n\nstd\nNaN\n5.714962\n6.513252\n6.596550\n\n\n\n\n\n\n\nIt’s easy to graph with pandas:\n\nwide_df.plot(\n    x='date', y=['TMAX', 'TMIN', 'TOBS'], figsize=(15, 5), \n    title='Temperature in NYC in October 2018'\n).set_ylabel('Temperature in Celsius')\nplt.show()",
    "crumbs": [
      "Home",
      "Wide vs. Long Format Data"
    ]
  },
  {
    "objectID": "notebooks/wide_vs_long.html#long-format",
    "href": "notebooks/wide_vs_long.html#long-format",
    "title": "Wide vs. Long Format Data",
    "section": "",
    "text": "Our variable names are now in the datatype column and their values are in the value column. We now have 3 rows for each date, since we have 3 different datatypes:\n\nlong_df.head(6)\n\n\n\n\n\n\n\n\ndate\ndatatype\nvalue\n\n\n\n\n0\n2018-10-01\nTMAX\n21.1\n\n\n1\n2018-10-01\nTMIN\n8.9\n\n\n2\n2018-10-01\nTOBS\n13.9\n\n\n3\n2018-10-02\nTMAX\n23.9\n\n\n4\n2018-10-02\nTMIN\n13.9\n\n\n5\n2018-10-02\nTOBS\n17.2\n\n\n\n\n\n\n\nSince we have many rows for the same date, using describe() is not that helpful:\n\nlong_df.describe(include='all')\n\n\n\n\n\n\n\n\ndate\ndatatype\nvalue\n\n\n\n\ncount\n93\n93\n93.000000\n\n\nunique\nNaN\n3\nNaN\n\n\ntop\nNaN\nTMAX\nNaN\n\n\nfreq\nNaN\n31\nNaN\n\n\nmean\n2018-10-16 00:00:00\nNaN\n11.470968\n\n\nmin\n2018-10-01 00:00:00\nNaN\n-1.100000\n\n\n25%\n2018-10-08 00:00:00\nNaN\n6.700000\n\n\n50%\n2018-10-16 00:00:00\nNaN\n11.700000\n\n\n75%\n2018-10-24 00:00:00\nNaN\n17.200000\n\n\nmax\n2018-10-31 00:00:00\nNaN\n26.700000\n\n\nstd\nNaN\nNaN\n7.362354\n\n\n\n\n\n\n\nPlotting long format data in pandas can be rather tricky. Instead we use seaborn:\n\nimport seaborn as sns\n\nsns.set(rc={'figure.figsize': (15, 5)}, style='white')\n\nax = sns.lineplot(\n    data=long_df, x='date', y='value', hue='datatype'\n)\nax.set_ylabel('Temperature in Celsius')\nax.set_title('Temperature in NYC in October 2018')\nplt.show()\n\n\n\n\n\n\n\n\nWith long data and seaborn, we can easily facet our plots:\n\nsns.set(\n    rc={'figure.figsize': (20, 10)}, style='white', font_scale=2\n)\n\ng = sns.FacetGrid(long_df, col='datatype', height=10)\ng = g.map(plt.plot, 'date', 'value')\ng.set_titles(size=25)\ng.set_xticklabels(rotation=45)\nplt.show()",
    "crumbs": [
      "Home",
      "Wide vs. Long Format Data"
    ]
  }
]